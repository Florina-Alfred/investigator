**Title & Citation**  
- *RoboKube: Establishing a New Foundation for the Cloud Native Evolution in Robotics*  
- Y. Liu & A. Hernández Herranz, Ericsson Research, 2025 (author–email provided)  

---

## Abstract  
Claim: Cloud‑native technologies can be transplanted to robotics by linking ROS 2 with Kubernetes.  
Evidence: The authors review existing literature/industry practices, identify limitations in cloud‑robot integration, and propose RoboKube, an adaptive framework that automates networking, containerization, and application deployment on any K8s‑compatible platform. They validate the approach with a teleoperation testbed of a UR5 arm and a joystick.  

---

## Introduction & Motivation  
1. **Cloud‑native evolution** (monolith → micro‑services, CI/CD, dynamic elasticity) has proved powerful in cloud industry.  
2. **IoT / CPS fields** are adopting the same shift (e.g., K3s, KubeEdge, Azure IoT Edge).  
3. **ROS 2** offers modularity, QoS, RT, security – key for industrial robotics.  
4. Yet, the **today’s gap**: containers for ROS exist, but orchestrating them across heterogeneous networks (edge ↔ core) remains hard.  
5. The paper’s goal: provide a **fully‑functional, production‑ready** platform that bridges ROS 2 applications with Kubernetes, especially over WAN/heterogeneous networks.  

---

## Methods / Approach  

### 1. Platform Choice  
- **Kubernetes‑compatible** distribution (no specific variant).  
- **K3s** is preferred: single‑binary, lightweight, fully‑compatible with K8s ecosystem → easier install for resource‑constrained edge devices.

### 2. Networking Strategy  
- ROS 2 relies on **DDS/RTPS** (broker‑less multicast over UDP).  
- RoboKube introduces an **overlay network** (via CNI) that makes “under‑lay” details invisible to ROS.  
- Two overlay back‑ends that support multicast:  
  * **Kube‑ovn** – tight integration with OVN, high throughput.  
  * **WeaveNet** – supports UDP multicast, but lower latency/bandwidth. (Benchmarks: Weave < Kube‑ovn).  

- **Ingress / NodePort**:  
  * NodePort exposes all ports on every node but limited to range 30000‑32767.  
  * Ingress (Traefik, etc.) offers a single entry point and flexible routing, albeit with a small bandwidth hit.  

- **Multicast optimisation**: enable IGMP snooping to cut unnecessary multicast packets.  

- **MTU coordination**: set pod MTU 100 bytes less than physical NIC MTU to avoid fragmentation/drop.  

### 3. ROS 2 Containerisation  

Two‑stage Docker builds:  

1. **Build stage** – install ROS deps and build node.  
2. **Runtime stage** – copy binaries, libs, ROS stack into minimal ROS base image.  

In addition, **DockerSlim** further prunes image by removing unused libraries → 30× size reduction.  

### 4. Deployment & Lifecycle  

- **Helm charts** are used for deployment: templated values, rolling updates, versioning.  
- One‑stop install: Helm chart → deploy → upgrade.  

### 5. Distribution / Clustering of ROS nodes  

- **Hardware affinity**: node-level scheduling based on required hardware (e.g., joystick USB).  
- **Performance metrics**: application‑level (mAP, SLAM error) influenced by system metrics (latency, CPU). Hence, profiling is advised.  
- **Offloading / Migration**: Static topology vs. dynamic migration; trade‑off between flexibility and overhead.  

---

## Experiments / Data / Results  

### Case Study: RoboKube‑Powered Teleoperation Testbed  

- **Hardware**: UR5 robot arm + USB joystick.  
- **Architecture** (Fig. 2): ROS nodes arranged in a pipeline – joy → servo → forward position controller → UR5 reverse interface.  
- **Containerisation**:  
  * Joy node container (only joystick support) → image shrinks 82 % (486 MB → 83 MB).  
  * UR5 driver container (servo + controller) → image 300 MB from 2.6 GB.  

- **Deployment**: two K3s nodes in distinct sub‑nets (demonstrating WAN).  
- **Device plugin**: expose joystick resource; pod requests `joystick:1`, guaranteeing node placement.  

- **Ingress** used to expose ports 50001/50002 (outside NodePort range).  

- **Outcome**: cloudified deployment works across WAN, upgradeable via Helm, pods migrate automatically except the joy node.

---

## Discussion & Analysis  

- **Feasibility**: RoboKube removes major obstacles: lack of multicast support, port coordination, device‑level resource definition.  
- **Performance**: Empirical data not quantified; but image shrink and overlay choice expected to yield low latency (best with Kube‑ovn).  
- **Scalability**: DockerSlim + K3s allows many nodes on edge; Helm provides lifecycle.  
- **Limitations**:  
  * Only ROS 2 nodes supported—ROS 1 requires adaptation.  
  * Multi‑pod node restriction: only one ROS 2 container per pod due to RTPS port conflicts.  
  * No discussion on security (network isolation, RBAC).  
  * Offloading/migration overhead not benchmarked.  

- **Open Questions**:  
  * How to automatically map QoS to K8s resource requests.  
  * Integration with existing robotic middleware (e.g., rosbridge).  
  * Handling of higher‑level network functions (WAN, cellular).  

---

## Conclusions  

- RoboKube provides a **complete** pipeline: K3s installation → CNI overlay → DockerSlim image build → Helm deployment → device‑level scheduling.  
- Offers a **production‑ready** foundation that simplifies cloud‑native scaling for ROS 2.  
- Case study demonstrates practically how a hardware‑dependent node (joy) can co‑exist with compute‑heavy nodes on distributed cluster.  

---

## Key Claims & Contributions  

| Claim                                    | Evidence / Reasoning                                         |
|------------------------------------------|--------------------------------------------------------------|
| ROS 2 can be effectively cloudified over Kubernetes | RoboKube integrates RTPS multicast via overlay, handles port coordination. |
| Multi‑stage Docker + DockerSlim yields significant image shrink | Joy node image shrank 82 % (486 → 83 MB). |
| Helm allows production‑grade deployment lifecycle | Rolling updates, rollback shown in teleoperation testbed. |
| Overlay network (Kube‑ovn) outperforms WeaveNet for DDS traffic | Benchmarks alluded; Kube‑ovn chosen. |
| Hardware‑dependent nodes (joystick) can be scheduled via device plugins | Device plugin example `joystick:1` ensures placement. |
| RoboKube works over heterogeneous networks (LAN/WAN) | Teleoperation testbed spans two sub‑nets. |

---

## Definitions & Key Terms  

- **Cloud‑native**: lightweight micro‑services, CI/CD, dynamic scaling.  
- **Kubernetes (K8s)**: orchestration platform, pods, services, CNI.  
- **K3s**: lightweight K8s derivative (single binary).  
- **ROS 2**: Robot Operating System version 2 (DDS/RTPS middleware).  
- **DDS/RTPS**: Data-Distribution Service / Real‑Time Publish‑Subscribe.  
- **QoS**: Quality of Service settings in DDS (latency, reliability, etc.).  
- **CNI**: Container Network Interface.  
- **Kube‑ovn**, **WeaveNet**: Two CNI plugins supporting multicast.  
- **Ingress**: K8s resource that exposes external traffic to an internal service.  
- **NodePort**: K8s service type that forwards port to all nodes.  
- **DockerSlim**: tool that prunes Docker images by runtime analysis.  

---

## Important Figures & Tables  

*Fig. 1* – Overlay network architecture bridging device‑edge‑cloud. Shows Docker containers on K8s nodes, overlay network, and multicast path.  

*Fig. 2* – Teleoperation testbed architecture: joystick → ROS nodes → UR5 reverse interface.  

*Fig. 3* – ROS 2 node network diagram: ellipses (nodes), rectangles (topics).  

---

## Limitations & Open Questions  

- **No comprehensive performance metrics** (latency, throughput) reported.  
- **Offloading vs. migration flexibility** not quantitatively benchmarked.  
- **Security considerations** (RBAC, network isolation) omitted.  
- **Scalability at cluster‑size level** not tested (only two nodes).  
- **ROS 1 support** not addressed.  

Open research avenues: automatic QoS‑to‑resource mapping; integration with other middleware (rosbridge, rqt); multi‑region communication over cellular.  

---

## References to Original Sections  
*(refer to section numbers in the paper)*  
- **Background** (Sec. I): Definitions, ROS 2 value proposition.  
- **Cloudification practice** (Sec. II): literature and industrial review.  
- **Robokube platform** (Sec. III): orchestration choice, networking methods.  
- **Application perspective** (Sec. IV): containerisation best practices.  
- **Case study** (Sec. V): teleop ecosystem.  
- **Concluding remarks** (Sec. VI): reflective synthesis.  

---  

**Executive Summary (optional)**  

- Adopts Kubernetes (K3s) as orchestrator for ROS 2.  
- Implements multicast‑aware overlay (Kube‑ovn) to satisfy DDS.  
- Introduces two‑stage Docker build + DockerSlim for lean images.  
- Uses Helm charts for deployment, upgrades, and device‑level scheduling.  
- Demonstrated with teleop UR5+joystick; works across two sub‑nets.  
- Presents practical guidelines for networking, distribution, and clustering of ROS nodes.  
- Highlights remaining gaps: performance data, security, larger‑scale evaluation.  

---  

**Supplementary Material** – None provided beyond the paper’s figures/tables.