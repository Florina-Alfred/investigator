**Title & Citation**  
*Enabling 5G QoS configuration capabilities for IoT applications on container orchestration platform* – Yu Liu & Aitor Hernández Herranz, Ericsson Research, Stockholm, Sweden (contact: yu.a.liu@ericsson.com / aitor.hernandez.herranz@ericsson.com).  

---

### Abstract  
Container orchestration platforms such as Kubernetes (K8s) have become the backbone of modern cloud, edge, and device‑cloud architectures. As 5G networks mature, many IoT use‑cases – robotics, XR, distributed SLAM – demand fine‑grained Quality‑of‑Service (QoS) on the access network.  However, K8s’ overlay networks (VXLAN, IP‑in‑IP, IPSec, etc.) hide per‑pod QoS metadata from the underlay network, making it hard to translate pod‑level priorities into 5G QoS flows.  
This work proposes a native, low‑footprint solution that injects QoS information from K8s into the 5G Network Exposure Function (NEF) by using Linux firewall marks (fwmarks). A custom Container Network Interface (CNI) plugin establishes a per‑pod QoS flow: it sets a fwmark on pod egress, contacts the NEF (or directly the 5G modem stack) to create a 5G QoS Flow Identifier (QFI), and installs Traffic‑Control (TC) filters so that marked packets are routed to the appropriate QoS flow.  Validation on a realistic distributed SLAM workload on a K3s testbed demonstrates that the CNI plugin can reliably embed QoS into the 5G data path without packet manipulation, and that QoS limits manifest as expected in latency‑sensitive applications.  The approach is fully Kubernetes‑native, non‑invasive, and requires no changes to the 3GPP standard or 5G core.

---

## 1. Introduction & Motivation  
1. **Container Orchestration & IoT** – K8s and derivatives orchestrate clusters spanning edge and cloud. To ease inter‑pod communication an overlay network (VXLAN, IP‑in‑IP, IPSec) is usually created.  
2. **5G QoS** – 5G introduces flow‑level QoS control (QFI, 5QI) so that UE can request different packet delay budgets, priorities, bandwidth, etc., for each PDU session.  Access networks (5G, Wi‑Fi) now advertise per‑flow QoS to enable AR/VR, robotics, distributed SLAM.  
3. **Gap** – Overlay encapsulation hides application QoS metadata from the access network. Existing K8s CNI plugins provide only local traffic shaping (TBF/HTB) on a node; they cannot propagate per‑pod QoS to the 5G Underlay. Existing SDN‑based approaches (OpenShift SDN, NSX‑T, NBWGuard, device plugins) are limited to node‑level or hardware‑dependent; none expose pod QoS to 5G.  
4. **Goal** – Develop a native K8s CNI plugin that can map pod‑level QoS to 5G NEF/AMF with zero packet header manipulation and minimal overhead, and validate the approach on a realistic IoT application (distributed SLAM).

---

## 2. Methods / Approach  

### 2.1 Architecture  
```
(UE pod) 
  ⇣  CNI traffic‑priority plugin (executes during ADD/DEL)
      ⇣  Sets fwmark on egress traffic
      ⇣  Contacts 5G NEF (via REST) or 5G modem stack (via AMF) to request a QFI
      ⇣  Adds TC filter to redirect marked packets to the created QoS flow
```
- *Binding to control plane*: The QoS requirement is entered into K8s via kube-apiserver → kube-controller-manager → kubelet.  
- *Journey within the runtime*: kubelet invokes the CNI plugin chain (e.g., networking, bridge, then traffic‑priority).  
- *Underlay mapping*: Once NEF creates the QoS flow, the CNI plugin installs *iptables* rules that set fwmark on egress traffic from the pod and *TC* redirect rules on the physical interface.

### 2.2 Linux fwmark Visibility & Availability  
- **Visibility** – In a typical K8s networking stack using flannel/VXLAN: pod packet → eth0 → veth pair → bridge cni0 → flannel.0 (VXLAN).  Before encapsulation, an iptables *mangle* rule can set fwmark; the mark survives encapsulation and is visible on the physical egress interface.  
- **Availability** – Table I shows fwmark bit allocations by popular CNI/SIM/SDN plugins.  Bits 0‑12, 16‑31 used by Cilium, bits 7, 13–15 by AWS CNI, Calico, etc.  In typical environments 13‑30 might be free; but in a Cilium‑only setup only 3 bits remain.  The plugin therefore checks free bits before assigning a mark.  

### 2.3 CNI Plugin Implementation  
- Four mandatory commands: **ADD**, **DEL**, **CHECK**, **VERSION** (CNI spec).  
- **ADD**:  
  1. Generate a unique fwmark (check free bits).  
  2. Insert corresponding *iptables* rules to tag egress traffic of the pod.  
  3. Ask 5G emulator / real NEF to create a QoS flow with a target 5QI (derived from requested priority).  
  4. Install TC filter on the physical interface to send marked packets into the created provider class.  
- **DEL**: Clean up all rules, marks, and destroy the QoS flow.  
- **CHECK / VERSION**: verify current state (not elaborated in the paper).  

### 2.4 Interaction with 5G Emulator  
- An emulator built on Linux TC replicates the 5G stack: each QoS flow is implemented by a TC *qdisc*/class/filters.  
- QoS flows are requested through exposed REST APIs (mirroring NEF).  
- Example (Fig. 5): three qdiscs (each representing a QoS flow) with distinct delay attributes; via filters, specific fwmarks are enqueued into corresponding classes. This emulates the UE‑to‑edge traffic path with 5G QoS.

---

## 3. Experiments / Data / Results  

### 3.1 Implementation Details  
- **Traffic‑priority CNI plugin** – Code packaged as a binary executed by the container runtime.  Only ADD/DEL needed for demonstration.  
- **5G emulator** – Linux‑TC based; REST APIs used to create/destroy flows; actual QoS delay values are hard‑coded.  

### 3.2 Validation with SLAM  
- **Testbed** – K3s cluster on Nvidia Jetson NX (edge), Jetson AGX (device), blade servers (cloud).  Multi‑arch (arm64, amd64) supported; Prometheus/Grafana observability.  
- **Application** – *maplab* (distributed SLAM) run on the cluster, using the device node to send camera + IMU data to the edge for localization, mapping, and optimization.  
- **Experiment categories**  
  - *QoS‑unlimited*: no QoS flow configured (default).  
  - *QoS‑limited*: pod assigned 10 ms latency QoS flow via CNI plugin.  
- **Metric** – Absolute Position Error (APE).  Two sub‑metrics: RMSE and Mean.  
- **Results (Table II)**  
| Category | RMSE (cm) | Mean (cm) |  
|---|---|---|  
| QoS‑unlimited | 9.24 ± 0.14 | 8.13 ± 0.16 |  
| QoS‑limited | 10.08 ± 0.12 | 8.85 ± 0.11 |  

*Interpretation*: The 10 ms QoS limit slightly increases RMSE and Mean – in line with earlier work [13] that shows latency directly degrades SLAM accuracy.  The difference is modest, demonstrating that the plugin correctly enforces the QoS without unexpected side‑effects.

---

## 4. Discussion & Analysis  

- **Transparency** – fwmark is set without altering packet payloads; therefore ETX/IPSecuriy or future upgrades need not affect the method.  
- **Native Kubernetes Integration** – The plugin follows the CNI spec, so it can be added to any node’s CNI chain without modifying kubelet or kubelet configuration.  
- **Overlay to Underlay Mapping** – By leveraging fwmark that survives encapsulation, the QoS origin in the overlay keeps a one‑to‑one mapping to underlay qdisc classes.  
- **Compatibility** – No changes to 3GPP‑defined NEF/AMF semantics; these maintain their own QFI.  The plugin only translates pod priority to QFI.  
- **Limitations** –  
  - Only egress (uplink) traffic considered.  Ingress traffic (downlink) remains unconfigured.  
  - QoS logic lives in the RAN (receive by gNB and UPF); no guarantee beyond core‑network, e.g., data‑network.  
  - fwmark bit constraints (Table I): in heavily plugin‑loaded environments, free fwmark bits may be scarce.  Careful conflict management is necessary.

---

## 5. Conclusions  

A Kubernetes‑native CNI plugin was proposed and implemented to expose per‑pod QoS to a 5G network. By setting Linux fwmarks on pod egress traffic and creating TC filters that match the corresponding NEF‑instantiated QFI, the approach non‑intrusively propagates QoS to the access network.  Validation on a realistic distributed SLAM workload shows expected latency‑dependent performance degradation, confirming correct QoS enforcement and low overhead.  The method is fully compatible with existing 5G infrastructure, requires no packet manipulation, and extends local bandwidth limits (CNI Bandwidth plugin) to the access network.

---

## 6. Key Claims & Contributions  

| Claim | Evidence/Reasoning | Section(s) |
|-------|--------------------|-------------|
| **Claim 1** – Container orchestration cannot expose QoS hidden in overlay packets due to encapsulation. | Discussion of VXLAN encapsulation (Fig 1). | I.A |
| **Claim 2** – Linux fwmark survives encapsulation and is visible on the physical interface. | Observation in Fig 3; path from cni0 to flannel.0. | II.B.1 |
| **Claim 3** – fwmark bits must be chosen carefully to avoid conflicts with existing plugins. | Table I fwmark registry. | II.B.2 |
| **Claim 4** – A CNI plugin can create a per‑pod QoS flow via NEF/AMF and map fwmarks to TC filters. | CNI plugin flow diagram (Fig 4) and implementation description. | III.C |
| **Claim 5** – The proposed method introduces no packet manipulation overhead. | Packet flow shows only fwmark set via iptables; no header changes. | III.D |
| **Claim 6** – The method correctly injects QoS into 5G emulator and affects application latency. | SLAM experiments (Table II). | IV.B.2 |
| **Claim 7** – The solution extends K8s bandwidth limitation from node to access network. | Reasoning in III.D “enhances the bandwidth limit… from node to access network.” | III.D |
| **Claim 8** – The method is fully compatible with existing 5G infrastructure. | Use of standard NEF/AMF APIs; no protocol changes. | III.D |
| **Claim 9** – Current limitation: only egress QoS; no ingress or data‑network QoS. | Limitations section in III.D/IV. | IV. |

---

## 7. Definitions & Key Terms  

| Term | Definition | Source |
|------|-------------|--------|
| **5G** | Fifth‑generation wireless network defined by 3GPP. | II.A |
| **QoS (Quality of Service)** | Set of flow characteristics (priority, bandwidth, delay budget, error rate) defined in 3GPP TS 23.501. | II.A |
| **NEF (Network Exposure Function)** | 3GPP component exposing APIs to request configuration of PDU sessions, QoS flows, etc. | II.A |
| **AMF (Access & Mobility Management Function)** | 5G RAN control‑plane entity that can actively request QoS changes. | II.A |
| **CNI (Container Network Interface)** | Specification defining plugins that configure container networking. | III.C |
| **fwmark** | 32‑bit Linux firewall mark used to tag packets in kernel. | II.B.1 |
| **TC (Traffic Control)** | Linux traffic‑control subsystem (qdisc, class, filter). | III.C |
| **QFI / 5QI** | QoS Flow Identifier / 5G QoS Identifier mapping the flow's QoS characteristics. | II.A |
| **K8s / K3s** | Kubernetes mainline / lightweight distribution used in experiments. | – |
| **Overlay/Underlay** | Overlay: networking abstraction (VXLAN, IP‑in‑IP) used inside cluster; Underlay: physical network interfaces. | I.A |
| **SLAM** | Simultaneous Localization and Mapping (used for distributed SLAM experiments). | IV.B.1 |
| **APE (Absolute Position Error)** | Metric used to evaluate SLAM accuracy. | IV.B.2 |

---

## 8. Important Figures & Tables  

| Fig. | Content | Significance |
|------|---------|--------------|
| **Fig 1** | Anatomy of VXLAN packet | Shows how overlay encapsulation hides QoS metadata until underlay. |
| **Fig 2** | Proposed architecture diagram | Illustrates UE→CNI→NEF/AMF→TC flow. |
| **Fig 3** | Linux fwmark visibility in K8s stack | Demonstrates fwmark persistence from pod to physical interface. |
| **Fig 4** | CNI plugin flow (ADD → NEF → TC). | Clarifies plugin interactions with kubelet/daemon. |
| **Fig 5** | TC qdisc/class/filter example after QoS enforcement | Visualizes how fwmark-based filter enqueues traffic into specific qdisc. |
| **Table I** | Linux fwmark registry for bitwise usage | Highlights potential bit conflicts with existing software. |
| **Table II** | Comparison of APE between QoS‑unlimited and QoS‑limited SLAM experiments | Provides quantitative validation. |

---

## 9. Limitations & Open Questions  

- **Egress‑only** – The plugin only handles uplink packets from pod to network; downlink traffic remains unlabelled.  
- **RAN‑only** – QoS is assured only to the 5G core (UPF).  Data‑network end‑to‑end guarantees (e.g., Internet) are out of scope.  
- **fwmark bit constraints** – In heavily plugin‑loaded clusters many bits may be occupied; plugin must negotiate a free range or implement bit‑pooling.  
- **Scalability** – Load balance across many pods may lead to many TC filters; performance overhead not measured in paper.  
- **Real‑world NEF interaction** – Emulator used for experiment; deployment on actual NEF needs additional handling for authentication, errors, etc.  
- **Ingress handling** – Future work: extend to downlink QoS by integrating with egress of neighbouring pods or via SR‑IOV.  

---

## 10. References to Original Sections  

- **Abstract** – Section I  
- **Problem Statement & Motivation** – Section I.A, I.B, I.C  
- **Background** – Section II.A–II.C  
- **Problem Overview** – Section II.B–II.C  
- **Architecture** – Section III.A  
- **fwmark Visibility** – Section III.B.1  
- **CNI Plugin Flow** – Section III.C  
- **Benefits/Limits** – Section III.D  
- **Implementation Details** – Section IV.A  
- **5G Emulator** – Section IV.A.2  
- **SLAM Validation** – Section IV.B  
- **Table II** – Section IV.B  
- **Discussion/Conclusions** – Section V  
- **References** – Section VII  

---

## 11. Executive Summary / Key Takeaways (Optional)  

- A Kubernetes‑native CNI plugin can expose per‑pod QoS to a 5G network by setting Linux fwmarks and inserting TC filters.  
- The proposal bridges the overlay‑to‑underlay gap: fwmarks survive VXLAN encapsulation, enabling transparent QoS propagation without packet modification.  
- Validation on a realistic distributed SLAM workload shows expected latency sensitivity, confirming correct QoS enforcement.  
- The solution extends local bandwidth limiting to the access network, is fully compatible with existing 5G infrastructure, and adds no overhead.  
- Primary limitations: egress‑only, fwmark bit conflicts in heavily‑plugin‑loaded clusters, and requires further work for ingress/downlink QoS.

---

## 12. Supplementary Material  

- **Code Repository** – The traffic‑priority CNI plugin source and Dockerfile were provided as supplementary content (not reproduced here).  
- **5G Emulator APIs** – REST endpoints used to create/destroy QoS flows; implemented to translate to Linux TC commands.  
- **K3s Testbed Configuration** – Cluster composed of Jetson NX (edge), Jetson AGX (device), standard blade servers (cloud) – details in IV.B.1.  

---