**Title & Citation**  
*RobotKube: Orchestrating Large‑Scale Cooperative Multi‑Robot Systems with Kubernetes and ROS*  
B. Lampe *, L. Reiher *, L. Zanger *, T. Woopen *, R. van Kempen, and L. Eckstein.  
IEEE (2023) – submitted to IEEE.  All authors are affiliated with the Institute for Automotive Engineering (IKA), RWTH Aachen University.  

---

### Abstract  
The authors introduce *RobotKube*, a framework that extends Kubernetes with ROS–based operator and recording applications to manage containerized microservices in large‑scale Cooperative Intelligent Transport Systems (C‑ITS). Two core components – an event detector and an application manager – enable developers to specify event‑driven triggers (e.g., vehicle proximity) that automatically deploy or re‑configure ROS containers. A demonstrated use case shows the automated deployment of a cloud‑based recording application when two lidar‑equipped vehicles approach each other, thereby producing data for collective learning. The entire setup, including Docker images and KinD cluster configuration, is publicly released on GitHub.  

---

### Introduction & Motivation  

1. **CPS & C‑ITS Landscape** – Modern cooperative transport systems consist of heterogeneous entities (vehicles, roadside units, control centers, edge clouds).  
2. **Micro‑services for CPS** – Splitting functionality into loosely coupled services (micro‑services) improves modularity and enable independent upgrades.  
3. **Orchestration Challenge** – While containers ease deployment, a robust orchestrator (e.g., Kubernetes) is required to handle scaling, failure, and dynamic application lifecycles.  
4. **Domain‑specific Shortcomings** – Kubernetes lacks C‑ITS‑specific logic such as “deploy an application when two vehicles become close” or “capture data relevant for DevOps”.  
5. **Goal of RobotKube** – Augment Kubernetes with ROS‑based *operator* and *recording* applications that detect data patterns, request deployments, and store data automatically.  

---

### Methods / Approach  

#### System Architecture (Fig. 1)  

- **Cluster** – Comprises robots, roadside units, edge/cloud nodes.    
- **Applications** – A set of microservices bundled into a contiguous device‑independent application (e.g., object detection + tracking).  
- **Operator** – Either human or software that manages cluster configuration, deployments, and extra cluster tasks.  
- **Component Flow** –  
  1) *Event detector* – buffers recent ROS messages, performs user‑defined analysis, triggers an event.  
  2) *Action plugin* – receives the event, forwards a *task description* to an *application manager*.  
  3) *Application manager* – translates the task to a declarative Kubernetes workload, possibly changing or deleting existing pods.  
  4) *Kubernetes Control Plane* – reconciles the desired state, creates pods/ services, performs roll‑outs/ roll‑backs.  

The architecture purposely separates **Event Detector** (ROS node) from **Application Manager** (Python node) to keep the detection logic ROS‑centric and the Kubernetes API logic lightweight.

#### Design Principles (Table I – abstracted)  

- *Node‑agnostic* deployment of services.  
- *Container‑per‑microservice* packaging.  
- *Decoupled operator chain* – operator applications can deploy further operator applications.  
- *Dynamic conflict resolution* – e.g., off‑loading to other nodes if resources exhausted.  
- *Verification & Validation* (V&V) before registering applications.  

#### Key Components  

- **Event Detector** –  
  *Buffer* (ring buffer of last T seconds).  
  *Analysis* – user‑defined patterns over the buffered data.  
  *Action Plugin* – e.g., an *operator plugin* that builds a task description (list of required microservices, node constraints, parameters).  
  Implemented in C++ ROS 2 node.

- **Application Manager** –  
  Receives the task description, chooses suitable services from registries, creates a Kubernetes Deployment/Job, sets node selectors and resource requests.  
  If the deployment cannot be satisfied, resolves conflicts or reports failure.  
  Implemented in Python using the Kubernetes Python API.  

- **Recording Application** –  
  Mirrors the same architecture with a *recording plugin* instead of the operator plugin.  
  Stores received ROS messages into a MongoDB database.  

- **Compatibility Layer** – MQTT bridges to forward ROS <-> MQTT messages, enabling cross‑cloud communication.  

---

### Experiments / Data / Results  

#### Experimental Setup (Section IV)  

| Parameter | Value |
|-----------|-------|
| **N** (vehicles) | 15 |
| **M** (lidar‑equipped) | 2 |
| **fₚ** (pose freq) | 100 Hz |
| **fₚc** (point cloud freq) | 10 Hz |
| **d_start** | 400 m |
| **d_stop** | 500 m |

- **Cluster** – KinD (Kubernetes‑in‑Docker) simulating a multinode cluster on one machine.  
- **Vehicle Data Flow** – Simulated using ROS bags; forwarded to cloud server **C** via MQTT.  
- **Event Detector** – Monitors poses, triggers when distance < d_start.  
- **Application Manager** – Deploys two MQTT clients + recording application in the cloud node.  
- **Data Storage** – Cloud MongoDB receives poses & point clouds; no further analysis.  

#### Figure 2 (Software Flow)  

Shows:  
1. Poses → MQTT → Cloud.  
2. Event detector → (via operator plugin) → application manager → Kubernetes.  
3. Application manager → (via control plane) → new pods (MQTT clients, recording).  
4. Recording pod ↔ MongoDB.  

The figure illustrates that the deployment is triggered only when the event happens.

#### Latency Evaluation (Section V)  

| Step | Median Latency |
|------|----------------|
| **Event detection** (pose distance calculation) | < 1 ms |
| **Task generation** (operator plugin) | < 1 ms |
| **Translation to workload** (application manager) | ~100 ms |
| **Cluster reconciliation** (Kubernetes) | ~5 s (dominant) |
| **Data storage** (MongoDB write) | ~0.5 s for 10 s worth of data |

The **cluster reconciliation** phase (pod creation, readiness checks, networking setup) is the primary latency contributor. Consequently, “launching a new application” has a minimum overhead of several seconds on a KinD cluster.  

---

### Discussion & Analysis  

- **Feasibility** – The experiment demonstrates that robotic applications can be deployed, reconfigured, and automatically shut down purely by data‑driven events.  
- **Scaling Considerations** – In a real C‑ITS environment, finite wireless bandwidth and longer distance latencies may further impact event detection.  
- **Latency Constraints** – While 5 s is acceptable for non‑real‑time use cases (e.g., data recording, heavy analytics), it may be too high for latency‑critical functions. Pre‑launching idle containers and smarter state recovery could reduce this overhead.  
- **Reproducibility** – All Docker images, Kubernetes YAML configs, and ROS bag data are available on GitHub; the entire experiment can be re‑run on any compatible machine.  
- **Extensibility** – The event‑detector/action‑plugin pattern can be extended to any domain logic; other middleware (e.g., ROS 2 DDS) would also fit.  

---

### Conclusions  

RobotKube offers a practical, ROS‑centric extension of Kubernetes that supports automated, data‑driven orchestration in large‑scale C‑ITS. By providing event detector and application manager components, the framework enables operators to specify “when‑something‑happens” logic which automatically creates or removes ROS micro‑services. A demonstrator scenario confirms the viability of the architecture, pinpoints the dominant reconciliation latency, and outlines future research directions (e.g., pre‑deployment strategies, real‑time constraints).  

---

### Key Claims & Contributions  

1. **Claim:** *RobotKube completes Kubernetes’ API surface with ROS‑specific operator logic.*  
   *Evidence:* Detailed description of event detector → action plugin → application manager; implementation in ROS nodes; case study.  

2. **Claim:** *Event detector can process any incoming ROS data and trigger deployments based on user‑defined analysis.*  
   *Evidence:* Ring‑buffer design; C++ node; high‑performance (< 1 ms).  

3. **Claim:** *Application manager translates abstract task descriptions into concrete Kubernetes workloads, handling conflicts and resource constraints.*  
   *Evidence:* Python node interfacing to Kubernetes API; discussion of validation, reconfig, shut‑down semantics.  

4. **Claim:** *Demonstration of a cloud‑based recording application triggered by vehicle proximity.*  
   *Evidence:* Experimental setup, Table II, Figure 2, latency data.  

5. **Claim:** *Providing publicly available code and data enables community reproducibility.*  
   *Evidence:* GitHub repo link, Docker compose, KinD configuration.  

---

### Definitions & Key Terms  

| Term | Definition | Context |
|------|-------------|---------|
| **Containerization** | Encapsulating an application and its dependencies into an isolated runtime unit. | Enables portable micro‑services. |
| **Micro‑service architecture** | Decomposing software into fine‑grained, loosely coupled services communicating via predefined protocols. | Main software paradigm in CPS. |
| **Kubernetes** | Open‑source orchestrator that manages pods, services, and lifecycle across a cluster. | Provides scaling, recovery, and deployment features. |
| **Robot Operating System (ROS)** | Middleware and libraries for robotic software development. | ROS 2 provides DDS by default. |
| **C‑ITS** | Cooperative Intelligent Transport Systems – vehicular CPS with traffic control, roadside units, etc. | Target domain of RobotKube. |
| **Event detector** | Component that buffers recent ROS messages and checks them against user‑defined patterns to raise events. | Core of operator logic. |
| **Action plugin** | Sub‑component of event detector that transforms an event into a concrete action (e.g., trigger deployment). | Decouples detection from execution. |
| **Application manager** | Translates a task description into a Kubernetes workload; manages deployment/reconfig of applications. | Bridge to control plane. |
| **DAG (Directed Acyclic Graph)** | Not explicitly used but underlying CI; Kubernetes uses DAG for pod scheduling. |
| **Cluster reconciliation** | Process by which Kubernetes brings the actual cluster state in line with the desired specification. | Main source of latency. |
| **DevOps** | Continuous integration/continuous delivery practice that RobotKube supports via dynamic deployment. | “Software‑defined vehicles.” |
| **Collective Learning (CL)** | Training models on data gathered across the fleet without human labelling. | Motivation for recording. |

---

### Important Figures & Tables  

- **Figure 1 (not reproduced)** – Overall system architecture of RobotKube (operator applications, recording applications, application manager, event detector, Kubernetes, ROS).  
- **Figure 2** – Software components in experimental cluster: poses → MQTT → Cloud (event detector) → application manager → K8s → MQTT clients & recording → MongoDB.  
- **Table I (abstracted)** – Design principles of RobotKube and operator applications.  
- **Table II** – Parametric configuration of the use‑case (vehicles, frequencies, trigger distances).  

---

### Limitations & Open Questions  

- **Latency** – The 5 s cluster reconciliation is measured on a local KinD cluster; real edge/cloud deployments may experience higher latencies.  
- **Scalability** – No formal evaluation of workload scaling; future work should benchmark Kubernetes behaviour under hundreds of nodes/vehicles.  
- **Real‑time Constraints** – The current framework is not designed for hard real‑time safety‑critical functions; how to integrate with dedicated real‑time scheduling is an open question.  
- **Network Reliability** – MQTT used in demonstration; robustness under lossy wireless links remains untested.  
- **Pre‑deployment Strategies** – Research into “warm start” or idle containers to reduce latency.  

---

### References to Original Sections (if available)  

| Claim/Result | Related Section |
|---------------|------------------|
| Event detector structure | III.A |
| Action plugin concept | III.A |
| Application manager description | III.B |
| Kubernetes reconciliation latency | V |
| Example use‑case description | IV |
| Experimental parameters | Table II |
| Design principles table | Table I |
| Figure 1 description | Fig. 1 |
| Figure 2 description | Fig. 2 |
| GitHub repo | “We publish … at github.com/ika‑rwth‑aachen/robotkube” (V) |

---

### Executive Summary / Key Takeaways (optional)  

- RobotKube extends standard Kubernetes with ROS‑based event‑driven operator logic.  
- Two core components – Event Detector and Application Manager – allow on‑demand deployment, reconfiguration, and shutdown of micro‑service containers.  
- A representative C‑ITS use‑case demonstrates automatic deployment of a recording application when two lidar‑equiped vehicles approach each other.  
- Latency measurements reveal 5 s reconciliation overhead as key bottleneck; pre‑launching or smarter idle containers could mitigate it.  
- All code and data are publicly available, enabling community adaptation and reproducibility.  

---  

### Supplementary Material (if present)  

- **GitHub repository**: `github.com/ika-rwth-aachen/robotkube` – contains Dockerfiles, Kubernetes manifests, ROS nodes, and ROS bag data.  
- **Docker images**: Pre‑built container images for event detector, application manager, recording application, MQTT bridge nodes.  
- **KinD configuration**: YAML cluster set‑up used in the experiments.  

---  

*End of Summary*