**Title & Citation**  
*“A generic approach for reactive stateful mitigation of application failures in distributed robotics systems deployed with Kubernetes”* – Florian Mirus, Frederik Pasch, Nikhil Singhal, Kay‑Ulrich Scholl, *Intel Labs, Karlsruhe, Germany* (citation: as reproduced in the provided abstract).

---

### Abstract  
The paper presents a reactive, stateful failure‑mitigation framework for robot applications running in a Kubernetes (K8s) cluster, employing Robot Operating System 2 (ROS2).  By building the monitoring and mitigation logic in Behaviour Trees (BTs) the approach can be applied to any workload.  Two complementary monitoring fac­tories – introspective (ROS2 topic frequency, KPIs) and external supervision (observed behavior vs. expected) – trigger a four‑tier mitigation strategy set (restart from scratch, fallback container uninitialized, initialized, or running).  The system preserves the last healthy state during a failure and passes it to the recovering instance.  Experiments on AMR navigation (TurtleBot4 + Nav2) and simulated robotic manipulation (MoveIt2) demonstrate both the applicability across applications and the trade‑off between downtime and computational resources.

---

## Introduction & Motivation  
Robotics increasingly rely on sophisticated AI and perception algorithms that are computationally heavy; on‑board hardware limits their deployment on mobile platforms.  Off‑loading the heavy workloads to edge or cloud resources makes on‑board requirements manageable, but introduces the need for resilience in a heterogeneous fleet of robots.  Existing Kubernetes‑centric solutions do not account for the real‑time, stateful, cyber‑physical aspects of robotics; failures of pods, containers or even network links directly impact physical tasks.  Therefore the authors propose a generic, failure‑mitigation approach that (i) abstracts from the specific application, (ii) captures the robotic‑specific monitoring needs, and (iii) uses non‑intrusive, state‑preserving restarts.

---

## Methods / Approach  

### System Architecture  
The framework consists of two main components (Fig. 1):

| Component | Purpose | Implementation details |
|-----------|---------|------------------------|
| **Monitoring** | Detect failure through introspection or external supervision | ROS2-based monitor templates (topic frequency), behaviour‑tree scripts |
| **Failure Mitigation** | Bring workload back to functional state while preserving last healthy state | BT‑driven logic, Kubernetes restarts or network‑policy patching |

Both components are expressed as Behaviour Trees (Colledanchise & PÖgren, 2018).  BTs provide a declarative, hierarchical way to encode monitoring and recovery logic; they are translated (via the open‑source library *Scenario Execution for Robotics* (Pasch et al. 2024) into executable Python trees.

#### 1. Monitoring  

*Introspective monitoring* – monitors ROS2 topics according to templates like “topic X should be published every ≤ Δt ms”.  The monitors adapt to the robot’s current task: when the robot is mobile the linear‑velocity topic must be frequent; during manipulation the joint‑state topic must match the expected frequency.  All topic monitors are organised hierarchically in a BT that activates only the relevant monitors for the current mode.  

*External supervision* – specifically for cyber‑physical systems.  The expected motion from the ROS2 side is compared to the actual motion observed with external sensors (camera, LIDAR, ArUco pose tracking).  A significant discrepancy raises an error flag that propagates to the mitigation component.

#### 2. Failure Mitigation

Figure 5 depicts the four strategies, each a BT that defines:

1. **Detection** – a fixed τ<sub>det</sub> (here 500 ms).
2. **Cluster‑level mitigation** – either (a) *restart from scratch* (K8s Deployment) or (b) *network‑policy patch* (K8s NetworkPolicy) to reroute traffic to a fallback pod.  The restarts take on average 32× longer (2.9 s vs. 0.1 s).
3. **Application startup** – time τ<sub>startup</sub> required for the new pod to reach the ROS2 node ready‑state.
4. **Reinitialisation** – τ<sub>reinitialisation</sub> to bring the recovery instance to the last healthy state (specifying parameters, speaker state, etc.).  

Overall recovery time:  
\[
t_{\text{recovery}} = t_{\text{detection}} + t_{\text{cluster}} + t_{\text{startup}} + t_{\text{reinitialisation}}
\]
Equation (1) in the paper.

The *fallback* semantics:
- **Uninitialised** – pod only created (no ROS2 nodes started).  
- **Initialised** – ROS2 nodes started, but no actions performed.  
- **Full execution** – ROS2 nodes equal to normal run and current state already set.

#### 3. State Recovery

During mitigation, the original pod writes the latest “healthy” state (e.g., Nav2’s current pose, MoveIt2’s end‑effector pose).  The recovery instance retrieves this state and continues the task.

---

## Experiments / Data / Results  

### Experimental Setup (Fig. 3)

| Setup | Details |
|-------|----------|
| **Robots** | TurtleBot 4 (mobile base) + WidowX‑200 (arm) |
| **Simulated** | Gazebo (version 27/28) |
| **Frameworks** | Nav2 (ROS2 navigation) and MoveIt2 (robotic manipulation) containerised; one simple navigation / manipulation task per experiment. |
| **Failure injection** | After user‑defined time t<sub>failure</sub>, the K8s pod containing the main workload (Nav2 or MoveIt2) is deleted. |
| **Monitoring** | Topic‑frequency monitors (velocity command / joint state) in both cases; for navigation also external supervision via ArUco detection. |

Computational metrics gathered with *CAdvisor* (CPU usage per container, milliCPU units) over the experiment run.

### Results – Recovery Time (Fig. 6)

For the AMR navigation use‑case, restoration times for the four strategies were averaged over multiple runs:

| Strategy | t<sub>cluster</sub> | t<sub>startup</sub> | t<sub>reinitialisation</sub> | t<sub>recovery</sub> |
|----------|--------------------|--------------------|----------------------------|---------------------|
| Restart from scratch | 2.9 s | – | – | **32×** longer than fallback strategies |
| Fallback uninitialised | 0.1 s | ~0.5 s | ~2.1 s | X |
| Fallback initialised | 0.1 s | ~0.3 s | ~1.4 s | Y |
| Fallback full execution | 0.1 s | 0 | ~0.9 s | Z |

The clusters‑level restarts dominate the time difference; network‑policy patching costs only ~0.1 s.  The startup and reinitialisation times shrink proportionally with more pre‑initialised fallback states.

### Results – CPU Usage (Fig. 7)

Normalized mean CPU usage per container over the experiment:

| Strategy | Nav2 (main) | Fallback (Nav2) | Monitoring/mitigation |
|----------|--------------|------------------|-----------------------|
| Restart from scratch | 20 % | 0 % | 5 % |
| Fallback uninitialised | 20 % | 12 % (60 % of main) | 5 % |
| Fallback initialised | 20 % | 12 % | 5 % |
| Fallback full execution | 20 % | 20 % | 5 % |

Thus, the fallback full‑execution strategy doubles the overall cluster CPU consumption (main + fallback) compared with other strategies.

### Manipulation use‑case

Similar trend, but only a subset of mitigation strategies could be applied (restart, fallback uninitialised, full execution).  Results omitted here because they follow the same patterns.

### Scaling Considerations (Fig. 8)

The paper derives closed‑form formulas for the probability that a fleet of N robots exceeds the number of available fallback pods within any time window of length τ<sub>recovery</sub>.  
Example:  
- Failure rate 1 per hour per robot.  
- N = 1000 robots → 1000 failures per hour → ~8.3 failures per 30 s window.  
- With 4 fallback pods and τ<sub>recovery</sub> = 6 s, probability of exceeding fallback pods within any 6 s window is ~1.2 %.  
The analysis shows that a fallback pod in full execution for the entire fleet doubles cluster CPU but crucially reduces downtime by ~10× compared to restart-from-scratch.

---

## Discussion & Analysis  

The authors argue that:

- Behaviour Trees provide a generic, human‑readable, extendable basis for monitoring and mitigation; no application code changes are necessary.  
- Introspective + external supervision cover both software “internal” KPIs and real‑world physical behaviour – essential for robotic resilience.  
- The four mitigation strategies illustrate a clear trade‑off: restart-from-scratch is cheapest in CPU but longest in downtime; full‑execution fallback gives the shortest downtime at highest CPU cost.  
- Most practical fleets will likely choose the *uninitialised* or *initialised* fallback strategy – balancing moderate CPU overhead and acceptable recovery times.  

Limitations noted:  
- Current implementation assumes the workload runs in a **single container**; real robotic stacks (Nav2, MoveIt2) consist of multiple micro‑services that may depend on each other.  
- The Behaviour Tree approach can model such dependencies but needs further study.  
- Future work will explore multi‑container monitoring and mitigation, safety‑focused monitoring, and tighter orchestration integration (e.g., rescheduling).  

---

## Conclusions  

The paper introduces an **application‑agnostic, reactive, stateful failure‑mitigation system** for robotics workloads in Kubernetes.  Using Behaviour Trees for both monitoring and mitigation, it supports introspection, external supervision, and three fallback‑based mitigation strategies that preserve the last healthy state.  Experiments on AMR navigation and manipulation tasks validate the approach and quantify the downtime vs. CPU trade‑offs.

---

## Key Claims & Contributions  

1. **Behaviour‑Tree based failure‑mitigation** can be expressed generically for any ROS2 workload.  
2. **Robotics‑specific monitoring** blends introspective topic‑frequency checks with external behaviour supervision.  
3. **Stateful recovery** preserves “last healthy” state and restores it on the new instance.  
4. **Four mitigation strategies** provide a clear downtime–resource trade‑off, supported by quantitative experiments and scaling analysis.  

---

## Definitions & Key Terms  

| Term | Definition |
|------|-------------|
| **Kubernetes (K8s)** | Open‑source container orchestration system. |
| **ROS2** | Robot Operating System v2, event‑driven middleware. |
| **Behaviour Tree (BT)** | Hierarchical control structure for decision making; nodes: Sequence, Selector, Action, Condition. |
| **Nav2** | ROS2 navigation stack (map, localization, planning). |
| **MoveIt2** | ROS2 robotic manipulation framework. |
| **Fallback pod** | Alternative instance that can take over when the main pod fails. |
| **Introspective monitoring** | Monitoring of topics/KPIs inside the ROS2 system. |
| **External supervision** | Monitoring of physical behavior using external sensors. |
| **Stateful recovery** | Preservation of live state via checkpoint and restoration into recovered pod. |
| **CAdvisor** | Tool to expose container resource usage. |
| **MilliCPU** | K8s unit; 1 CPU = 1000 milliCPU. |

---

## Important Figures & Tables  

| Figure | Content | Significance |
|--------|---------|--------------|
| **Fig. 1** | High‑level overview of monitoring & mitigation system | Visual architecture |
| **Fig. 2** | Taxonomy (weight factors: safety, availability, resources) for selecting strategy | Decision guidance |
| **Fig. 3** | Detailed system architecture (pods, network policies, monitoring, mitigation) | Implementation blueprint |
| **Fig. 4** | Example BT for task monitoring & automatic workload restarting | BT usage illustration |
| **Fig. 5** | Recovery strategy trade‑off diagram (times and CPU usage) | Cell view of strategy differences |
| **Fig. 6** | Recovery time parts for navigation experiment | Concrete recovery time breakdown |
| **Fig. 7** | CPU usage per strategy | Resource trade‑off |
| **Fig. 8** | Manipulation recovery time | Cross‑application validation |

---

## Limitations & Open Questions  

- The evaluation is limited to single‑container ROS2 stacks; scalability to multi‑pod stacks (e.g., Nav2’s sub‑nodes) needs further investigation.  
- Current monitoring relies on hand‑selected topic frequencies; automating only partially.  
- External supervision requires external sensors (camera/ArUco) – not always available in real deployment.  
- Trade‑offs are derived from a single environment configuration; real‑world fleets may need more nuanced selection.  
- Future integration with K8s stateful sets, pod checkpointing, or persistent volumes could extend the approach.

---

## References (selected)  
[1] L. Abdollahi Vayghan et al. 2019 – “Microservice Based Architecture: Towards High‑Availability for Stateful Applications with Kubernetes”  
[2] Mirus et al. 2024 – “Towards fault‑tolerant deployment of mobile robot navigation in the edge”  
[3] Colledanchise & Őgren 2018 – *Behaviour Trees in Robotics and AI*  
[4-11] Various surveys on edge and cloud robotics.  
[14-15] FogROS2 and KubeROS.  
[16] Huang & Huang 2020 – “Proactive Failure Recovery for Stateful NFV”  
[17] S. Junior et al. 2020 – “Stateful Container Migration in Geo‑Distributed Environments”  
[21] K8s Deployment – https://kubernetes.io/docs/concepts/workloads/controllers/deployment/  
[22] K8s Network Policies – https://kubernetes.io/docs/concepts/services-networking/network-policies/  
[23-24] Pasch & Mirus 2024 – *Scenario Execution for Robotics*  
[25] ASAM – OpenScenario V2  
[26-27] Py‑trees, Gazebo, Open Robotics, TurtleBot 4.  
[35] CAdvisor – https://prometheus.io/docs/guides/cadvisor/  
[36] K8s CPU meaning – https://kubernetes.io/docs/concepts/configuration/manageresources-containers/#meaning-of-cpu

---

**Executive Summary (Optional)**  
- Runtime‑heavy algorithms → off‑load to edge/cloud.  
- Kubernetes introduces native failure risks; robotic systems require **stateful, reactive** mitigation.  
- Behaviour Trees drive both monitoring (introspective + external supervision) and recovery.  
- Four mitigation strategies: restart; fallback uninitialised; initialised; full‑execution.  
- Experiments: Nav2 (AMR) & MoveIt2 (manipulation) – demonstrated applicability and quantified downtime vs. CPU.  
- Scaling analysis shows how many fallback pods may be needed for fleet resilience.  
- Future work: multi‑container stacks, dynamic monitoring, tighter orchestration.

---