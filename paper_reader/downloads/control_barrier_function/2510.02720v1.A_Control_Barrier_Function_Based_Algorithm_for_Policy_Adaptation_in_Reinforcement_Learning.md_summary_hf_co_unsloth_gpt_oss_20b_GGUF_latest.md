**Title & Citation**  
*A Control-Barrier-Function-Based Algorithm for Policy Adaptation in Reinforcement Learning*  
Wenjian Hao, Zehui Lu, Nicolas Miguel, Shaoshuai Mou – Purdue University. Conference‑style paper (full URL not provided; video demo available at https://youtu.be/iYwDWh-JNkU).

----------------------------------------------------------------

### Abstract  
The authors formulate the problem of adapting a pre‑designed RL policy (parameter vector θ) to a new objective J(θ) while limiting deviation from the original cost G(θ). The problem becomes a constrained optimisation:
\[
\min_{\theta,c}\; J(\theta)+w\,c^2\quad\text{s.t.}\; G(\theta)\le G(\theta^G)+c,\;c\ge0,
\]
where θ^G is a known minimiser of G. To solve it they design a closed‑loop system for the policy parameters, with a controller that adjusts the gradient of the additional cost so the constraint is never violated. The dynamics use control‑barrier‑functions (CBF) to guarantee constraint satisfaction. Numerical experiments on OpenAI Gym Cartpole and Lunar Lander and on a quadruped robot demonstrate the method’s effectiveness.

----------------------------------------------------------------

### Introduction & Motivation  
- RL is popular for robotics but often requires many episodes; when task specs change, retraining from scratch is costly.  
- Existing transfer‑learning (RL‑TL) or policy‑distillation methods simply re‑train or adapt the policy with new reward terms but can overshoot or degrade the original objective.  
- Control‑barrier‑functions (CBFs) provide safety guarantees for dynamics; prior work used them in collision avoidance and most recent paper [9] used CBFs for constrained optimisation.  
- The authors propose to treat policy‑adaptation as a constrained optimisation over parameters of a pre‑optimised policy.  
- Side‑by‑side with existing methods: (a) “nearest‑optimal” transfer, (b) “gradient adaptation with penalty”, (c) “CMBO‑like QP”.

----------------------------------------------------------------

### Methods / Approach  

#### 1) Formal Problem  
- Parameters θ∈ℝ^p.  
- Original objective G(θ).  
- Additional objective J(θ).  
- Known minimiser θ^G of G.  
- Introduce relaxation variable c≥0.  
- Constrained optimisation (2): \min_{θ,c} J(θ)+w c^2  such that G(θ)≤G(θ^G)+c.  
- w trades off the importance of staying close to G vs minimising J.

#### 2) Closed‑loop Parameter Dynamics  
- Proposed continuous‑time dynamics:
\[
\dot θ = -∇_θ J(θ) + a(θ),\qquad
\dot c = ψ(θ) \le 0,
\]
- a(θ) is a controller (to be defined) that enforces the constraint (1).  
- ψ(θ) = -½ (G(θ)-G(θ^G)-c)² is a stabilising dynamics for c.  

#### 3) Control‑Barrier‑Function (CBF) Framework  
- Define B(θ)=h(θ)+c, with h(θ)=G(θ)-G(θ^G).  
- B is a CBF if ∂B/∂θ⋅(-∇_θ J(θ)+a(θ)) + κ(B(θ)) ≥ 0 for some class‑κ, e.g. κ(z)=γ_h z, γ_h>0.  
- CBF property ensures forward invariance of set C_{θ,c} = {θ | h(θ)≤c}.  
- Hence if θ(0)=θ^G∈C_{θ,c}, and a(θ) satisfies the CBF inequality, then constraint (1) holds for all time.

#### 4) Quadratic‑Programming (QP) to obtain a(θ) and c\\*  
- Formulate minimisation:
\[
\min_{a,c}\;\|a\|^2 + (c-c^0)^2
\]
subject to: a⋅∇_θ h(θ) + \frac12\|∇_θ h(θ)\|^2 + κ(h(θ)+c) ≤ 0,\;c≥0.  
- The closed‑form solution (Lemma 1):
\[
a(θ)=\frac{[\hat a^-]_{\mathcal A}+ \left\{\frac12 \hat R - \hat s,_-\right\}\nabla_θ h(θ),
\]
where \(\hat a^-\) is the unconstrained descent direction \(-∇_θ J(θ)\), \(\hat R = \nabla_θ h(θ)^T[\mathcal A]^{-1}\nabla_θ h(θ)\) and \(\hat s,_- = \min\{0,\hat s\}\), with \(\hat s=\gamma_h(h(θ)+c)-\frac12\hat R\).  
- Minimal relaxation:
\[
c^* = \max\{0,\; -h(θ)- (\hat s,_-)/\gamma_h\}.
\]

#### 5) Discrete‑time implementation  
- In practice, the actor‑critic algorithm uses Euler discretisation:  
\[
θ_{k+1}=θ_k + α_μ a(θ_k),
\]
- Lemma 3 (cf. [40]) argues: if a(θ) satisfies a tightened CBF inequality \(\mathcal L a + \kappa(h(θ)+c)(1-\alpha_μ\|L g\|)≥0\) then discrete‑time invariance still holds.

#### 6) RL‑Specific Actor‑Critic Adaptation  
- Starting from pre‑trained actor µ(·,θ̂^µ) and critic ˆQ(·,·,θ̂^Q) obtained via DDPG.  
- New critic Q(·,·,θ^Q) and actor µ(·,θ^µ) are initialized with θ^µ=θ̂^µ.  
- Iterative updates:
\[
θ^Q_{k+1} \gets θ^Q_k + α_Q ∇_{θ^Q}\mathcal L_Q(θ^Q,θ^µ_k),\quad
θ^µ_{k+1} \gets θ^µ_k + α_µ a(θ^µ_k),
\]
with α_µ=εα_Q.  
- Each episode collects experience tuples (x_t,u_t,x_{t+1}) with exploration noise; critical loss \(\mathcal L_Q\) and actor loss derived from the Q‑function.

----------------------------------------------------------------

### Experiments / Data / Results  

#### 1) Illustrative 2‑D Example  
- θ = [x,y]^T, J(θ)=sin(x)+(y-8)², G(θ)=x³+y³, with non‑negative x,y.  
- θ^G solves G(θ)=0 along line [-x, x] and is known.  
- Three methods compared: standard gradient descent (GD), multi‑objective gradient descent with penalty (MOGD), and the proposed CBF‑PA.  
- Performance measured: J(θ_k), G(θ_k), average G over iterations.  
- Findings: CBF‑PA behaves like MOGD; decreases J faster for small w; when w large, trajectories converge close to joint optimum of J and G.  
- Sensitivity to γ_h and α observed: smaller γ_h speeds convergence; large α gives faster but may overshoot.

#### 2) Cartpole (OpenAI Gym)  
- Dynamics with state x=[p_x,ṗ_x,θ,θ̇], control u∈[-1,1].  
- Original cost: keep pole upright (stage cost ϕ̂) + penalty for large θ.  
- Additional cost: bring cart to position p_x=2: cost ϕ.  
- Baselines: Multi‑objective RL (ϕ_MORL = ϕ̂ + wϕ) and behaviour‑cloning (ϕ_BC = ϕ̂ + w||µ - µ̂||²).  
- Hyperparameters: w=0.01, α=0.001.  
- Results:  
  * Fig 6a: Episode costs – CBF‑PA matches DDPG on original cost ~‑499.  
  * Fig 6b: Additional cost is smaller for CBF‑PA than baselines.  
  * Fig 7a: Performance (pole upright) comparable across all methods.  
  * Fig 7b: Additional cost significantly lower for CBF‑PA.  
  * Statistical ANOVA: p<0.05 for additional cost; Tukey post‑hoc: CBF‑PA < BC = MORL = pretrained.

#### 3) Lunar Lander (OpenAI Gym)  
- State dims 8, control u∈[-1,1]².  
- Original cost: soft‑touchdown at (0,0).  
- Additional cost: penalise energy u² during landing.  
- Baselines same as Cartpole.  
- Hyperparameters: same as above.  
- Results:  
  * Fig 8: Episode reward (original) similar; additional cost convergence faster for CBF‑PA.  
  * Fig 9a: All methods achieve landing.  
  * Fig 9b: Additional cost lower for CBF‑PA; statistical tests with ANOVA and Tukey identical to Cartpole.

#### 4) Real‑world Quadruped  
- Kinematics: unicycle model x=[p_x,p_y,θ], u=[u_v,u_ω], control limits ±1.  
- Two adaptation experiments:
  1. From pre‑trained obstacle‑avoidance + goal‑reach to new target position 1.5 m.  
  2. After obtaining above, add new obstacle at (0.5,0.2).  
- Stage costs: pure penalisation for collisions (distance < threshold).  
- Additional stage costs: distance to new target or energy term.  
- Results:  
  * Fig 10: Trajectories shown.  
  * Fig 11: Time‑lapse and actual trajectories of pre‑trained, during‑training at episode 90, and adapted policies. Adapted policy reaches new goal while still avoiding obstacles.  
  * Fig 12b,c: Trajectories during training and final pose.  
  * Fig 13: Demonstrated generalizability with different initial positions.  
  * Fig 14–15: Show second adaptation (added obstacle) preserving both previous and new safety constraints.  
  * Video (not reproduced) demonstrates success.

----------------------------------------------------------------

### Discussion & Analysis  

- **Trade‑off**: The relaxation variable c allows algorithm to find a feasible solution even when J(θ) and G(θ) have disjoint minima.  
- **CBF advantage**: By constructing a bilevel QP, the controller a(θ) automatically ensures the constraint; no need to explicitly compute gradients of G (only its gradient is required).  
- **Closed‑loop benefit**: Instead of repeatedly solving constrained optimisation from scratch, a(θ) acts like a feedback controller; computationally efficient (only matrix operations each iteration).  
- **Hyper‑parameter Role**:  
  * w: larger → tighter around G; smaller → more aggressive improvement on J.  
  * γ_h: more aggressive CBF derivative in inequality → faster adaptation but risk of numeric instability.  
  * α: controls learning speed; too small → slow; too large could break constraint due to discretisation (addressed by Lemma 3).  
- **Comparison with TL**: Behavior‑cloning and MORL share same actor update; they roughly recover same original behaviour because they “freeze” the pre‑tested policy for more steps; CBF‑PA naturally balances both objectives.  
- **Stability Guarantees**: Theorems 1+2 prove that if initial θ∈C_{θ,c∗} then for all t≥0, h(θ(t))≤c∗ and thus G(θ(t))≤G(θ^G)+c∗.  

----------------------------------------------------------------

### Conclusions  
1. Proposed CBF‑PA properly formulations policy adaptation as a constrained optimisation over parameters, using a relaxation variable to handle incompatible objectives.  
2. Closed‑loop dynamics + CBF‑formulated QP give analytic a(θ), c∗, guaranteeing constraint satisfaction.  
3. Empirical results confirm that the algorithm achieves lower additional cost while maintaining original performance, outperforming baseline RL transfer methods.  
4. Real‑world demonstration on a quadruped robot show that CBF‑PA can sequentially adapt to new goals and obstacles without violating constraints.  

### Key Claims & Contributions  
- **Claim 1**: Policy adaptation can be cast as minimisation of additional objective subject to a linear‑in‑h(θ) constraint. *Evidence:* Problem (2) & derivations.  
- **Claim 2**: A continuous‑time controller derived from a CBF‑based QP ensures forward invariance of G(θ)≤G(θ^G)+c* for all time. *Evidence:* Lemma 1, Theorem 1.  
- **Claim 3**: Discrete‑time implementation preserves constraint under a tightened CBF inequality (Lemma 3). *Evidence:* Result from Lemma 3 citing [40].  
- **Claim 4**: In robotics tasks, CBF‑PA produces lower additional cost and comparable original cost compared to MORL and BC. *Evidence:* Cartpole, Lunar Lander results.  
- **Claim 5**: Real‑world quadruped experiments show successful sequential adaptation. *Evidence:* Fig. 11–15.

### Definitions & Key Terms  

| Term | Definition | Source |
|------|-------------|---------|
| **θ** | Policy parameter vector | Section II, A |
| **G(θ)** | Original objective (stage cost expectation) | Eq. (5) |
| **J(θ)** | Additional objective (new task stage cost) | Eq. (8) |
| **θ^G** | Known minimiser of G | Eq. (1) definition |
| **h(θ)** | Constraint residual: h(θ)=G(θ)−G(θ^G) | Eq. (1) |
| **c** | Relaxation (deviation) variable | Section II, A |
| **a(θ)** | Closed‑loop controller that adjusts gradient | Eq. (9) |
| **ψ(θ)** | Dynamics for c (stabilising) | Eq. (9) |
| **CBF** | Function B(θ) satisfying B≥0 & Lie‑derivative≥0 | Definition 1 (Sec. III A) |
| **γ_h** | Class‑K function parameter for B | Def. 1 assumption |
| **L_f, L_g** | Lie derivatives of h(θ) w.r.t. dynamics | Eq. (15) |
| **a(θ) (closed‑form)** | Expression from Lemma 1 | Eq. (18) |
| **c^*** | Minimal relaxation from QP | Eq. (19) |

### Important Figures & Tables  

| Figure | Content | Significance |
|--------|----------|---------------|
| 1 | Pre‑trained vs adapted trajectories in obstacle‑avoidance / goal‑reach | Visualise adaptation benefits |
| 2 | Contour of G(x,y) & behavior of CBF‑PA for various w | Demonstrates trade‑off |
| 3 | J vs w| Shows how J improves as w ↓ |
| 4 | Sensitivity of γ_h | Shows effect on convergence |
| 5 | Sensitivity of α | Shows step‑size influence |
| 6 | Cartpole episode costs (original & additional) | Quantitative comparison |
| 7 | Cartpole test performance curves | Additional task gain |
| 8 | Lunar Lander episode rewards/costs | Similar evidence |
| 9 | Lunar Lander test performance | Additional cost decrease |
| 10 | Quadruped kinematic model | Setup illustration |
| 11 | Time‑lapse / real trajectories for robot – pre‑trained, training, adapted | Real‑world adaptation process |
| 12 | Demonstration trajectories (Figure 12) | Visual evidence of improved path |
| 13 | Transferability (different initial states) | Generalizability claim |
| 14–15 | Second adaptation (new obstacle) | Sequential adaptation capability |

(Exact figure numbers approximate; equivalents in the paper.)

### Limitations & Open Questions  

1. **Noise Sensitivity** – actor‑critic RL uses exploration noise which can interfere with the tight constraint; authors note this as shortcoming.  
2. **Non‑convex critic/actor** – multiple local optima may affect convergence; algorithm assumes gradient descent works well.  
3. **Hyper‑parameter tuning** – γ_h, w, α may require problem‑specific tuning; no systematic method provided.  
4. **High‑dimensional dynamics** – while matrix operations are cheap, for very large p the QP solution may still be heavy; no evaluation on extremely high‑dim models.  
5. **Theoretical extension to stochastic dynamics** – the CBF invariance proof assumes deterministic h(θ) and continuous‑time; discrete‑time lemma gives some safety guarantees but still empirical.  

### References to Original Sections  

| Section | Reference |
|---------|-----------|
| I | Intro motivation |
| II A | Problem formulation |
| III A,B | CBF properties & QP set |
| III C,D | Lemma 1, 2, Theorem 1 proof |
| IV A–C | Experiment design & results |
| V   | Hardware demonstration |
| VI  | Discussion, limitations |
| Appendix | Proofs for Lemma 1–3 |

----------------------------------------------------------------

### Executive Summary (Key Take‑aways)  

- Policy adaptation as constrained optimisation over known‐optimal parameters with CBF‑based QP controller provides analytic update guaranteeing bounded deviation and near‑optimality.  
- Closed‑loop dynamics reduce computational cost relative to solving a large optimisation each step.  
- Experiments on simulated and real robotic platforms show that CBF‑PA matches or outperforms multi‑objective RL and behaviour‑cloning while preserving the primary objective.  
- Method handles additional tasks sequentially and can restore constraints in high‑dim state spaces.  

---  

*End of Summary*