## A Control-Barrier-Function-Based Algorithm for Policy Adaptation in Reinforcement Learning

Wenjian Hao, Zehui Lu, Nicolas Miguel, Shaoshuai Mou

Abstract -This paper considers the problem of adapting a predesigned policy, represented by a parameterized function class, from a solution that minimizes a given original cost function to a trade-off solution between minimizing the original objective and an additional cost function. The problem is formulated as a constrained optimization problem, where deviations from the optimal value of the original cost are explicitly constrained. To solve it, we develop a closed-loop system that governs the evolution of the policy parameters, with a closed-loop controller designed to adjust the additional cost gradient to ensure the satisfaction of the constraint. The resulting closed-loop system, termed control-barrier-function-based policy adaptation, exploits the set-invariance property of control barrier functions to guarantee constraint satisfaction. The effectiveness of the proposed method is demonstrated through numerical experiments on the Cartpole and Lunar Lander benchmarks from OpenAI Gym, as well as a quadruped robot, thereby illustrating both its practicality and potential for real-world policy adaptation.

## I. INTRODUCTION

R EINFORCEMENT learning (RL) has recently gained significant attention in robotics due to the growing complexity of robotic systems [1]-[4] and the challenges posed by robotic environments. For instance, interactions in manipulation and locomotion tasks are difficult to predict, and the intricate dynamics of contact and friction forces are challenging to model accurately [5]. To address these challenges, RL offers a model-free way for learning optimal policies that maximize cumulative rewards (or equivalently minimize cumulative stage costs) over time, without requiring explicit knowledge of the system dynamics.

In standard RL, robots learn policies through trial-and-error interactions with the environment, commonly implemented using an actor-critic framework [6], where the critic evaluates policy performance and the actor updates the policy parameters accordingly. However, training optimal policies from scratch in high-dimensional, nonlinear robotic systems often demands a large number of training episodes. Moreover, when task specifications change or new objectives are introduced, retraining from scratch may significantly increase both time and computational costs. These challenges highlight the importance of efficient policy adaptation methods that enable robots to achieve new objectives while leveraging existing pretrained policies.

Control barrier functions (CBFs) were introduced in response to the growing need for feasible safety-critical controller design [7]. Traditional safety-critical optimal control (OC)

The authors are with the School of Aeronautics and Astronautics, Purdue University, West Lafayette, IN 47907, USA. { hao93, lu846, nmiguel, mous } @purdue.edu .

The experimental video is available at https://youtu.be/iYwDWh-JNkU.

<!-- image -->

(b) Pretrained policy: equivalently adapted policy 1, obstacle 1 avoidance + goal reaching (blue trajectory). Adapted policy 2: obstacle 1 avoidance + goal reaching + additional obstacle 2 avoidance (green trajectory).

Fig. 1: A robot executes pretrained policies to perform original tasks. Adapted policies enable the robot to fulfill both original tasks and additional tasks.

methods either fail to account for system dynamics feasibility or incorporate safety via potentially nonlinear constraints. These methods can lead to computational intractability and suffer from the curse of dimensionality [8]. To address these issues, CBFs provide safety guarantees while maintaining computational tractability, leveraging Nagumo's principle of invariance [7]. The application of CBFs in existing research primarily focuses on collision avoidance, where the set of states leading to collisions is defined as the unsafe set, which is then treated as a constraint within a CBF quadratic programming (CBFQP) problem. In those frameworks, the CBF-QP minimally adjusts the reference control input to avoid entering the unsafe set. Consequently, the CBF-QP is typically considered an additional control to a reference closed-loop controller, providing corrections to the reference input when safety risks are detected. While CBFs have become well-established in collision avoidance contexts, their use for optimality guarantees is less explored. Notably, recent work [9] explores the use of CBFs to ensure optimality, utilizing them to design safe gradient flows for solving constrained nonlinear optimization problems.

Motivated by the above challenges of policy adaptation and the promising capabilities of CBFs, this paper presents a closedloop system that governs the dynamics of policy parameters. The proposed method minimizes an additional objective while ensuring near-optimal performance with respect to the original task throughout the learning process. The main contributions are summarized as follows:

- We formulate policy adaptation in RL as a constrained optimization problem over the parameters of a predesigned optimal policy, where the additional objective is minimized subject to constraints on the deviation from the original optimal cost.
- We propose a closed-loop system for solving the constrained problem, in which a closed-loop controller modifies the gradient of the additional objective to ensure bounded deviation while improving efficiency by avoiding repeated solutions of the original optimization problem.

## A. Related Work

This subsection provides an overview of related work on policy adaptation, focusing on the areas of tuning OC systems and transfer learning in RL.

1) Tuning OC Systems: RL and OC both aim to design closed-loop controllers that minimize an objective function [10], [11]. Each relies on a system defined by states and inputs, with a model (plant or environment) describing state transitions. Tuning OC systems means adjusting parameters within the OC framework to minimize an additional loss function and meet extra performance requirements.

Early work in the OC community approached this through neighboring extremal optimal control (NEOC) [12], [13]. NEOC studies how small variations in control preserve optimality when the initial condition of a nonlinear system changes slightly. This is achieved via second-variation theory, which reduces the problem to solving a time-varying linearquadratic control problem along the original optimal trajectory. Subsequent research examined perturbations in other factors, such as changes in parameters within the loss function. For example, [14] addressed time-varying parameters in nonlinear OC and proposed an algorithm to compute gradients of the optimal index with respect to these parameters, reusing precomputed solutions. Later work, such as [15], studied how larger parameter shifts affect nonlinear OC. Since most of these methods were continuous-time, extending NEOC to discretetime systems became a natural step [16].

Recently, Pontryagin's Differentiable Programming (PDP) [17] differentiates Pontryagin's Maximum Principle (PMP), allowing parameters to appear in controls, objectives, and dynamics. Building on PDP, a cooperative tuning method is proposed for multi-agent systems, where optimal trajectories are tuned in a distributed fashion [18]. Although NEOC and PMP-based approaches are effective, they require knowledge of the full optimal trajectory-a significant limitation in robotics, where tasks often have infinite horizons or high complexity. They also involve solving extra linear-quadratic problems, which become intractable in high-dimensional systems. To overcome these challenges, an online control-informed learning framework was recently introduced [19]. Unlike trajectorybased methods, it incrementally tunes OC systems using single data points in a recursive fashion.

2) Transfer Learning: Transfer Learning (TL), originally introduced in educational psychology [20], is a machine learning paradigm where knowledge from one domain or task improves performance in another. In RL, TL accelerates learning in new but related tasks by leveraging prior experience [21], and knowledge can also be transferred through expert action distributions [22].

Methodologically, TL is commonly categorized into reward shaping, inter-task mapping, representation transfer, and policy transfer [23]. Reward shaping adjusts the reward distribution in the target domain using external knowledge to guide policy learning [24], [25]. A related line of work incrementally adapts reward functions based on human feedback, which can also be viewed as interactive reward shaping [26]. Intertask mapping focuses on mapping state-action spaces between source and target domains [27], [28], while representation transfer leverages shared feature representations such as value functions [29] or Q-functions [30].

Of these, policy transfer is most relevant to this paper. Here, knowledge is encoded in pretrained source-task policies and transferred to the target task. Policy distillation is one approach, where multiple expert policies are distilled into a student policy by minimizing divergence in action distributions [22], [31], [32]. Another is policy reuse, which directly incorporates source policies into target learning [33]. Policy transfer is closely related to imitation learning, as it discourages deviations from the source policy. However, the transferred policy may not always replicate source behavior during training or deployment.

While TL methods primarily transfer knowledge from source tasks, this paper addresses policy adaptation in RL by leveraging the invariant set property of control barrier functions (CBFs), offering a conceptually distinct approach.

The rest of this paper is organized as follows. Section II formulates the problem of interest. Section III presents the proposed algorithm and its associated theoretical guarantees. Section IV discusses the optimization strategy and the selection of key parameters, and further demonstrates the application of the method to policy adaptation in RL through simulation studies. Section V describes hardware experiments that validate the effectiveness of the approach. Section VI concludes the paper and discusses limitations and future directions.

Notations. ∥·∥ denotes the Euclidean norm. Given a matrix A ∈ R n × m , A ′ denotes its transpose.

## II. THE PROBLEM

This section first formulates the problem under consideration and subsequently interprets it as a policy adaptation problem within the reinforcement learning (RL) framework.

## A. Problem Formulation

Let θ ∈ R p denote the vector of tunable parameters. Define G ( θ ) : R n × R m → R and J ( θ ) : R n × R m → R as the original and additional objective functions, respectively, both

assumed to be Lipschitz continuous. Let θ ∗ G ∈ R p denote a given minimizer of G ( θ ) , i.e.,

<!-- formula-not-decoded -->

The problem of interest is to find the optimal parameters of J ( θ ) within a bounded neighborhood near the known reference G ( θ ∗ G ) . Specifically, this neighborhood is formalized as the following constraint:

<!-- formula-not-decoded -->

where c ≥ 0 is a relaxation variable, which measures the tolerance for deviation from G ( θ ∗ G ) . It is introduced to address cases where the optimal solution sets of J and θ ∗ G do not intersect. By allowing a bounded relaxation with respect to G ( θ ∗ G ) , the inclusion of c ensures feasible solutions that balance performance across conflicting objectives.

Accordingly, this paper aims to solve the following constrained optimization problem:

<!-- formula-not-decoded -->

where w ≥ 0 denotes the constant penalty weight for c .

Let ( θ ∗ , c ∗ ) denote the optimal solution of (2). Problem (2) can be interpreted as a policy adaptation problem in RL. When c ∗ = 0 , policy adaptation corresponds to finding θ ∗ that jointly minimizes both J ( θ ) and G ( θ ) while starting from θ ∗ G . When c ∗ &gt; 0 , the solution θ ∗ represents a tradeoff between minimizing J ( θ ) and G ( θ ) , while allowing a maximum relaxation c ∗ of the reference G ( θ ∗ G ) . The weight parameter w governs this trade-off: for sufficiently small w , the objective in (2) emphasizes minimizing J ( θ ) , whereas for sufficiently large w , the emphasis shifts toward minimizing G ( θ ) . Section IV-A provides simulation results that empirically validate this behavior. The detailed connection between (2) and policy adaptation in RL is discussed in the following subsection.

Remark 1: In the context of policy adaptation in RL, the formulation in (2) differs fundamentally from conventional unconstrained multi-objective optimization approaches, which are typically formulated as

<!-- formula-not-decoded -->

This objective incorporates a penalty term to discourage deviation from the reference solution G ( θ ∗ G ) , where θ is often initialized via warm-start parameters θ = θ ∗ G . In contrast, the proposed problem in (2) introduces a relaxation variable c to treat the deviation from G ( θ ∗ G ) as a constraint, while minimizing a quadratic penalty on the maximum deviation, i.e., c 2 . This formulation explicitly characterizes the trade-off between minimizing J ( θ ) and G ( θ ) through the tolerance variable c , which can be regulated using standard constrained optimization methods. As demonstrated in Section IV, for the same penalty weight w , the proposed formulation (2) yields smaller deviations from G ( θ ∗ G ) compared to (3).

## B. Problem Interpretation as Policy Adaptation in RL

We consider a standard RL setup, where an agent interacts with an environment at discrete-time steps t = 0 , 1 , 2 , · · · . At any time t , given a system state x ( t ) ∈ X ⊂ R n , the agent selects the control input u ( t ) ∈ U ⊂ R m according to a parameterized policy/actor:

<!-- formula-not-decoded -->

where µ ( · , θ µ ) : X → U represents a function with a known structure parameterized by θ µ ∈ R p . Upon executing the u ( t ) , the agent moves to a new state x ( t + 1) ∼ P ( ·| x ( t ) , u ( t )) , where P : X ×U → X denotes an unknown Markov transition kernel that describes the evolution of the system states. In the rest of this paper, we denote x t and u t as constant state and input vectors, respectively, to distinguish them from the state and input variables.

1) Policy Adaptation Problem in RL: Objectives of actorcritic algorithms [34] usually involve finding a policy that minimizes the discounted sum of stage costs, i.e.,

<!-- formula-not-decoded -->

where P 0 denotes the distribution of the initial state x ( t ) , u ( s ) is the control input generated from the policy in (4), 0 &lt; ˆ γ ≤ 1 is a discount factor, and ˆ ϕ ( x ( s ) , u ( s )) : X × U → R is the stage cost function.

Since computing G ( θ µ ) in (5) requires knowledge of the unknown transition kernel P , actor-critic methods approximate the optimal policy that minimizes G ( θ µ ) in a model-free manner. This is achieved by defining the action-value function ˆ Q µ ( x t , u t ) for G ( θ µ ) , which quantifies the expected return starting from a fixed state-action pair ( x t , u t ) and subsequently following policy µ in (4). It is defined as

<!-- formula-not-decoded -->

Then, an optimal critic function [35], ˆ Q ( x t , u t , θ ˆ Q ∗ ) , is used to approximate the action-value function, where ˆ Q ( · , · , θ ˆ Q ∗ ) : X × U → R is defined with known structure and constant parameters θ ˆ Q ∗ ∈ R ˆ q , such that

<!-- formula-not-decoded -->

Finally, the optimal policy that minimizes (5) is indirectly obtained by minimizing the critic evaluation:

<!-- formula-not-decoded -->

where ρ µ denotes the known stationary discounted state visitation distribution induced by policy µ .

Given that the optimal policy for minimizing the original objective in (5) has been obtained using actor-critic methods, i.e., the optimal actor µ ( · , θ ˆ µ ∗ ) in (7) is available. The policy adaptation problem in RL seeks to further tune θ ˆ µ ∗ to obtain

a trade-off solution between minimizing the original objective in (5) and a secondary optimization problem defined as:

<!-- formula-not-decoded -->

where 0 &lt; γ ≤ 1 is the discount factor, ϕ ( x ( s ) , u ( s )) : X × U → R denotes the stage cost function, and control input u ( s ) is generated by (4) initialized at θ µ := θ ˆ µ ∗ .

2) Connection Between the Proposed Constrained Optimization and Policy Adaptation in RL: The policy adaptation problem in RL can be cast as the constrained optimization in (2) by setting θ ∗ G := θ ˆ µ ∗ as the parameters of the predesigned optimal policy, θ := θ µ as the adaptable policy parameters, and G ( θ ) and J ( θ ) as the original objective in (5) and the additional objective in (8), respectively. Specifically, this yields the optimization problem

<!-- formula-not-decoded -->

where θ ˆ µ ∗ denotes the pretrained optimal policy parameters obtained from (7), G ( θ µ ) is the original objective defined in (5), and J ( θ µ ) as the secondary objective defined in (8).

## III. MAIN RESULTS

In this section, we first highlight the key challenges and principles underlying the solution of (2). Building on these insights, we introduce a closed-loop dynamical system to address (2) and provide a theoretical analysis establishing constraint satisfaction throughout the optimization process.

## A. Challenges and Key Ideas

Recent studies [9], [36], [37] have explored how solutions to constrained optimization problems can be interpreted as closed-loop control laws acting on the dynamics of the decision variables. This control-oriented perspective models optimization algorithms as closed-loop systems, enabling the application of stability, invariance, and robustness theory to algorithm design.

Motivated by these developments, we consider the following continuous-time parameter dynamics to solve (2):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where a ( θ , c ∗ ) : R p × R → R p denotes a closed-loop controller introduced to enforce the constraint in (1), and ψ ( θ ) : R p → R characterizes the dynamics of the minimal relaxation constant c ∗ , to be determined. We assume that the gradient ∇ θ J ( θ ) is locally Lipschitz continuous. Since a ( θ , c ∗ ) depends on both c ∗ and θ , and c ∗ itself is determined by θ , we adopt the shorthand notation a ( θ ) := a ( θ , c ∗ ) in the rest of this paper. For brevity, given any c ∗ ≥ 0 , we define the constraint-admissible set associated with (1) as

<!-- formula-not-decoded -->

Three main challenges arise in the design of a ( θ ) in (9).

1) When h ( θ ) is highly nonlinear, deriving a closed-form expression for a ( θ ) that enforces the constraint (1) is difficult. 2) Since the constraint in (1) is defined directly in terms of the parameter θ , it is necessary to construct a feasible set for the parameter dynamics ˙ θ in (9), denoted as C ˙ θ,c ∗ , such that if θ is initialized at θ ∗ G and ˙ θ ∈ C ˙ θ,c ∗ , then θ ∈ C θ,c ∗ .

3) Efficiently determining the dynamics corresponding to the minimal relaxation constant (10) is nontrivial.

To address these challenges, we construct the feasible set C ˙ θ,c ∗ for the dynamics in (9) based on the set-invariance property of continuous-time control barrier functions (CBFs), which enables the nonlinear constraint (1) to be reformulated as a linear constraint without any relaxation. Based on this formulation, we design a quadratic programming (QP) that simultaneously determines a ( θ ) and c ∗ .

Feasible set construction for parameter update direction. Given the parameter dynamics in (9), we have the following definitions related to CBFs along with their fundamental property:

Definition 1 (CBFs for Continuous-time Dynamical Systems [7]): A continuously differentiable function B ( θ ) : R p → R , B ≥ 0 is a CBF if there exists an extended class K function κ such that ∀ θ ∈ R p , it meets the following condition:

<!-- formula-not-decoded -->

where L f B ( θ ) = -∂B ( θ ) ∂ θ ∇ θ J ( θ ) ∈ R and L g B ( θ ) = ∂B ( θ ) ∂ θ ∈ R 1 × p are Lie derivatives. Accordingly, we define the superlevel set of h as the safe set C , given by:

<!-- formula-not-decoded -->

Definition 2 (Constraint Satisfaction with CBFs [7]): Suppose the dynamical system is defined as in (9), with a CBF B ( θ ) specified in Definition 1 and the corresponding safe set C given in (13). Any Lipschitz continuous controller that satisfies (12) ensures that the safe set C remains invariant, thus guaranteeing constraint satisfaction of B ( θ ) ≥ 0 .

According to Definitions 1-2, we define the feasible set C ˙ θ ,c ∗ for ˙ θ within the CBFs framework by treating h ( θ ) + c ∗ as a candidate CBF. The validity of this candidate will be formally established in Section III-C. By Definition 2, any a ( θ ) that satisfies the CBF inequality guarantees forward invariance of the set C θ,c ∗ in (11). Consequently, for any c ∗ ≥ 0 , we introduce the following CBF inequality constraint:

<!-- formula-not-decoded -->

where

<!-- formula-not-decoded -->

are Lie derivatives of the function h ( θ )+ c ∗ under the parameter dynamics given in (9). Furthermore, by applying (9), ˆ g ( a , c ∗ ) in (14) can be equivalently written as:

<!-- formula-not-decoded -->

which leads to the definition of the following feasible set of ˙ θ : C. Analysis

<!-- formula-not-decoded -->

Remark 2: As shown in (16), finding the control input a in (14) is equivalent to determining the parameters update direction ˙ θ ∈ C ˙ θ ,c ∗ . Furthermore, according to the properties of CBFs stated in Definition 2, if the parameters θ is initialized within the set C θ,c ∗ , then any update direction ˙ θ ∈ C ˙ θ ,c ∗ guarantees that the corresponding parameter trajectories remains within θ ∈ C θ,c ∗ throughout the evolution.

Construct the QP problem. Consider the parameter dynamics in (9) and the feasible set specified in (16). We construct the following QP problem to derive a ( θ ) and c ∗ :

<!-- formula-not-decoded -->

Remark 3: For any θ ∈ R p , (17) is formulated to derive the closed-loop controller a ( θ ) and the minimal relaxation constant c ∗ that enforce the CBF inequality in (14), thereby guaranteeing satisfaction of constraint (1), while simultaneously minimizing both quantities.

## B. The Proposed Closed-Loop System

Consider the parameter dynamics in (9), where the parameter is initialized as θ = θ ∗ G . We introduce the proposed closedloop controller a ( θ ) for (9) to solve (2), which corresponds to the closed-form solution of (17), as formalized in the following lemma. In the remainder of this paper, we refer to dynamics (9) with proposed a ( θ ) as the control-barrier-function-based policy adaptation (CBF-PA) algorithm.

Lemma 1: For any θ ∈ R p , if κ ( h ( θ ) + c ) = γ h ( h ( θ ) + c ) with γ h &gt; 0 a given constant, then the closed-loop controller a ( θ ) and minimal relaxation constant c ∗ that solve (17) are:

<!-- formula-not-decoded -->

where the auxiliary terms are given by

<!-- formula-not-decoded -->

and L f and L g are defined in (15).

The proof of Lemma 1 is provided in the Appendix.

Remark 4: For the special case where w = 0 and c is a given constant. Under the same assumptions and notations as in Lemma 1, the following closed-form solution holds:

<!-- formula-not-decoded -->

The proof proceeds analogously to the proof of Lemma 1 and is thus omitted for brevity.

This subsection presents a theoretical analysis of the proposed closed-loop controller in Lemma 1 for solving the constrained optimization problem in (2). Since this paper utilizes the properties of CBFs to ensure (1) is satisfied, it is essential to verify that ∀ c ≥ 0 , the function h ( θ ) + c in (1) is a valid CBF for C θ,c . To this end, we establish the following:

Lemma 2: Given the parameter dynamics in (9), for any c ≥ 0 , h ( θ ) + c in (1) is a valid CBF for C θ,c in (11).

The proof of Lemma 2 is given in the Appendix. The following theorem establishes the constraint satisfaction associated with the optimal closed-loop controller in (18):

Theorem 1: Consider the parameter dynamics (9) with a ( θ ) and c ∗ defined in (18). Suppose θ is initialized as θ = θ ∗ G , ∇ θ J ( θ ) in (9) is locally Lipschitz continuous and bounded, and the classK function κ ( h ( θ )+ c ∗ ) satisfies κ ( h ( θ )+ c ∗ ) = γ h ( h ( θ ) + c ∗ ) with γ h &gt; 0 . Then, if θ evolves according to (9), it follows that θ ∈ C θ,c ∗ as defined in (11).

The proof of Theorem 1 is provided in the Appendix.

## D. Applying the Proposed CBF-PA to Policy Adaptation in RL

We now propose an algorithm that integrates the CBF-PA mechanism to address the policy adaptation problem in RL. The approach extends the deep deterministic policy gradient (DDPG) method [38] by employing CBF-PA as the policy update rule. In contrast to standard DDPG, the proposed CBF-PA algorithm refines the learned policy to minimize an additional task objective while preserving near-optimal performance on the original task. This refinement is achieved by augmenting the policy gradient with the analytically derived closed-loop controller in Lemma 1.

The design builds upon the DDPG framework, where the optimal critic Q ( x t , u t , θ Q ∗ ) is used to approximate the actionvalue function of J ( θ µ ) in (8). The optimal policy parameters θ µ ∗ is then obtained by minimizing the critic feedback, i.e., θ µ ∗ = arg min θ µ ∈ R p E x t ∼ ρ µ [ Q ( x t , µ ( x t , θ µ ) , θ Q ∗ )] , where ρ µ denotes the discounted state visitation distribution under policy µ .

To initialize the proposed algorithm, it is assumed that a pretrained optimal critic ˆ Q ( · , · , θ ˆ Q ∗ ) , as defined in (6), and an optimal policy µ ( · , θ ˆ µ ∗ ) , as specified in (7), are already available. The learning process begins by constructing a new critic function Q ( · , · , θ Q ) : X × U → R , and initializing a policy µ ( · , θ µ ) , where µ and µ share the same functional structure, with the initial policy parameters set as θ µ = θ ˆ µ ∗ .

Building upon the actor-critic framework, we propose an iterative method for estimating the optimal parameters θ Q ∗ and θ µ ∗ . Recall k = 0 , 1 , 2 , · · · denote the iteration index, and let θ Q k and θ µ k represent the estimates of θ Q ∗ and θ µ ∗ at the k -th iteration, respectively. The constant step sizes for updating the critic and actor parameters are denoted by α Q and α µ = ϵα Q , respectively, where 0 &lt; ϵ &lt; 1 is a constant used to ensure that the critic update proceeds at a faster rate than the actor update. The following iterative update rules are employed to refine these estimates:

1) Critic Update: To find the optimal critic that approximates the action-value function of the objective function in (8), the critic parameters θ Q k is updated as follows [35]:

<!-- formula-not-decoded -->

where L Q ( θ Q k , θ µ k ) denotes the temporal difference learning loss function [34] defined by

<!-- formula-not-decoded -->

2) Policy Adaptation using CBF-PA: After updating θ Q k via (19), the policy parameters θ µ k is subsequently updated using the proposed CBF-PA, given by:

<!-- formula-not-decoded -->

where a ( θ µ k ) is computed by (18) with the components:

<!-- formula-not-decoded -->

The complete procedure is summarized in Algorithm 1.

Algorithm 1: Control-Barrier-Function-Based Policy Adaptation (CBF-PA)

- 1 Initialize: Pretrained optimal policy µ ( · , θ ˆ µ ∗ ) and optimal critic ˆ Q ( · , · , θ ˆ Q ∗ ) ; Empty data memory; initialize Q ( · , · , θ Q ) , µ ( · , θ µ ) with θ µ = θ ˆ µ ∗ , and the iteration index k = 0 ; copy ¯ Q ← Q, ¯ µ ← µ
- 2 Set the number of episodes E , task horizon T , update weight τ , batch size N , γ h , and step sizes α Q and α µ 3 for episode = 0 , 1 , 2 , · · · , E do

4

- Initialize random process W for action exploration and randomize initial state x (0)

5

6

7

8

9

10

for

t

= 0

,

1

,

2

,

, T

· · ·

do

Execute control input

(

t

) =

u

+

, observe resulting

W

(

t

)

µ

k

(

(

t

)

,

)

µ

x

θ

x

(

data tuple

Sample

, and store

t

+1)

(

t

t

t

+1

,

,

)

x

u

x

N

in data memory tuples uniformly from data memory

- Update critic and actor following (19) and (20), respectively, wherein the expectation terms are approximated using the averaged values computed over the N sampled tuples

Update the copied networks and

Q

µ

¯

¯

<!-- formula-not-decoded -->

Return:

θ

¯

Q

,

θ

¯

µ

:

E. Discussion of Discrete-Time Implementation

Since lim α µ → 0 θ µ k +1 -θ µ k α µ = ˙ θ µ , the policy update rule in (20) can be interpreted, as α µ → 0 , as the Euler discretization of the continuous-time dynamics

<!-- formula-not-decoded -->

This scheme introduces a discretization error of order O ( α µ ) per gradient descent step [39], thereby causing a modeling discrepancy due to the nonzero α µ . To ensure that the discrete-time implementation in (20) using a ( θ µ k ) preserves the performance guarantees of the continuous-time formulation in (9) with a ( θ µ ) in (17), we adopt the following result:

Lemma 3 (Inter-Sample Safety Guarantees, Theorem 3 [40]): Recall the policy parameter dynamics from (9) and the CBF B ( θ ) defined in Definition 1. Suppose that -∇ θ J ( θ ) and a ∈ A ⊂ R p are bounded. Let ∆ be a constant such that

<!-- formula-not-decoded -->

where L B denotes the Lipschitz constant of B ( θ ) . Then, it holds that the closed-loop controller a satisfying the following tightened CBF inequality constraint of (14):

<!-- formula-not-decoded -->

will guarantee the invariance of the C in (13).

Lemma 3 says that if the discrepancy between the continuoustime dynamics and its Euler-discretized counterpart is bounded, then the control performance guaranteed by the CBF condition in the continuous-time setting can be preserved in the discretized system by employing a more conservative CBF inequality constraint.

## IV. NUMERICAL SIMULATIONS

In this section, the proposed CBF-PA is first applied to a simple illustrative example, facilitating a direct comparison with standard gradient descent and the multi-objective optimization in (3). This example also provides insight into the impact of key parameters, including the penalty weight w , step size α , and CBF parameter γ h , on optimization performance. Subsequently, Algorithm 1 is evaluated in two benchmark simulation environments, Cartpole and lunar lander, from OpenAI Gym [41], to assess its effectiveness and generalizability. For additional comparison, the method is evaluated against two representative transfer learning approaches, thereby highlighting its relative advantages.

## A. An Illustration Example

Let θ = [ x, y ] ′ ∈ R 2 . We define the additional and original objective functions as J ( θ ) = sin( x ) + ( y -8) 2 and G ( θ ) = x 3 + y 3 , s.t., x ≥ 0 , y ≥ 0 , respectively. The goal is to solve the optimization problem θ ∗ = arg min θ ∈ R 2 J ( θ ) , starting from a known initialization θ = θ ∗ G , where

<!-- formula-not-decoded -->

Note that for any θ = [ -x, x ] ′ , one obtains G ([ -x, x ] ′ ) = G ( θ ∗ G ) = 0 . This setup is designed to evaluate the algorithm's

Fig. 2: Proposed CBF-PA under various values of w , shown over the contour plot of the function G ( x, y ) .

<!-- image -->

ability to preserve the performance on the pre-optimized task G while minimizing J . Accordingly, the performance metrics are defined as additional cost function J ( θ k ) , original cost function G ( θ k ) , and average value of G ( θ k ) over K iterations:

<!-- formula-not-decoded -->

Benchmarks. This experiment examines how different constant weight parameters w influence the performance metric of the discrete-time implementation of the proposed method, defined as

<!-- formula-not-decoded -->

where a ( θ k ) is given in Lemma 1. We compare (21) with two baseline methods: standard gradient descent (GD) and a multi-objective gradient descent (MOGD). The GD method follows the classical update rule:

<!-- formula-not-decoded -->

while the MOGD baseline incorporates a regularization term involving G , and is defined as:

<!-- formula-not-decoded -->

For the proposed method, we set γ h = 10 , and use a fixed step size α = 0 . 001 and initial parameter θ ∗ G for all methods. By varying w , we evaluate the relative optimization behavior and the trade-offs between minimizing the primary loss J and maintaining performance with respect to G .

Results Analysis. As illustrated in Figs. 2-3, the proposed method exhibits an optimization behavior similar to that of the MOGD algorithm under various weight parameters w . Specifically, decreasing w drives the parameter trajectories of both methods toward the optimizer of the additional cost J ( θ ) . Conversely, as w increases, both methods yield parameter trajectories that approach the joint optimum of J and G . Moreover, for larger values of w , the proposed CBF-PA demonstrates a faster convergence rate than MOGD.

Key Parameters Investigation. To evaluate the impact of key parameters in the proposed method (21), two sets of experiments are conducted. In the first set, the CBF parameter γ h is varied while fixing the step size α = 0 . 001 and penalty weight w = 0 . 01 . In the second set, the step size α is varied with γ h = 10 and w = 0 . 01 .

Results Analysis. As shown in Figs. 4-5, reducing γ h from 10 to 0 . 1 accelerates the convergence of both J and G . Conversely, when γ h ≥ 50 , CBF-PA tends to prioritize minimizing the additional cost J over the original cost G . Furthermore, reducing the step size α leads to slower convergence while yielding parameter trajectories similar to those of the proposed method with larger α .

## B. Cartpole Example

The state of the Cartpole system is represented as x ( t ) = [ p x ( t ) , ˙ p x ( t ) , θ ( t ) , ˙ θ ( t )] ′ , where -4 . 8 ≤ p x ( t ) ≤ 4 . 8 denotes the cart's position, ˙ p x ( t ) represents the cart's velocity, -0 . 418 ≤ θ ( t ) ≤ 0 . 418 is the pole's angle, and ˙ θ ( t ) is the pole's angular velocity. The control input -1 ≤ u ( t ) ≤ 1 is applied to the center of the cart to push the cart to move left or right. The dynamics of the Cartpole are given by:

<!-- formula-not-decoded -->

where l , g , m c , m p denote the length of the pole, gravity, the mass of the cart, and the mass of the pole, respectively.

Setup. We set the time interval ∆ t = 0 . 05 seconds for simulating the dynamics in (22) to generate the training data. The control inputs are derived from the policy u ( t ) = µ ( x ( t ) , θ µ k ) + W ( t ) , where W ( t ) represents exploration noise sampled from a normal distribution. The primary control objective for the Cartpole is to maintain the pole upright by appropriately moving the cart. For this, a predefined stage cost is defined as follows:

<!-- formula-not-decoded -->

The predesigned optimal policy ˆ µ and critic ˆ Q are achieved by running the existing DDPG algorithm. An additional objective is to drive the cart to the position p x ( t ) = 2 , for which we define the stage cost function as:

<!-- formula-not-decoded -->

For comparison purposes, we define the stage cost for multiobjective RL as: ϕ MORL t = ˆ ϕ ( θ ( t )) + wϕ ( p x ( t ) , ˙ p x ( t )) , and define the stage cost based on the concept of behavior cloning [42] from transfer learning as:

<!-- formula-not-decoded -->

Fig. 5: Proposed CBF-PA for different α , plotted on the contours G ( x, y ) .

<!-- image -->

Finally, all algorithms are trained for 200 episodes. Each episode terminates either when the state constraint is violated or when the time step t exceeds 200 . The initial state for each episode is randomly sampled from a uniform distribution within the range [ -0 . 05 , -0 . 05 , -0 . 05 , -0 . 05] ′ to [0 . 05 , 0 . 05 , 0 . 05 , 0 . 05] ′ . The proposed method is deployed to minimize the accumulated cost ϕ in (24), while the DDPG algorithm is implemented to minimize ϕ BC t and ϕ MORL t . All methods are initialized from the same predesigned policy µ and critic ˆ Q . To mitigate the effects of stochasticity arising from random initialization and training of DNNs, all experiments are repeated over 5 independent trials.

Results Analysis. Fig. 6 illustrates the training cost for all comparison algorithms. The proposed algorithm effectively preserves the original task, reaching a cost of -499 over the 200 training epochs across 5 trials. Moreover, it exhibits a faster convergence rate in minimizing the additional stage cost compared to the baseline methods. Fig. 7 illustrates the performance of adapted policies across all evaluated algorithms over 50 test episodes. In Fig. 7a, all methods maintain optimal performance on the original task, successfully balancing the Cartpole for the 500 time steps for each episode, indicating that no method compromises the original objective. In contrast, Fig. 7b highlights clear differences in performance on the additional task: the proposed algorithm significantly reduces the additional stage cost compared to the baseline methods.

To evaluate statistical significance, a one-way analysis of variance (ANOVA) [43, Chapter 14] is conducted. The analysis reveals a statistically significant difference in the additional task cost across methods (p-value &lt; 0.05). Post-hoc comparisons using Tukey's HSD test indicate the following relationship: Proposed algorithm &lt; Behavior Cloning = Multi-objective RL = Pretrained model. This result confirms that the proposed algorithm achieves the additional objective more effectively without sacrificing performance on the original task.

Fig. 6: Training cost for the Cartpole example, where Fig. 6a denotes the episode cost refers to the cumulative original stage cost ˆ ϕ in (23) accumulated over each episode, while Fig. 6b is the averaged episode cost represents the average additional stage cost ϕ in (24), accounting for variations in initial positions. The solid line in the plots illustrates the mean value across 5 experimental trials, and the shaded region indicates the standard deviation over these trials.

<!-- image -->

## C. Lunar Lander Example

In this subsection, we evaluate the proposed CBF-PA algorithm on the OpenAI Gym Lunar Lander environment [41]. The system state x ( t ) ∈ R 8 comprises the horizontal and vertical positions, velocities, angular orientation, angular velocity, and ground-contact indicators of the lander and is subject to [ -1 . 5 , -1 . 5 , -5 . 0 , -5 . 0 , -3 . 14 , -5 . 0 , 0 , 0] ′ ≤ x ( t ) ≤ [1 . 5 , 1 . 5 , 5 . 0 , 5 . 0 , 3 . 14 , 5 . 0 , 1 . 0 , 1 . 0] ′ . The control input u ( t ) ∈ R 2 , which commands the main and lateral thrusters, is similarly bounded by [ -1 , -1] ′ ≤ u ( t ) ≤ [1 , 1] ′ . For a detailed description of the underlying dynamics, see [41].

Setup. As in the Cartpole example, the system state-input data pairs are generated using the policy u ( t ) = µ ( x ( t ) , θ µ k ) + W ( t ) , where W ( t ) represents exploration noise drawn from a normal distribution. Each episode begins with the lunar lander spawned at a fixed initial pose, subject to a random perturbation at its center of mass, and terminates when either a state constraint is violated or the time index t exceeds 200 . The primary control objective is to achieve a soft touchdown at the target pad located at ( x, y ) = (0 , 0) . The underlying predefined stage cost ˆ ϕ ( x ( t ) , u ( t )) are adopted from [41]. Both the predesigned policy µ and critic ˆ Q are obtained by running the DDPG algorithm. To evaluate the proposed CBF-PA framework, an additional task to minimize energy consumption during the landing phase is defined as:

<!-- formula-not-decoded -->

<!-- image -->

(b) Cost of additional task

ϕ

(

p

x

(

t

)

,

˙

p

x

(

t

))

.

Fig. 7: Cost of original and additional task for 50 test episodes with the Cartpole.

For comparative analysis, we define the stage cost for multiobjective RL as: ϕ MORL t = ˆ ϕ ( x ( t ) , u ( t )) + wϕ ( u ( t )) , and for behavior cloning [42] as ϕ BC t = ϕ ( u ( t )) + w ∥ µ ( x ( t ) , θ µ ) -µ ( x ( t ) , θ ˆ µ ∗ ) ∥ 2 . Each algorithm was trained for 200 episodes and evaluated across 5 independent trials.

Results Analysis. Fig. 8 showcases the training process for all comparison algorithms. The proposed algorithm successfully maintains the original task over 400 training episodes across 5 trials. Moreover, it demonstrates faster convergence, reducing the additional stage cost within 200 episodes, surpassing the baseline methods in efficiency. Fig. 9 presents the performance of the adapted policies across all evaluated algorithms over 50 test episodes. In Fig. 9a, all methods successfully maintain the objective on the original task, enabling the lunar lander to reach the designated landing position. To assess statistical differences, an ANOVA [43, Chapter 14] was conducted, which finds no statistically significant differences in the original task cost among methods (p-value &gt; 0 . 05 ).

In contrast, Fig. 9b reveals notable differences in the additional task performance. The proposed algorithm achieves a substantially lower additional stage cost, effectively minimizing energy consumption during the landing process compared to baseline methods. An ANOVA confirms that the differences in additional task cost are statistically significant (p-value &lt; 0 . 05 ). Post-hoc analysis using Tukey's HSD test further identifies the following relationship: Proposed algorithm &lt; Behavior Cloning = Multi-objective RL = Pretrained model. These results demonstrate that the proposed algorithm enhances

Fig. 8: Training plots for lunar lander example, where Fig. 8a denotes the episode rewards, which refers to the original rewards accumulated over each episode, while Fig. 8b denotes the averaged episode cost, representing the average additional stage cost ϕ in (25), accounting for variations in initial positions. The solid line in the plots illustrates the mean value across 5 experimental trials, and the shaded region indicates the standard deviation over these trials.

<!-- image -->

performance on the additional objective with an acceptable degradation in performance on the original task.

## V. REAL-WORLD DEMONSTRATIONS

In this section, we demonstrate the performance of the proposed CBF-PA algorithm on a quadrupedal robot platform, highlighting its effectiveness in path planning adaptation scenarios. The kinematic dynamics of the quadrupedal robot are modeled as a unicycle model, described as:

<!-- formula-not-decoded -->

where the system state and control input at any time t are defined as x ( t ) = [ p x ( t ) , p y ( t ) , θ ( t ) ] ′ ∈ R 3 and u ( t ) = [ u v ( t ) , u ω ( t ) ] ′ ∈ R 2 , respectively. Here, -2 . 4 m ≤ p x ≤ 2 . 4 m , -1 . 8 m ≤ p y ≤ 1 . 6 m , -π ≤ θ ≤ π are the positions on the x-axis and y-axis, and yaw angle, respectively. -1 m/s ≤ u v ≤ 1 m/s and -3 rad/s ≤ u ω ≤ 3 rad/s are the linear velocity and yaw angular velocity of the robot, respectively. The initial state is uniformly generated between [ -1 . 9 , -0 . 2 , -0 . 2] ′ and [1 . 5 , 0 . 2 , 0 . 2] ′ . An illustration of this unicycle model is shown in Fig. 10.

<!-- image -->

(b) Cost of additional task

ϕ

(

u

(

t

))

.

Fig. 9: Cost of original and additional tasks for 50 test episodes with the lunar lander.

Fig. 10: High-level quadrupedal robot kinematics definition.

<!-- image -->

## A. Policy Adaptation to a Different Target Position

In this experiment, we apply the proposed method to adapt a pretrained policy that enables the robot to avoid obstacles while reaching a specified target position.

Experiment Setup. As shown in Fig. 11a, the original task of the quadrupedal robot is to avoid collision with the obstacle placed in the center with p x = -0 . 5 m , p y = 0 m and radius 0 . 265 m, for which we define the following predefined stage cost:

<!-- formula-not-decoded -->

where the collision means (( p x ( t ) + 0 . 5) 2 + p 2 y ( t )) 1 2 ≤ 0 . 595 m. The stage cost ˆ ϕ ( x ( t )) in (27) is designed to give the robot a penalty when the collision occurs and to encourage the robot to move away from the position p x = 0 , p y = 0 . A pretrained policy µ ( x ( t ) , θ ˆ µ ) and its corresponding critic

<!-- image -->

(a) Time-lapse trajectory with pretrained policy.

<!-- image -->

(b) Time-lapse trajectory with policy during adaptation training at episode 90.

<!-- image -->

(c) Time-lapse trajectory with adapted policy.

<!-- image -->

- (d) Actual trajectory with pretrained policy.

(e) Actual trajectory with policy during adaptation training at episode 90.

- (f) Actual trajectory with adapted policy.

Fig. 11: Time-lapse and actual position trajectories of the robot using the pretrained policy, the policy during adaptation training at episode 90, and the adapted policy. The pretrained policy is designed for the original task of obstacle avoidance. The adapted policy enables the robot to reach a target position (an additional task) while still avoiding collisions (the original task).

<!-- image -->

Fig. 12: Demonstration trajectories. Colors indicate training epochs, where lighter colors correspond to later stages in training.

ˆ Q ( x ( t ) , u ( t ) , θ ˆ Q ) are obtained using the DDPG algorithm. A demonstration of trajectories generated by µ is shown in Fig. 12a. The additional task is to reach the target position p x = 1 . 5 m , p y = 0 m, as shown in Fig. 11a. To achieve this, we define the following additional stage cost function:

<!-- formula-not-decoded -->

The proposed algorithm is subsequently employed to adapt the original control policy to minimize the cost function in (28), while ensuring that the adapted policy continues to satisfy the original task constraints. The demonstration trajectories generated by the adapted policy µ ( x ( t ) , θ µ ) illustrated in Fig. 12c, and the corresponding trajectories observed during the training process are presented in Fig. 12b. This setup illustrates how the proposed algorithm can effectively adapt a pretrained policy to accommodate an additional objective, provided that the optimal solution sets for the two tasks have a non-empty intersection.

Results Analysis. As shown in Fig. 11a, the robot initially executes the pretrained policy, which was developed solely for obstacle avoidance. While the robot successfully avoids collisions, it fails to reach the target position since goal-reaching was not included in the original task. The actual trajectory of the robot, recorded via a motion capture system, is depicted in Fig. 11d. Subsequently, as illustrated in Fig. 11b, the robot still does not reach the goal at the 90 -th episode of the adaptation training due to the incomplete adaptation process. Nevertheless, the proposed algorithm effectively utilizes information from both the original policy and the additional goal-reaching task to guide the adaptation. The corresponding trajectory is shown in Fig. 11e. After completing the adaptation training, the robot executes the fully adapted policy. As demonstrated in Fig. 11c, the robot successfully reaches the goal without any collisions, thus fulfilling both the original (collision avoidance) and additional (goal-reaching) task objectives. The trajectory for this successful trial is also shown in Fig. 11e. Furthermore, to assess

<!-- image -->

(a) Time-lapse trajectory with adapted policy.

<!-- image -->

- (b) Actual trajectory with adapted policy.

Fig. 13: Time-lapse and actual position trajectories of the robot using the same adapted policy with a different initial position. The adapted policy enables the robot to reach a target position (additional task) while still avoiding collisions (original task).

the generalizability of the adapted policy, an additional trial is conducted from a different initial position. As shown in Fig. 13, the robot reaches the goal without collisions, demonstrating the effectiveness of the adapted policy in meeting both original and additional task objectives. Details are available in the supplementary video.

## B. Policy Adaptation to an Additional Obstacle

In this experiment, we aim to utilize the proposed method to adapt the previously learned policy, which enables the robot to avoid obstacles and reach a specified goal, to additionally account for a new obstacle introduced into the environment.

Experiment Setup. As shown in Fig. 14, under the same setup as in Section V-A, the new original task for the quadrupedal robot is to avoid collision with the orange obstacle and reach the target position using the resulting policy from Section V-A.

The additional task is to avoid collision with a newly introduced obstacle located at p x = 0 . 5 m , p y = 0 . 2 m with a radius of 0 . 17 m, as shown in Fig. 14 and marked in violet. Notably, this obstacle is deliberately positioned on the planned trajectories generated by the resulting policy from Section V-A, as illustrated in Fig. 15a. To incorporate this additional task,

(a) Time-lapse trajectory with second adapted policy.

<!-- image -->

(b) Actual trajectory with second adapted policy.

<!-- image -->

Fig. 14: Time-lapse and actual position trajectories of the robot using the first adapted policy as the new pretrained policy for the new original task. The new pretrained policy is designed for the new original task of orange obstacle avoidance and goal reaching. The second adapted policy enables the robot to reach a target position while still avoiding collisions with original (orange) and additional (violet) obstacles.

we define the following stage cost function:

<!-- formula-not-decoded -->

where the collision here means (( p x ( t ) -0 . 5) 2 + ( p y ( t ) -0 . 2) 2 ) 1 2 ≤ 0 . 42 m. The proposed algorithm is subsequently utilized to further adapt the initially modified control policy from Section V-A, enabling the robot to avoid the newly introduced obstacle. The adaptation process is illustrated through the training trajectories shown in Fig. 15b, while the trajectories generated by the final adapted policy are presented in Fig. 15c.

Results Analysis. Fig. 14 presents the experimental results. After completing training, the robot executes the second adapted policy. As shown in Fig. 14a, the robot successfully reaches the target position without any collisions, satisfying both the original task (collision avoidance for the orange obstacle and goal reaching) and the additional task (collision avoidance for the violet obstacle). The collision-free robot trajectory is illustrated in Fig. 14b. Multiple trials with different initial positions, available in the supplementary video, were

Fig. 15: Demonstration trajectories. Colors indicate training epochs, where lighter colors correspond to later stages in training.

<!-- image -->

conducted to evaluate the generalizability. These experiments also highlight that the proposed algorithm supports sequential policy adaptation to multiple additional tasks.

## VI. CONCLUDING REMARKS

In this paper, we have presented a policy adaptation algorithm, termed CBF-PA, for adjusting the parameters of a predesigned policy to minimize an additional task while simultaneously preserving the near-optimal performance of the original task. The key contributions of this work are: (i) formulating the policy adaptation problem as a constrained optimization problem, and (ii) leveraging the constraint satisfaction properties of CBFs to guarantee the fulfillment of the original task while minimizing an additional objective. The main advantages of the proposed CBF-PA approach are threefold. First, the introduction of a CBF with a relaxation variable addresses the potential empty intersection of optimal sets for the original and additional tasks. Second, the proposed algorithm operates as a closed-loop system, thereby reducing computational complexity. Finally, we validate the efficacy of the method through experiments on both low- and highdimensional systems, demonstrating that CBF-PA outperforms traditional imitation and transfer learning methods by achieving lower additional costs while strictly maintaining the deviation from the optimal cost of the original task throughout the learning process.

One shortcoming of the proposed algorithm discussed in this paper is that standard RL algorithms often require the addition of exploration noise to the control policy for improved data exploration. However, this noise may negatively impact the performance of the proposed algorithm during training. Future work could focus on improving the definition of exploration noise within the context of the proposed algorithm. Additionally, the optimal critic and actor in the system may have nonconvex structures, potentially leading to multiple optimal solutions for the proposed methods.

## REFERENCES

- [1] Sridhar Mahadevan and Jonathan Connell. Automatic programming of behavior-based robots using reinforcement learning. Artificial Intelligence , 55(2-3):311-365, 1992.
- [2] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar. Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost. In 2019 International Conference on Robotics and Automation (ICRA) , pages 3651-3657. IEEE, 2019.
- [3] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics , 2019.
- [4] Mahmoud Selim, Amr Alanwar, Shreyas Kousik, Grace Gao, Marco Pavone, and Karl H Johansson. Safe reinforcement learning using black-box reachability analysis. IEEE Robotics and Automation Letters , 7(4):10665-10672, 2022.
- [5] Ananye Agarwal, Ashish Kumar, Jitendra Malik, and Deepak Pathak. Legged locomotion in challenging terrains using egocentric vision. In Conference on Robot Learning , pages 403-415. PMLR, 2023.
- [6] Richard S Sutton, Andrew G Barto, and Ronald J Williams. Reinforcement learning is direct adaptive optimal control. IEEE Control Systems Magazine , 12(2):19-22, 1992.
- [7] Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath, and Paulo Tabuada. Control barrier functions: Theory and applications. In 2019 18th European Control Conference (ECC) , pages 3420-3431. IEEE, 2019.
- [8] W. Xiao, G.C. Cassandras, and C. Belta. Safety-critical optimal control for autonomous systems. J Syst Sci Complex , 34:1723-1742, 2021.
- [9] Ahmed Allibhoy and Jorge Cort´ es. Control-barrier-function-based design of gradient flows for constrained nonlinear programming. IEEE Transactions on Automatic Control , 69(6):3499-3514, 2024.
- [10] Warren B Powell. Ai, or and control theory: A rosetta stone for stochastic optimization. Princeton University , page 12, 2012.
- [11] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research , 32(11):1238-1274, 2013.
- [12] Arthur Earl Bryson. Applied optimal control: optimization, estimation and control . CRC Press, 1975.
- [13] Ayush Rai, Shaoshuai Mou, and Brian DO Anderson. Closed-loop neighboring extremal optimal control using hj equation. In 2023 62nd IEEE Conference on Decision and Control (CDC) , pages 3270-3275. IEEE, 2023.
- [14] V Rehbock, KL Teo, and LS Jennings. A computational procedure for suboptimal robust controls. Dynamics and Control , 2(4):331-348, 1992.
- [15] Bomin Jiang, Adrian N Bishop, Brian DO Anderson, and Samuel P Drake. Optimal path planning and sensor placement for mobile target detection. Automatica , 60:127-139, 2015.
- [16] Reza Ghaemi, Jing Sun, and Ilya V Kolmanovsky. Neighboring extremal solution for nonlinear discrete-time optimal control problems with state inequality constraints. IEEE Transactions on Automatic Control , 54(11):2674-2679, 2009.
- [17] Wanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Pontryagin differentiable programming: An end-to-end learning and control framework. Advances in Neural Information Processing Systems , 33:7979-7992, 2020.
- [18] Zehui Lu, Wanxin Jin, Shaoshuai Mou, and Brian. D. O. Anderson. Cooperative tuning of multi-agent optimal control systems. In 2022 IEEE 61st Conference on Decision and Control (CDC) , pages 571-576, 2022.
- [19] Zihao Liang, Tianyu Zhou, Zehui Lu, and Shaoshuai Mou. Online control-informed learning. Transactions on Machine Learning Research , 2025.
- [20] Burrhus Frederic Skinner. Science and human behavior . Number 92904. Simon and Schuster, 1965.
- [21] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research , 10(7), 2009.
- [22] Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation. In The 22nd International Conference on Artificial Intelligence and Statistics , pages 1331-1340. PMLR, 2019.
- [23] Zhuangdi Zhu, Kaixiang Lin, Anil K Jain, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A survey. arXiv preprint arXiv:2009.07888 , 2020.

- [24] Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning , ICML '99, page 278-287, San Francisco, CA, USA, 1999.
- [25] Anna Harutyunyan, Sam Devlin, Peter Vrancx, and Ann Now´ e. Expressing arbitrary reward functions as potential-based advice. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 29, 2015.
- [26] Wanxin Jin, Todd D. Murphey, Zehui Lu, and Shaoshuai Mou. Learning from human directional corrections. IEEE Transactions on Robotics , 39(1):625-644, 2023.
- [27] Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for temporal difference learning. Journal of Machine Learning Research , 8(9), 2007.
- [28] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces to transfer skills with reinforcement learning. arXiv preprint arXiv:1703.02949 , 2017.
- [29] Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE International Conference on Robotics and Automation (ICRA) , pages 2169-2176. IEEE, 2017.
- [30] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671 , 2016.
- [31] St´ ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics , pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
- [32] Haiyan Yin and Sinno Pan. Knowledge transfer for deep reinforcement learning with hierarchical experience replay. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 31, 2017.
- [33] Andr´ e Barreto, Will Dabney, R´ emi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in Neural Information Processing Systems , 30, 2017.
- [34] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning , 3:9-44, 1988.
- [35] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine Learning , 8:279-292, 1992.
- [36] Adrian Hauswirth, Zhiyu He, Saverio Bolognani, Gabriela Hug, and Florian D¨ orfler. Optimization algorithms as robust feedback controllers. Annual Reviews in Control , 57:100941, 2024.
- [37] Runyu Zhang, Arvind Raghunathan, Jeff Shamma, and Na Li. Constrained optimization from a control perspective via feedback linearization. arXiv preprint arXiv:2503.12665 , 2025.
- [38] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 , 2015.
- [39] Taiki Miyagawa. Toward equation of motion for deep neural networks: Continuous-time gradient descent and discretization error analysis. In Advances in Neural Information Processing Systems , 2022.
- [40] Thomas Gurriet. Applied Safety Critical Control . PhD thesis, California Institute of Technology, 2020.
- [41] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.
- [42] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE International Conference on Robotics and Automation (ICRA) , pages 6292-6299. IEEE, 2018.
- [43] Richard Lowry. Concepts and applications of inferential statistics. Online Statistics Textbook , 2014.

## VII. APPENDIX

This appendix presents the proofs and experimental details underlying the results reported in the main text.

## A. Proof of Lemma 1

To achieve the closed-form solution of the QP problem (17), we consider the following Lagrangian associated with (17):

<!-- formula-not-decoded -->

where λ 1 ≥ 0 and λ 2 ≥ 0 are Lagrange multipliers of function ˆ g ( a , c ) and c , respectively. By following the Karush-KuhnTucker (KKT) condition, the following holds:

<!-- formula-not-decoded -->

Let L a = L f + γ h ( G ( θ ∗ G ) -G ( θ )) , if κ ( h ( θ )+ c ) = γ h ( h ( θ )+ c ) with γ h &gt; 0 , the closed-form solution of (30) is:

<!-- formula-not-decoded -->

■

## B. Proof of Lemma 2

Because the closed-loop controller a ( θ ) to the nominal gradient descent update is not bounded and can take any necessary magnitude and direction, it is sufficient to show that when L g ≡ 0 ′ p , the CBF inequality constraint L f + κ ( h ( θ ) + c ) ≥ 0 holds [7]. Following the parameter dynamics in (9), ∀ c ≥ 0 , the corresponding Lie derivatives are given by:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Clearly, (31b) can only be nullified if ∇ θ h ( θ ) = 0 p , which would also nullify (31a). Therefore, when L g = 0 ′ p , L f + κ ( h ( θ ) + c ) ≥ 0 becomes κ ( h ( θ ) + c ) ≥ 0 , which always holds by the definition of classK functions [7]. ■

## C. Proof of Theorem 1

Since the parameter is initialized as θ = θ ∗ G , the constraints in (1) holds for any c ∗ ≥ 0 , which implies that the initial θ ∈ C θ,c ∗ . Furthermore, since ( a ( θ ) , c ∗ ) in (18) represents solution satisfying the KKT condition of the QP problem in (17), and Lemma 2 establishes that h ( θ ) + c ∗ in (1) is a valid CBF, it follows from Definition 2 that, under the closed-loop dynamics (9), the controller a ( θ ) satisfying (14) guarantees the forward invariance of the constraint-admissible set C θ,c ∗ . Therefore, we conclude that θ ∈ C θ,c ∗ . ■