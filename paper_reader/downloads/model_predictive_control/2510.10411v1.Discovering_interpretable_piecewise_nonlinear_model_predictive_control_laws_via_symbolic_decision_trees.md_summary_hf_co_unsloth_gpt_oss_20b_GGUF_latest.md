**Title & Citation**  
*Discovering interpretable piecewise nonlinear model predictive control laws via symbolic decision trees*  
Ilias Mitrai, 2025.

---

## 1. Abstract  
The paper proposes **symbolic decision trees** as interpretable surrogate models for learning model predictive control (MPC) laws. Unlike traditional decision trees that use piecewise constant or linear functions, each leaf node contains a **non‑linear symbolic expression** built from a set of pre‑selected basis functions. The tree structure—including branching logic and local expressions—is learned **simultaneously** by formulating a **mixed‑integer linear program (MILP)**, which is solved to global optimality using commercial solvers (Gurobi). The approach is applied to an isothermal CSTR (continuous stirred tank reactor) MPC problem. Results show that the learned set of decision trees achieves **lower prediction error** than baseline models (sparse regression, constant and linear decision trees) and yields closed‑loop performance comparable to a conventional MPC implementation.

---

## 2. Introduction & Motivation  
- MPC is dominant for constrained processes but suffers from **online computational burden**.  
- Existing acceleration methods either linearise the dynamics (linear MPC) or implement sophisticated optimisers; still often **slower than the sampling period** for high‑dimensional nonlinear systems.  
- **Machine‑learning surrogates** can approximate the implicit controller map  \( \pi(x_0)\) → \(u\), thereby avoiding optimization at runtime.  
- Deep neural nets are universal approximators but are *black‑box*, requiring expensive verification for stability.  
- **Decision trees** (piecewise constant or linear) are interpretable and can expose critical regions but often need deep trees to capture strong non‑linearity; tree depth grows **exponentially**.  
- **Symbolic decision trees** therefore combine interpretability with expressiveness: local nonlinear symbolic functions (with basis functions) are fitted within each leaf while the tree partitions the state space automatically.  
- They position themselves between baseline surrogate models—explicit sparse regression, constant decision tree, and linear decision tree—and are formulated as a *mixed‑integer optimisation* rather than a heuristic genetic algorithm as in [28].

---

## 3. Methods / Approach  

### 3.1 Symbolic Decision Tree Model  
- **Tree structure**: depth \(D\) (here \(D=2\)), nodes \(n\in\{1,\dots,2D+1\}\), terminals \(T=\{2D,\dots,2D+1-1\}\), internal \(N_{\text{int}}\).  
- **Variables**  
  - \(d_n\in\{0,1\}\): node is branching (\(d_n=1\)) or terminal (\(d_n=0\)).  
  - \(z_{i n}\in\{0,1\}\): data point \(i\) is routed to node \(n\).  
  - \(a_{f n}\in\{0,1\}\): branching feature index for node \(n\).  
  - \(b_n\in\mathbb{R}\): split threshold at node \(n\).  
  - \(c_{k n}\in[\underline{c},\overline{c}]\): coefficient of basis function \(k\) at node \(n\).  
  - \(\hat{y}_{i n}\) and \(\hat{c}_{k n}\) for linearised bilinear terms.  
- **Constraints** (all taken from the paper):  
  - Tree consistency (nodes can branch only if parent branches).  
  - Root node \(d_1=1\); leaf nodes \(d_n=0\).  
  - Routing: data cannot be assigned to a branching node: \(\sum_{i} z_{i n} \le (1-d_n)\).  
  - Every point routed to exactly one terminal: \(\sum_{n\in T} z_{i n}=1\).  
  - If a point routed to \(n\), all its ancestors are branching: \(z_{i n}\le \sum_{m\in \mathcal{A}(n)} d_m\).  
  - Arch logic: \(a_{f n}=1 \Longleftrightarrow d_n=1\).  
  - Splitting condition: for branching node, \(x_{if}\le b_n,\; z_{i 2n}\le a_{f n}\), \(x_{if}\ge b_n,\;z_{i 2n+1}\le a_{f n}\).  
  - Leaf node expression: \(\hat{y}_{i n}=\sum_k c_{k n}\,\phi_k(x_i)\). If node branches (\(d_n=1\)), coefficients forced to zero.  
  - Final prediction: \( \hat{y}_i = \sum_{n\in T} z_{i n}\,\hat{y}_{i n}\).  
- **Objective**  
  \[
  \min_{variables}\;
  \underbrace{\sum_i |\hat{y}_i - y_i|}_{L_\text{loss}}
  + \underbrace{\lambda_c\sum_{k,n}|\!c_{k n}\!|}_{L_\text{compl}}
  + \underbrace{\lambda_m\sum_i |\! \hat{y}_i \!|}_{L_\text{m}} .
  \]
  \(L_\text{loss}\) penalises absolute prediction error, \(L_\text{compl}\) discourages unnecessary basis‑functions, \(L_\text{m}\) normalises the magnitude of predictions.  

### 3.2 Training symbolic MPC laws  
- The MPC **map** \(\pi(x_0)\) is learned by treating \(x_0\) as input features and the optimal control action \(u^\ast\) (obtained by solving the nonlinear optimization in Eq. (15)) as labels.  
- For the reactor example, **basis functions** (19) from Table I were used; tree depth fixed to 2.  
- The MILP had 3,662 constraints, 1,615 variables, 363 binary; solved in 3.4 s with Gurobi 11.0.3.

---

## 4. Experiments / Data / Results  

### 4.1 System & Data Generation  
- **Isothermal CSTR** dynamics (Eq. (14)) with parameters \(V=50\,\text{L}\), \(x_f=1\,\text{mol/min}\), \(k=2\,\text{L}^2/(\text{min}\,\text{mol}^2)\).  
- MPC solved for 50 initial concentrations \(x_0\in[0.1,0.9]\) to generate training data (labels: optimal flow rate \(F^\ast\)).  
- Sampling period used later \(0.1\,\text{min}\).  

### 4.2 Learned Tree  
- **Splitting logic** (Fig. 2): three splits at \(x=0.62, 0.48, 0.69\).  
- **Leaf expressions** (Table I) combine basis functions; leaf 7 predicts constant \(F=75\,\text{L/min}\) but non‑constant due to symmetry.  

### 4.3 Prediction Accuracy (Fig. 4)  
| Model | MAE (Pred.) | IAE (Closed‑loop) |
|-------|--------------|------------------|
| **Symbolic Decision Tree** | **0.0621** | **1.9 %** |
| Sparse Regression | 1.2734 | 32 % |
| Constant Decision Tree | 1.8727 | 89 % |
| Linear Decision Tree | 0.4221 | 32 % |

**Claims**:  
- **Claim:** The symbolic decision tree achieves a **two‑order‑of‑magnitude** lower MAE than the sparse regression (0.0621 vs 1.2734).  
- **Claim:** Compared to constant and linear decision trees, symbolic trees attain lower MAE (0.0621 vs 1.8727 and 0.4221 respectively) because local non‑linear functions capture non‑linearities efficiently.  

### 4.4 Closed‑loop Simulation (Fig. 5)  
- Simulated from initial \(x=0.75 \,\text{mol/L}\).  
- **Performance**:  
  - Symbolic tree’s **integral absolute error** (IAE) close to that of the explicit MPC (difference < 2 %).  
  - Baseline models show larger IAE (linear 89 %, sparse 32 %, constant 32 %).  

### 4.5 Computational Load  
| Model | Mean CPU (s) |
|-------|--------------|
| MPC | 0.063 |
| Symbolic tree | 0.000227 |
| Linear tree | 0.000038 |
| Sparse regression | 0.000015 |

- Symbolic tree’s extra local nonlinear functions **do not increase runtime** significantly; inference is essentially tree navigation plus arithmetic.

---

## 5. Discussion & Analysis  

1. **Interpretability** – The gating logic (splits) and leaf symbolic expressions can be inspected directly; every branch corresponds to a clear linear inequality, and leaf functions are explicit polynomial/non‑linear terms from the chosen basis set.  
2. **Expressiveness vs. Complexity** – By allowing nonlinear functions with few basis terms, the learned tree can achieve high accuracy while remaining shallow (depth = 2), contrasting with standard decision trees that would need deeper structures to approximate similar non‑linearity.  
3. **Optimization robustness** – The MILP formulation ensures global optimality; back‑propagation of errors via the objective avoids local minima that often plague heuristic methods such as genetic programming.  
4. **Scalability** – The experiment demonstrates feasibility for a 1‑D input variable; other authors (e.g., Bertsimas & Dunn [26]) have shown MILP formulations for classification trees, suggesting that increasing input dimensionality will increase binary variables and constraints but remains tractable with modern solvers for moderate sizes.  
5. **Model‑free MPC** – No explicit dynamic model is required for online use; only the function \(\pi(\cdot)\) derived offline is needed, making it compatible with data‑driven or black‑box process descriptions.  

---

## 6. Conclusions  

The authors introduced **symbolic decision trees** as a principled way to learn piecewise‐nonlinear control laws for MPC. The approach jointly learns the partition of the input space and the symbolic expression on each leaf using a **mixed integer linear program**, solved globally. Applied to an isothermal CSTR, the method reproduced the MPC policy with lower prediction error than sparse regression or conventional decision trees, and the resulting closed‑loop behaviour was comparable to that of the original MPC. This demonstrates that interpretability can be combined with high control performance by exploiting symbolic expressions and MILP optimisation.

---

## 7. Key Claims & Contributions  

- **Claim 1:** Symbolic decision trees provide an interpretable surrogate model for MPC that retains high predictive accuracy.  
- **Claim 2:** The mixed‑integer linear formulation yields globally optimal trees for given basis function sets.  
- **Claim 3:** For the CSTR case, the symbolic tree attains **two orders of magnitude lower prediction error** than the sparse regression baseline.  
- **Claim 4:** Closed‑loop simulation shows that the control performance degrades by less than 2 % relative to explicit MPC.  

**Contributions**:  
- Novel decision tree architecture with non‑linear symbolic leaf functions.  
- MILP formulation of learning, including routing, branching selection, and leaf expression constraints.  
- Empirical evidence on a realistic process control problem.  

---

## 8. Definitions & Key Terms  

| Term | Definition |
|------|-------------|
| **MPC (Model Predictive Control)** | A control strategy solving a future optimisation problem given current state. |
| **Decision Tree** | A hierarchical model where each internal node routes samples based on a feature test, and leaves provide a label or function. |
| **Symbolic Decision Tree** | A decision tree where leaves contain an analytical expression (symbolic) built from basis functions. |
| **Basis Function** | A pre‑selected functional form (e.g., polynomials, exponentials) used to build leaf expressions. |
| **MILP (Mixed Integer Linear Program)** | An optimisation problem with continuous and binary variables, objective and constraints linear in variables. |
| **Global Optimality** | The solution found is provably optimal over the entire feasible space; here guaranteed by using an MILP solver. |
| **Tree Depth \(D\)** | Number of layers from root to leaves; total nodes \(2D+1\). |

---

## 9. Important Figures & Tables  

- **Figure 1** – Schematic of symbolic decision tree (nodes, branching, leaf expressions).  
- **Figure 2** – Learned splitting logic (splits at 0.62, 0.48, 0.69).  
- **Figure 3** – Predicted control action vs. ground truth on test set.  
- **Figure 4** – MAE comparison among models; shows symbolic tree lowest.  
- **Figure 5** – Closed‑loop concentration trajectory; symbolic tree ≈ MPC.  
- **Table I** – Coefficients of basis functions at each leaf; matrix shows actual symbolic expressions.  

---

## 10. Limitations & Open Questions  

| Limitation | Possible Impact |
|------------|-----------------|
| Only one case study (CSTR) | May not generalise to higher‑dimensional processes. |
| Fixed depth 2 | Deeper trees may be required for more complex dynamics; complexity grows combinatorially. |
| Basis function set pre‑selected | Performance depends heavily on chosen functions. |
| No explicit stability proof | While interpretable, formal guarantees (e.g., Lyapunov) not provided. |
| Scalability of MILP not empirically tested | For large state dimension, solver time may become prohibitive. |

**Open Questions**  
1. Can the approach handle **time‑varying set‑points** or **multi‑objective MPC**?  
2. How to choose a **well‑conditioned basis set** for a given process family?  
3. Does the interpretation of symbolic trees enable *formal* verification of **gain scheduling** or safety constraints?  

---

## 11. References to Original Sections  

- Mixed‑Integer formulation: Section II‑A (“Learning symbolic decision trees via mixed integer optimization”).  
- Baseline comparisons: Section III‑C (“Comparison with baselines”).  
- Closed‑loop results: Section III‑D (“Closed loop simulations”).  
- Discussion on interpretability: Section I (“Introduction & Motivation”).  

---

## 12. Executive Summary (optional)  

- Introduced **symbolic decision trees**: interpretable, piecewise‑nonlinear MPC surrogates.  
- Jointly learns partition + local symbolic functions via **global MILP**.  
- Case study on isothermal CSTR: MAE 0.0621 vs 1.2734 (sparse) and >0.4 (standard/linear trees).  
- Closed‑loop performance similar to explicit MPC (1.9 % IAE difference).  
- Solver time: 3.4 s offline; inference milliseconds.  
- Interpretability: explicit splits and algebraic leaf expressions.  
- Limitations: only shallow tree evaluated, basis choice critical, scalability untested.  

---

## 13. Supplementary Material  

- Basis function list (19 functions) provided in Table I.  
- Full MILP constraints & linearisation steps shown in Section II‑A.  

---