**Title & Citation**  
**GATO: GPU‑Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control**  
Alexander Du¹, Emre Adabag¹,², Gabriel Bravo³,⁴, Brian Plancher ³,⁴  
¹School of Engineering and Applied Science, Columbia University  
²University of Michigan, USA  
³Barnard College, Columbia University & Dartmouth College, USA  
⁴Corresponding author: plancher@dartmouth.edu  

**Abstract**  
The authors introduce **GATO**, an open‑source, GPU‑accelerated batched trajectory‑optimization (TO) solver crafted for **Model Predictive Control (MPC)**.  Existing GPU‑accelerated solvers either (i) parallelize a single solve, (ii) scale to very large batch sizes at sub‑real‑time rates, or (iii) require restrictive models (e.g. point‑mass dynamics).  GATO co‑designs algorithm, software, and hardware (block, warp, and thread level parallelism) to deliver real‑time throughput for *moderate* batch sizes (tens to low‑hundreds of problems, each with a few hundred knot points).  Experiments on simulated 6‑DoF manipulators show speedups of 18–21× over CPU baselines (OSQP) and 1.4–16× over the state‑of‑the‑art GPU baseline (MPCGPU) as batch size grows.  Case studies illustrate better disturbance‑rejection, convergence, and real‑world validation on an industrial manipulator.

---

## Introduction & Motivation  

* MPC yields strong performance in robotics but requires solving batches of nonlinear TO problems online.  
* Traditional solvers are either slow, restricted to a single solve per GPU assignment, or work only with very large batch sizes.  
* Many recent MPC applications need batches of **tens to low‑hundreds of solves** (e.g., 8–128 problems with 64‑knot horizons).  
* Existing GPU‑solvers are ill‑suited: they either under‑use GPU resources or impose strict assumptions about the model.  
* The authors therefore set out to build a solver that (1) operates **in real‑time** for *moderate* batch sizes and (2) is **general enough** to handle complex robot dynamics (via GRiD).  

---

## Methods / Approach  

### 1. Direct Trajectory Optimization (Section II.A)  

* Trajectory: states \(X=\{x_0,\dots,x_N\}\in\mathbb{R}^n\) and controls \(U=\{u_0,\dots,u_{N-1}\}\in\mathbb{R}^m\).  
* Dynamics: \(x_{k+1}=f(x_k,u_k,h)\).  
* Cost function: additive \(J(X,U)\).  
* The direct method follows the classic SQP chain (steps 1–3 in Section II.A, the same pipeline as in [37],[38],[39]):  

  1. **Quadratic program formation** (Taylor expansion → Hessian \(\mathbf{H}\), gradient \(\mathbf{g}\)).  
  2. **KKT linear system** \(\mathbf{A}\Delta z^\star = -\mathbf{g}\) (Eq. (2)).  
  3. **Update & line search** using an L1‑based merit function (Eq. (3)).

* Adabag *et al.* [21] introduced a **symmetric stair preconditioner** Φ⁻¹ for PCG on the GPU – GATO re‑uses this core idea.

### 2. Schur Complement & PCG (Section II.B)  

* The KKT system is transformed into a Schur complement \(S\) (block‑tri‑diagonal) (Eq. (5)).  
* PCG solves \(S\lambda^\star = \gamma\) where the preconditioner Φ⁻¹ matches S’s block‑stair structure.  
* GATO implements **batched PCG**:  

  * Each **CUDA thread block** solves one linear system (Eq. (6)).  
  * **Warp‑level** distribution over knot points of that job; **thread‑level** operations on per‑knot state/control blocks.  
  * All vector‑updates, matrix‑vector products, and reductions happen *inside* the block – no inter‑block synchronisation.  
  * Data is stored **row‑major** by batch‑x‑time to enable coalesced global loads.  
  * Shared memory tiles and warp‑shuffles are used for block‑diagonal and off‑diagonal multiplications.  
  * Only 1 global barrier (after all systems solved) – this is essential for *edge* or *multi‑GPU* deployments.

* The solver also **unrolls** loops over small compile‑time dimensions and compiles with `-O3 -use_fast_math` for maximum throughput.

### 3. Batched Problem Setup & Line Search (Section III.A)

* **Problem formation**: all \(N\times M\) dynamic and cost derivatives are computed **in parallel** (blocks → threads).  
* Lep-**fals**: gradient and Hessian of dynamics (via GRiD) and cost matrices.  
* **Merit function** \( \mathcal M = \|\mathbf{e}\|_1 + \nu \|\Delta z\|_2 \) (Eq. (3)) computed for all line‑search iterates (up to A=20).  
* Using **grid‑like** iterations, all **merit evaluations** are performed in a single kernel.  
* Only a single **global sync** is required after Φ⁻¹ construction.  
* The kernel is written so that temporary variables are in fast shared memory; final S, γ, Φ⁻¹ are in global memory ready for the batched PCG.

### 4. Overall Flow (Algorithm 1)

```
for b=1…N⋅M  in parallel blocks:
    compute S_b, γ_b, Φ⁻¹_b   (III‑A)
for b=1…M in parallel blocks:
    compute δZ* via (4) using warp‑level PCG  (III‑B)
for b=1…N⋅M⋅A in parallel blocks:
    compute Merit_b            (III‑A)
for b=1…M:
    compute α*_b               (III‑A)
return X*,U*
```

---

## Experiments / Data / Results  

| Section | Setup | Baselines | Main Findings |
|---------|-------|-----------|----------------|
| **IV.B** *Scalability* | 6‑DoF Neuromeka Indy7, horizon N=64, motion: figure‑8 | OSQP (CPU), MPCGPU (GPU) | GATO outperforms: 18–21× over CPU, 1.4–16× over GPU as M→128. |
| **IV.C** *Loss–heat map* | M×N varying | GATO reaches kHz rates for M=512,N=8 and M=32,N=128 | GATO’s performance largely tracks total number of knot points (N⋅M). |
| **IV.D** *Case Study 1 – Batching over hyper‑param.* | 7‑DoF KUKA, N=64, h=0.05 s, log‑spaced ρ ∈ [10⁻⁸,10¹] | Single‑solve (M=1) vs batches | Batches M≥16 reduce merit faster; M=32/128 nearly halve initial merit after 1 SQP iteration. |
| **IV.D** *Case Study 2 – Disturbance Rejection.* | 6‑DoF figure‑8, external force at end‑effector, varying M* | Single‑solve vs M=1,8,32,64,128 | Tracking error & joint velocity reduce with M until a sweet spot at M≈32; beyond that latency overpasses optimality. |
| **IV.E** *Case Study 3 – Planning Under Uncertainty.* | Pick‑and‑place, 15 kg pendulum, N=16, h=0.01 s | M=1 vs M=32 | Table I: success rate rises from 33 % (M=1) to 99.2 % (M=128) while total solve time drops. Figure 6 visualises improved coverage. |
| **IV.E** *Hardware Deployment.* | Industrial KUKA iiwa, M=1 vs M=32, two tasks (a) 5‑goal 100 Hz, (b) 3‑goal 1000 Hz, h=0.02 s | GATO vs single‑solve | Table II: M=32 reaches more goals faster (Task b: 6.71 s vs 24.00 s). |

**Speed‐up Table (Figure 3, left)**  
| Batch M | CPU/OSQP time (ms) | MPCGPU time (ms) | GATO time (ms) |
|--------|--------------------|-------------------|-----------------|
| 1 | 8.3 | 4.5 | 2.5 |
| ... | ... | ... | ... |
| 128 | 1660 | 531 | 42 |

*(Times are illustrative; exact numbers are in the paper’s figure.)*

**Heat‑map (Figure 3, right)**: Shows solve‑time plateau at ≈100 µs for (M,N) pairs where N⋅M≤512, followed by linear scaling.

---

## Discussion & Analysis  

* **Algorithmic Efficiency** – By keeping all operations inside a single CUDA kernel whenever possible, GATO reduces global transfers and memory stalls.  
* **Parallelism Trade‑offs** – Block‑level parallelism caters to *batches*, warp‑level to *knot‑points*, thread‑level to *small‑linear‑systems*.  This three‑tier approach fully stretches GPU parallelism.  
* **Memory Layout** – Row‑major storage for S, γ, Φ⁻¹ ensures coalesced reads/writes.  Extra padding removes bounds checks.  
* **Preconditioner** – Symmetric stair matches the block‑tri‑diagonal structure giving a PCG that converges within 10–15 iterations for the tested problems.  
* **Benchmark vs Baselines** – The comparison explicitly excludes Jax‑based GPU solvers because those take tens of ms even on modest GPUs.  OSQP’s GPU backend is unsuitable for these problem sizes, justifying its absence.  
* **Case Studies** – Show that modest batch sizes (8–64) give the biggest practical benefit; beyond that latency outweighs finer hypothesis granularity.  In a pick‑and‑place scenario, ~30 % faster completion time and ~20 % more reliability are achieved.  

---

## Conclusions  

GATO provides a **real‑time**, **GPU‑accelerated batched solver** for trajectory optimization that:  

1. Works for *moderate* batch sizes (tens‑to‑low‑hundreds).  
2. Offers **speedups** up to 21× over CPU and 16× over previous GPU baselines.  
3. Improves disturbance rejection, convergence, and reliability in simulated and hardware robotics tasks.  
4. Is **open‑source** (GitHub: https://github.com/a2r‑lab/GATO).

Future avenues highlighted: actor‑critic RL guidance, contact‑implicit optimization, low‑power edge GPUs (e.g., Jetson derivatives).  

---

## Key Claims & Contributions  

| Claim | Evidence |
|-------|-----------|
| **GATO achieves real‑time throughput for batches M∈[1,≈128]** | Scalability benchmark (Figure 3, left); measure <1 kHz for >512 points. |
| **Speedup 18–21× over CPU + 1.4–16× over GPU** | Figure 3; Table I & II quantify. |
| **Improved disturbance rejection / convergence** | Case Study 2 analysis; plotting error vs M. |
| **Signal‑3d works with complex dynamics** | Use of GRiD dynamics; KUKA iiwa experiments. |
| **Fully co‑designed algorithm, software, hardware** | Block‑warp‑thread parallelism; data layout; shared memory usage. |
| **Open‑source implementation** | GitHub repo. |

---

## Definitions & Key Terms  

* **MPC (Model Predictive Control)** – control strategy solving an optimization over a finite horizon.  
* **TO (Trajectory Optimization)** – direct method solving for states and controls.  
* **KKT System** – Karush‑Kuhn‑Tucker equations forming linear system (2).  
* **Schur Complement (S)** – block‑tri‑diagonal matrix derived from KKT (Eq. 5).  
* **PCG (Preconditioned Conjugate Gradient)** – iterative solver using preconditioner Φ⁻¹.  
* **Symmetric Stair Preconditioner** – Φ used in [40]; matches block‑tri‑diagonal S.  
* **Merit Function (𝓜)** – scalar measure combining cost and constraints; used for line search (Eq. 3).  
* **Batch** – group of M independent TO problems solved concurrently.  
* **Knot Point** – discrete time instant in horizon N.  
* **GRiD** – CUDA library for analytical rigid‑body dynamics [30].  
* **Pinocchio** – CPU‑side dynamics library [42].  

---

## Important Figures & Tables  

1. **Figure 1** – Diagram of GATO’s parallel architecture across batch, knot, and thread levels.  
2. **Figure 2** – Workflow of batched solver:  
   * (a) Setup & line search – highly parallel across solves & timesteps.  
   * (b) PCG – warp‑level inside each solve.  
   * (c) Line‑search – batch‑level reductions.  
3. **Figure 3 (left)** – Solve time vs M for GATO, MPCGPU, OSQP.  
4. **Figure 3 (right)** – Heat‑map of solve time vs M,N.  
5. **Figure 4** – Normalised merit function evolution vs SQP iteration, varying Q,R, M.  
6. **Figure 5** – Disturbance force study: tracking error & joint velocity vs M.  
7. **Figure 6** – Simulation of pick‑and‑place: success vs failure markers for M=1,32.  
8. **Figure 7** – Cumulative density of solve times for different M; shows tighter distribution as M increases.  
9. **Figure 8** – Hardware experiment: trajectories for M=32 vs M=1 under disturbance.  
10. **Table I** – Success rate & mean time vs batch size in pick‑and‑place.  
11. **Table II** – Performance metrics for hardware pick‑and‑place (Tasks a,b).  

---

## Limitations & Open Questions  

1. **Hardware Specificity** – While designed for GPUs, the approach still relies on high core counts (e.g., RTX 4090).  Performance on low‑power edge GPUs (Jetson) is future work.  
2. **Dynamic Scaling** – GATO’s parallelism assumes block‑tri‑diagonal structure; extension to highly sparse or non‑tri‑diagonal problems not evaluated.  
3. **Learning‑based Control** – Integration with actor‑critic RL is proposed but not implemented; open which loss terms bring the most benefit.  
4. **Fault‑Tolerance** – GPU kernels are deterministic; no evaluation of transient faults or shared‑memory corruption.  
5. **Adaptive Batch Size** – The strategy uses a fixed M per iteration; dynamic adjustment based on disturbance magnitude remains unexplored.  

---

## References to Original Sections  

* Introduction: **Section I**  
* Direct TO & algorithm pipeline: **Section II.A**  
* Schur Complement & PCG: **Section II.B**  
* Design & implementation: **Section III** (Algorithm 1, III‑A/B)  
* Scalability benchmarks: **IV.B**  
* Case studies: **IV.D** (hyper‑param & disturbance) and **IV.E** (planning under uncertainty)  
* Detailed tables & figures: **Tables I & II**, **Figures 3–8**  
* Limitations & future work: **Section V**  

---

## Supplementary Material  

* Source code on GitHub: <https://github.com/a2r-lab/GATO>  
* Python wrappers for CPU/GPU functions with timing via `timeit`.  
* Pre‑compiled CUDA kernels for PCG ✔️  
* Documentation on installation and usage with GRiD and Pinocchio.  

---  

**Executive Summary (Optional)**  

* A GPU‑accelerated, batched trajectory optimizer named GATO is presented.  
* Co‑design across algorithm, software, and hardware yields 18–21× speedups over CPU and 1.4–16× over prior GPU solvers.  
* Demonstrated improved disturbance rejection, convergence, and real‑world deployment on an industrial manipulator.  
* Open source library available with robust benchmarks.  

---  

  

**End of summary**