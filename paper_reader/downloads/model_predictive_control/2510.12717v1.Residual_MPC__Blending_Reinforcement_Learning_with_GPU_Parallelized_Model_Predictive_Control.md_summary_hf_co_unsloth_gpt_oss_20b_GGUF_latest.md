# Residual MPC: Blending Reinforcement Learning with GPU‑Parallelized Model Predictive Control  
**Citation**: Jeon, S. H., Lee, H. J., Hong, S., & Kim, S. (2025) *Residual MPC: Blending Reinforcement Learning with GPU‑Parallelized Model Predictive Control*.  

---

## Abstract  
Model Predictive Control (MPC) delivers interpretable, tightly‑governed locomotion by solving a physics‑based optimisation.  Its performance decays under model mismatch, and solving MPC continuously incurs a real‑time computational cost.  Reinforcement Learning (RL) can learn robust policies but suffers from black‑box interpretability, out‑of‑distribution failures, and a heavy dependence on handcrafted rewards.  The authors propose a **GPU‑parallelised residual policy** that blends the structured MPC output with a learned neural correction at the torque level.  The whole‑body kinodynamic MPC is solved at 100 Hz for **thousands of agents in parallel** during RL training (IsaacGym).  A 3‑layer MLP, initialized to zero, is trained with PPO; its output is added to the MPC’s joint‑torque command via a scaling factor λ (Section IV‑B).  The result is a controller that combines MPC’s interpretability and constraint handling with RL’s adaptability: higher asymptotic rewards, expanded velocity command range, zero‑shot adaptation to unseen gaits and uneven terrain, and improved sim‑to‑real fidelity.

---

## Introduction & Motivation  
* MPC provides transparent, physics‑grounded control but is fragile to **model mismatches** (friction, contact timing, compliance) because errors accumulate between replanning steps.  
* RL eliminates explicit models, but its **black‑box policies** are hard to debug, and reward‑shaping drives sample‑inefficiency.  
* Prior work has blended MPC and RL hierarchically or via learned dynamics;  residual‑policy learning (Residual Policy Network) has shown sample‑efficiency gains in other domains.  
* The unique contribution: **parallel GPU‑MPC** integrated into the RL loop at 100 Hz, enabling many agents (2048) to solve a *kinodynamic whole‑body MPC* in every Monte‑Carlo roll‑out (Section III‑A/B).

---

## Methods / Approach  

| Category | Content |
|---|---|
| **MPC Formulation** | Whole‑body floating‑base dynamics reduced to the base coordinates (Eq. 5).  Dynamics: \(M_b\ddot q + h_b = J_b^T F\).  Variables: \(q,\dot q, F\).  Objectives: base–joint position/velocity tracking, foot height Bezier curve, contact friction constraints, joint limits.  No self‑collision constraints. |
| **Solver** | Real‑time iteration: single SQP → quadratic program (QP).  QP solved with OSQP (alternating direction ADMM).  Factorisation via cuDSS (CUDA).  Algorithm 1 details the kernel generation (using CasADi/CusADi) and the QP solve steps. |
| **Parallelisation** | All 2048 MPC solves encoded in a single GPU kernel per timestep.  After factorising KKT once, subsequent linear solves reuse the LDL data (Algorithm 1, Fig. 5).  Carries exactly the equal memory and compute load as CPU‑only environments but avoids CPU‑GPU transfer latency. |
| **Residual Policy** | 3‑layer MLP (256‑ELU units).  State vector \(o=[p⊕θ⊕q⊕j\omega⊕v⊕\dot q ⊕ j\phi⊕V_{MPC}]\) (54‑dim).  Outputs leg torques \(a∈ℝ^{10}\).  Action blending options (Section IV‑B) – chosen joint‑torque, torque‑blending with scalar λ=0.1. |
| **RL Loop** | PPO with GAE (Algorithm 2).  10,000 episodes; 24 simulation steps per PPO iteration.  Reward vector (Table I) comprises only *sparse* physics‑aligned terms (velocity tracking, action rate, torque penalty, height, self‑collision penalty, termination). |
| **Evaluation** | Command set: v_x∈[−1.5,1.5], v_y∈[−1.5,1.5], ω_z∈[−1.0,1.0] rad/s.  On‑policy performance assessed by 95 % kernel density of achieved commands (Fig. 10).  Comparison to pure MPC and end‑to‑end RL. |  

---

## Experiments / Data / Results  

| Experiments | Key Findings |
|---|---|
| **GPU‑MPC Speed** | Full 2048 solves at 100 Hz require ~0.5 s per simulator step.  GPU speed‑up ~2.5× compared to perfect CPU 10‑core run (Fig. 5). |
| **Solver Iteration Sensitivity** | Survival vs. QP iterations (Fig. 4): ≈25 iterations suffice; additional iterations give diminishing returns. |
| **Blending Strategy** | Joint‑torque vs. torque‑torque ways: *joint‑torque* produces best sample‑efficiency; torque‑torque is similar, cable‑torque worst (Fig. 6). |
| **Reward Curve** | Residual outperforms pure MPC and end‑to‑end RL in reward vs. sample (Fig. 7). |
| **Behavioral Qualities** | Residual policy shows *lower extremes* of joint velocities, torques, power, and ground‑reaction forces (Fig. 8).  End‑to‑end policy uses *glide* style, unrealistic for hardware. |
| **Coverage of Commands** | 95 % kernel density: residual covers v_x +78 %, v_y +12 %, ω_z +9 % over MPC baseline (Fig. 10). |
| **Self‑collision Mitigation** | MPC fails (Δ self‑collision ≈1) whereas Residual brings penalty to ≈0 across trials (Fig. 11). |
| **Zero‑shot Adaptation** | • *Gait change*: altering MPC’s contact schedule (double stance / flight) fails for MPC, succeeds for Residual (Fig. 12).  • *Uneven terrain*: Residual traverses rough & step terrain without retraining; MPC crashes (Fig. 13).  • *Swing‑height adjustment*: increasing Bezier swing height from 0.075 to 0.15 m: residual adapts instantly (Fig. 13). |
| **Hardware Validation** | MIT Humanoid (25 kg) real‑time MPC + residual policy at 100 Hz; stable forward, lateral, turning locomotion; video evidence (Fig. 14). |  

---

## Discussion & Analysis  

1. **Residual Policy Effectiveness** – The MPC prior acts as a *warm‑start*, keeping the policy near feasible dynamics and thus improving exploration (Figure 8).  
2. **Torque‑Ratio Dynamics** – Residual torque magnitude is higher for forward commands, especially near stance‑to‑swing transitions, implying the policy actively modifies MPC’s stance behaviour (Fig. 15).  
3. **Actuator Alignment** – Cosine similarity between residual and MPC torques peaks at transitions ≈-1 (anti‑aligned), showing corrective behaviour (Fig. 16B).  
4. **Greedy vs. Reactive Correction** – Residual learns *heel‑toe* end‑of‑stance effects even though not in reward (Fig. 17).  
5. **GPU‑Parallelization Trade‑offs** – Additional QP iterations yield diminishing returns; latency dominated by KKT factorisation (see Table II).  
6. **Sample‑Efficiency vs. Wall‑Clock** – Residual 3–4× longer in wall‑clock (hours) compared to end‑to‑end RL (30 min) (Section V‑B) due to MPC solution cost, but sample‑efficiency is far better.  

---

## Conclusions  

- A *GPU‑parallelised whole‑body MPC* can be efficiently embedded into an RL loop at 100 Hz for thousands of agents per simulation step.  
- A *residual policy* that outputs torque corrections learns *fast* (thousands of episodes) and reaches higher rewards while maintaining interpretable, physically‐plausible behaviours.  
- The MPC prior **biases policy search** away from unrealistic RL‑only solutions, reduces reward engineering needs, and delivers robust zero‑shot adaptation to unseen gaits or terrain.  
- The combined controller successfully transfers to a real MIT Humanoid (Section V‑D).  

---

## Key Claims & Contributions  

| Claim | Supporting Evidence |
|---|---|
| **1.** *GPU‑parallel MPC at 100 Hz for RL* – Algorithm 1, Tables II, Fig. 5. |
| **2.** *Residual policy outperforms pure MPC & end‑to‑end RL* – Figures 7–10, reward curves. |
| **3.** *Residual policy learns self‑collision avoidance without explicit constraints* – Fig. 11. |
| **4.** *Zero‑shot adaptation to changed gaits and uneven terrain* – Fig. 12–13. |
| **5.** *Hardware validation on MIT Humanoid* – Fig. 14, video link. |

---

## Definitions & Key Terms  

| Term | Definition |
|---|---|
| **MPC** | Optimal control over a finite horizon subject to dynamics and constraints. |
| **Residual Policy** | A network trained on the residual of a baseline controller’s output (π =π₀ +π_res). |
| **Kinodynamic MPC** | MPC that includes kinetic constraints but reduces full body dynamics to the base coordinates. |
| **CasADi / CusADi** | Symbolic differentiation tools to generate CUDA kernels. |
| **cuDSS** | NVIDIA library for direct sparse LDL factorisation and solve. |
| **OSQP** | Operator‑splitting QP solver used on CPU for deployment. |
| **Self‑collision** | Contact forces between robot links detected during simulation. |
| **λ (lambda)** | Scaling factor of residual policy output when blending torques. |

---

## Important Figures & Tables  

| Figure / Table | Description |
|---|---|
| Fig. 1 | Comparison of MPC vs. residual torque behavior at ±velocity commands. |
| Fig. 2 | Residual policy architecture diagram. |
| Fig. 3 | Bezier swing‑height curve. |
| Fig. 4 | Survival vs. QP iterations. |
| Fig. 5 | GPU vs. CPU run time for 1000 environments. |
| Fig. 6 | Reward comparison for blending strategies. |
| Fig. 7 | Reward curves (MPC, residual, end‑to‑end). |
| Fig. 8 | Distribution of joint velocity, torques, mechanical power. |
| Fig. 9 | UMAP state evolution start vs. end of training. |
| Fig. 10 | 95 % kernel density of trackable commands. |
| Fig. 11 | Self‑collision penalty over turning. |
| Fig. 12 | Gait parameter sweep – residual vs. MPC. |
| Fig. 13 | Terrain adaptation – residual vs. MPC. |
| Fig. 14 | Hardware validation video images. |
| Fig. 15 | Ratio of residual to MPC torque across commanded velocities. |
| Fig. 16 | Residual torque ratio vs. contact phase; cosine similarity. |
| Fig. 17 | Residual vs. MPC data on uneven terrain: torque, height, velocity. |
| Table I | Reward functions & weights. |
| Table II | GPU‑parallelised MPC function counts & timings. |

---

## Limitations & Open Questions  

1. **Computational Overhead** – While GPU parallelization is efficient, wall‑clock training is still ≈8× slower than end‑to‑end RL, limiting scalability to larger networks or higher sample counts.  
2. **Solver Assumptions** – Real‑time iteration with single SQP step may not fully converge; however, empirical analysis shows it's sufficient for stability under simple initial disturbances.  
3. **MPC Prior Design** – The chosen kinodynamic formulation omitted self‑collision constraints; residual policy learns to compensate, but further MPC‑level constraints might further simplify learning.  
4. **Domain Gap** – Despite promising sim‑to‑real results, differences in contact dynamics remain; further fine‑tuning or additional model‑based constraints could reduce sim‑real variance.  
5. **Policy Complexity** – MLP architecture is minimal; richer architectures (e.g., attention or hierarchical modules) might capture more global dynamics.  

**Open Problems**:  
- *Adaptive blending factor λ*: e.g., learning λ or adapting it online.  
- *Policy‑based MPC parameter learning*: let the MLP output cost weights or constraint thresholds.  
- *Uncertainty‑aware MPC*: using value estimates of MPC solutions as uncertainty signals for balancing MPC vs. policy torque.  

---

## References to Original Sections  

* Methods: Section III (MPC formulation & parallelisation), Section IV (Residual architecture).  
* Experiments: Section V (GPU‑MPC characteristics, reward curves, behavior analysis).  
* Figures: See “Important Figures & Tables” above.  

---

## Executive Summary (Optional)  

- GPU‑parallelised whole‑body MPC solved at 100 Hz for thousands of agents.  
- Residual policy outputs torque corrections; minimal reward engineering required.  
- Achieves higher asymptotic reward, expands velocity command range, adapts zero‑shot to unseen gaits and terrain.  
- Sim‑to‑real transfer validated on MIT Humanoid.  
- Residual policy largely anti‑aligned with MPC torques near contact transitions, explaining robust ankle behaviours.  
- Future work: policy‑based MPC parameter learning, uncertainty‑aware blending, scaling to larger networks.  

---