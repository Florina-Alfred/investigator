**Title & Citation**  
*Residual MPC: Blending Reinforcement Learning with GPU‑Parallelized Model Predictive Control*  
Authors: Se Hwan Jeon, Ho Jae Lee, Seungwoo Hong, Sangbae Kim (MIT Harvard)  
Conference/Workshop: *to be determined*  

---

## Abstract  
- Demonstrates a *GPU‑parallelized residual policy* that blends MPC torques with a learned residual network at the torque level.  
- Uses a *kinodynamic whole‑body MPC* evaluated in parallel for thousands of self‑contained agents at 100 Hz during PPO training.  
- Residual policy learns *targeted corrections* rather than full policies, combining interpretability (MPC) and adaptability (RL).  
- Achieves higher sample‐efficiency, higher asymptotic rewards, a wider range of trackable velocity commands and zero‑shot adaptation to unseen gaits/uneven terrain compared to standalone MPC or end‑to‑end RL.  

---

## Introduction & Motivation  

| Theme | Key Points | References |
|-------|------------|------------|
| **Model‑Predictive Control (MPC)** | • Grounded in physics; explicit predictions; interpretable; but sensitive to model mismatches (terrain, friction, actuator dynamics) and needs frequent replanning. | [1–7] |
| **Reinforcement Learning (RL)** | • Learns robust policies from diverse sim conditions; but policies are black‑box, hard to debug; reward design is delicate; may exploit simulation artifacts. | [9–14] |
| **Hybrid MPC‑RL** | • Idea: MPC gives a structured baseline; RL corrects it. Prior work: hierarchical (policy controls MPC parameters), tracking approaches, embedding learning constraints. | [19–22] |
| **Residual Policy** | • Networks output corrections to a nominal controller via a “skip” connection; improves sample efficiency & convergence in robotics. | [23–26] |
| **Bottleneck of MPC in RL loop** | • Solving MPC at high frequency over thousands of agents is computationally heavy. Prior work largely used CPU clusters, low frequency (2–3 Hz). | [20] |
| **Our contribution** | • GPU‑parallelized MPC with operator‑splitting QP solver (OSQP) + casadi+cuDSS → MPC at 100 Hz for 2048 agents in IsaacGym. | This work |
| **Goal** | • Show that a residual network can learn to correct MPC torques, granting extended command ranges, self‑collision avoidance, and zero‑shot robustness to terrain / gait changes. | - |

---

## Methods / Approach  

### 1. MPC Formulation  

- Floating‑base dynamics:  
  \[ M(q)\ddot{q} + h(q,\dot{q}) = J(q)^\top F + S^\top \tau \]  
  *\(M,\;h,\;J,\;S\)* defined for robot configuration \(q\).  
- Simplify: only base dynamics (rows of \(S_{\rm base}\)) are used → *kinodynamic* MPC.  

| *Decision Variable* | *Dimension* | *Purpose* |
|---------------------|--------------|------------|
| \(q,\,\dot{q},\,F\) | vector of length \(n_{\rm vars}\) | State trajectory and contact forces |
| \(T=12\) | horizon length (0.15 s for 100 Hz) | Prediction horizon |  

- **Constraints**  
  - Dynamics enforced via integration equations.  
  - Contact force limits: \(\|F_{x,y}\| \le \mu\,F_z\) (friction cone).  
  - Contact schedule via phase variable \(\phi\) (0.5 threshold).  
  - Swing foot height via 5th‑order Bézier curves (Eq. 12).  
  - Joint limits \(q_{\rm joint},\,\dot{q}_{\rm joint}\).  

```text
   • Initial guess: nominal pose repeated over horizon.  
   • θ_MPC updated with command (desired height, v_x, v_y, ω_z).  
   • QP solved by OSQP (ADMM).  
   • The resulting first–step state fed to inverse dynamics (RNEA) to get feedforward torques τ_MPC.  
```

### 2. Solver Strategy  
- Use **Operator‑Splitting Quadratic Program (OSQP)**: minimal per‑iteration symbolic work, ADMM iterations, only factorization needed once per MPC solve.  
- KKT system for each MPC: \( \begin{bmatrix} P & A^\top \\ A & 0 \end{bmatrix} \) solved via **cuDSS** (CUDA Direct Sparse Solver).  
- **CusADi** code‑generates kernels for all other operations; they run in parallel across all agents.  
- **Real‑time iteration**: only one QP iteration per MPC step (fast enough for 100 Hz).  

### 3. Residual Policy Architecture  
- A 3‑layer MLP, each with 256 ELU units.  
- Input vector:  
  \[ o = [ p,\; \theta,\; q,\; j,\;\omega,\; v,\; \dot{q},\; j,\;\phi,\;V_{\rm MPC}] \in \mathbb{R}^{54}\]  
  *p*: positions, *q*: velocities, etc.  
- Output: leg torque vector \(a\in\mathbb{R}^{10}\).  
- **Blending strategies** (Eq. 18–20) – tested:  
  1) joint‑joint\n 2) joint‑torque\n 3) torque‑torque.  
  *Found: joint‑torque with scaling λ=0.1 was most robust.*  

### 4. Reward Design  
- Minimal, consistent with MPC cost; no sophisticated shaping, only sparse/non‑differentiable terms:  
  - Self‑collision penalty (±1).  
  - Termination penalty (−100).  
  - Action‑rate regularization.  
  - Linear/angular velocity tracking, orientation stability, height tracking, joint regularization.  
  - Torques penalty.  

```
   R(s,a) = Σ_{rewards} w_i * f_i(s,a)
```  

### 5. Training Pipeline  
- IsaacGym sim (GPU physics) – 2048 envs per rollout.  
- PPO with clipped objective (stable policy update).  
- 100 Hz control frequency shared by MPC and policy.  

---

## Experiments / Data / Results  

| Experiment | Setup | Key Findings |
|------------|-------|---------------|
| **GPU‑parallelization** | 1000 agents @ 100 Hz | 492 ms per MPC step on RTX‑3090; ~2.5× speedup vs CPU (10‑core). |
| **QP‐iteration sweep** | Randomized initial velocity (∥v∥≤0.5 m/s, ∥ω∥≤0.5 rad/s) | Survival rate saturates around N_QP=25; further iterations yield diminishing returns. |
| **Residual policy vs baselines** (PPO training for 10k iterations) | 3 agents: MPC (fixed), Residual, End‑to‑End | Residual: higher asymptotic return, 3× faster reward convergence than end‑to‑end. |
| **Command range** | Uniform velocity commands (v_x,v_y,ω_z) | Residual extends viable range by 78 % (v_x), 12 % (v_y), 9 % (ω_z). |
| **Self‑collision** | MPC vs Residual on turning (ω_z=2.5 rad/s) | MPC collides ~20 % of time; Residual eliminates penalty after training. |
| **Out‑of‑distribution gaits** | MPC contacts altered to double‑stance and flight phase after training | Residual follows new gait with zero extra training. |
| **Uneven terrain** | Step elevations 0.075–0.15 m; slope 5° | Residual traverses terrain flawlessly; MPC fails immediately. |
| **Hardware validation** | MIT Humanoid (25 kg, 10 leg joints + 8 arm joints) | Real‑time MPC on CPU (OSQP) + residual torques yields stable walking in forward/lateral/turning motions; no catastrophic events. |

---

## Discussion & Analysis  

### 1. Residual‑MPC Interaction  
- **Torque ratio** (∥τ_res∥/∥τ_MPC∥) is asymmetric: higher when commanded forward velocity is high, lower for lateral/turning combos that risk knee collision.  
- **Cosine similarity** (Fig. 16B) shows residual torques *almost opposite* the MPC torques especially near contact switches (φ≈0.5).  
- **Interpretation**: residual policy frequently substitutes “risk‑averting” ankle strategies (toe‑off, heel‑touch).  

### 2. Residual Advantages  
- **Warm‑start**: The MPC prior biases early exploration; reward signals alone (without TORUS) failed to guide a vanilla RL policy successfully.  
- **Less hyper‑parameter tuning**: Minimal reward shaping yields robust policies; RL alone required many reward tweaks to avoid exploitation of simulation artifacts.  
- **Adaptability**: Residual modifications automatically generate new contact schedules/gaits even if unseen during training; implies residual can compensate for inaccuracies in the simplified dynamics and contact model.  

### 3. Computational Trade‑offs  
- Training wall‑clock time ≈ 3–4 h (1000 agents) vs 30 min for end‑to‑end RL.  
- Nevertheless, *higher sample efficiency* reduces number of PPO iterations needed for a given reward.  
- Proposed *Operator‑Splitting* is critical; factorization with cuDSS costs ~80 ms but is only once per solve.  

### 4. Limitations & Open Issues  
- **Solver scalability**: Adding more sophisticated constraints (e.g., collision constraints) would inflate per‑iteration count, potentially breaking real‑time 100 Hz.  
- **Feed‑forward torques only**: The MPC solution is only used to compute τ_MPC (first step). Future work: incorporate full prediction beyond the first step (e.g., via belief state).  
- **Domain shift**: Transfer from simulation to real robot required separate evaluation with more accurate contact dynamics; still shows positive sim‑to‑real gap.  
- **Generalisation scope**: Only leg torques are modified; enabling arm torques or camera‑based sensing remains an open research channel.  

---  

## Conclusions  

- **Residual MPC** bridging MPC and RL at torque level yields controllable, physically consistent, and robust locomotion on the MIT Humanoid.  
- **GPU‑parallelization** of kinetic MPC makes RL training at 100 Hz feasible on a single desktop.  
- Residual policy improves reward convergence, extends command range, eliminates self‑collision, and adapts to unseen gaits/terrain with zero extra training.  
- **Future work**:  
  1) extend residual network to output MPC parameters (costs, schedules).  
  2) use MPC value to estimate uncertainty, modulate weight λ.  
  3) design MLPs that emulate factorization / ADMM steps → reduce solver cost.  

---  

## Key Claims & Contributions  

- **Claim:** A GPU‑parallelized kinodynamic MPC can be solved in real‑time (100 Hz) for thousands of agents simultaneously.  
  **Evidence:** Table II shows per‑step evaluation time ≈ 493 ms for 1000 agents on RTX‑3090; 2.5× speedup vs CPU.  

- **Claim:** Residual policy trained with only the MPC prior and minimal NLP‐style rewards converges faster and to higher returns than end‑to‑end RL.  
  **Evidence:** Fig. 7, Fig. 6 – reward curves show residual ∼3× faster ascent; Fig. 3 shows MPC basis remains constant while residual improves.  

- **Claim:** Residual MPC extends the controllable velocity command space by +78 % (forward), +12 % (lateral), +9 % (yaw).  
  **Evidence:** Fig. 10 – 95 % kernel density estimates.  

- **Claim:** Residual MPC eliminates self‑collision when turning, while MPC persists.  
  **Evidence:** Fig. 11 – self‑collision penalty vs time.  

- **Claim:** Residual MPC zero‑shot adapts to unseen gaits and uneven terrain.  
  **Evidence:** Fig. 12, 13 – trajectories unchanged or improved on modified phase schedule and step heights.  

---  

## Definitions & Key Terms  

| Term | Definition |
|------|------------|
| **MPC (Model Predictive Control)** | Optimization of a cost over a finite horizon subject to system dynamics & constraints. |
| **Kinodynamic MPC** | MPC where only the floating‑base dynamics are enforced; kinematic constraints (joint limits) still considered. |
| **Residual Policy** | Neural network that outputs corrections to a nominal controller via a shortcut; initialised to zero to not break the prior. |
| **Operator‑Splitting Quadratic Program (OSQP)** | ADMM‑based QP solver that updates primal & dual using QP KKT system. |
| **cuDSS** | NVIDIA CUDA Sparse Direct Solver for LDL factorization/solve. |
| **Self‑collision** | Contact force detection between any two robot links; penalised in reward. |
| **Termination** | Flag raised when base height, orientation, or velocity deviates beyond bounds. |
| **Torque‑torque blending** | Strategy where residual network outputs torques directly, blended with MPC torques. |
| **Hybrid cost** | Weighted sum of squared terms: velocity tracking, orientation, height, joint regularization, torques, action rates. |
| **Estimate of MPC value** | Output of ocp solver’s cost; can be used as an uncertainty signal. |

---  

## Important Figures & Tables  

| Label | Content | Importance |
|-------|---------|------------|
| **Fig. 1** | MPC vs residual animation – residual prevents self‑collision. | Illustrates qualitative gain. |
| **Fig. 2** | Residual architecture diagram – shows blended torques. | Key system architecture. |
| **Fig. 3** | Foot height Bézier constraints. | Visualises swing profile. |
| **Fig. 4** | MPC survival rate vs QP iterations. | Shows diminishing returns after N_QP=25. |
| **Fig. 5** | CPU vs GPU evaluation time comparison. | Validates speedup. |
| **Fig. 6** | Reward curves for different blending strategies. | Justifies choice of joint‑torque blending. |
| **Fig. 7** | Reward over training for residual vs end‑to‑end. | Evidence of faster convergence. |
| **Fig. 8** | Snapshots of residual vs end‑to‑end behaviours. | Visual comparison. |
| **Fig. 9** | UMAP of state distributions – shows residual stays near MPC. | Evidence of cold‑start effects. |
| **Fig. 10** | Velocity command vs achieved error. | Quantifies expanded command range. |
| **Fig. 11** | Self‑collision penalty over time for MPC vs residual. | Confirms residual eliminates collisions. |
| **Fig. 12** | Gait parameter changes – residual still works. | Zero‑shot gaits. |
| **Fig. 13** | Uneven terrain traversals. | Zero‑shot terrain adaptation. |
| **Fig. 14** | Hardware deployment sequence. | Demonstrates real‑world transfer. |
| **Fig. 15** | Ratio of residual/MPC torques vs commanded velocities. | Highlights asymmetric assistance. |
| **Fig. 16** | Phase‑dependent torque ratio & cosine similarity. | Shows residual antagonism near contact switches. |
| **Fig. 17** | MPC vs residual analysis over uneven terrain – reveal ankle toe‑off. | Illustrates learnt safe behaviours. |
| **Table I** | Reward terms and weights. | Reference for training objective. |
| **Table II** | Timing of MPC sub‑functions. | Shows computational distribution. |

---  

## Limitations & Open Questions  

1. **Scalability of constraints** – Adding collision constraints or more complex friction models would increase N_QP, potentially surpassing 100 Hz real‑time.  
2. **Accuracy of first‑step torques** – the MPC solution only provides τ_MPC for the next step; effect of future states not considered.  
3. **Generalisation to other robots** – The hierarchy of vanilla and residual policy may need adaptation for different kinematics or actuator counts.  
4. **Uncertainty estimation** – Using MPC value as an uncertainty proxy is suggested; requires systematic exploration.  

---  

## References to Original Sections  

- **Section I** – Introduction.  
- **Section II** – Lateral: MPC definition, residual network.  
- **Section III** – MPC formulation; Fig. 3; Algorithm 1.  
- **Section IV** – Residual architecture, blending, reward design.  
- **Section V** – Experiments: GPU‑parallelization (Fig. 4, 5), training (Fig. 6, 7), performance (Fig. 10‑13).  
- **Section VI** – Residual policy analysis (Fig. 15‑17).  
- **Section VII** – Conclusions.  

---  

## Executive Summary (Bullet Points)  

- GPU‑parallelized kinodynamic MPC solved in 100 Hz for 2048 agents.  
- Residual policy (MLP) learns to correct MPC torques; keeps MPC’s physical interpretability.  
- Residual architecture achieves higher rewards, reduces sample inefficiency, and extends command capabilities.  
- Residual policy learns to avoid self‑collision and adapts to unseen gaits/terrain without additional training.  
- Residual outputs are largely *antagonistic* to MPC near contact switches, especially ankle articulation.  
- Real‑world deployment on MIT Humanoid demonstrates stable locomotion.  
- Residual policies provide a natural warm‑start for RL, reducing reward engineering effort.  
- Suggest future research: dynamic MPC parameter adaptation, uncertainty estimation, and solver‑inspired network architectures.  

---  

## Supplementary Material (Appendix)  

- **Appendix A** – Transformation of joint torques to 6‑D ground‑reaction wrench.  
- Detailed derivation of quasi‑static mapping, Jacobian inversion, and enforcement of contact directions.  
- Used to interpret advantages observed in Fig. 17 (toe‑off, heel‑touch).  

---  

*End of summary.*