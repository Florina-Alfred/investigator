**Title & Citation**  
*MAKO: Meta‑Adaptive Koopman Operators for Learning‑Based Model Predictive Control of Parametrically Uncertain Nonlinear Systems*  
Minghao Han, Kiwan Wong, Adrian Wing‑Keung Law, Xunyuan Yin – Nanyang Environment & Water Research Institute, NTU, Singapore (and ETH Zurich, Switzerland, and National University of Singapore).  
This work appears in the IEEE Transactions on Automatic Control (exact volume/issue not given in the source).  

---

## Abstract  
The authors present **MAKO** (Meta‑Adaptive Koopman Operator), a meta‑learning framework that learns a Koopman representation from a multi‑modal data set and adaptively refines it online for systems whose dynamics depend on unknown parameters.  MAKO can be used for model‑predictive control (MPC); the associated closed‑loop system is shown to be stable even for previously unseen parameters.  Simulations on three benchmark nonlinear systems (cart‑pole, gene‑regulatory network, and a 3‑tank chemical reactor) demonstrate superior modeling and control performance compared to a state‑of‑the‑art baseline (DeSKO).  

---

## 1 Introduction & Motivation  
1. **Parametric uncertainty** in nonlinear dynamic systems (e.g., payload variations) often induces performance loss or instability, motivating adaptive control [1,2].  
2. **Model‑Predictive Control (MPC)** is effective for constrained systems, but adaptive MPC (AMPC) has limited success for general nonlinear dynamics [3–10].  
3. **Koopman Operators** linearise nonlinear dynamics in a higher‑dimensional observable space [11–13] and are now being fused with machine‑learning to build data‑driven models [15–22].  
4. Existing Koopman‑based methods focus on a fixed set of parameters; they do **not** give an online learning scheme that generalises to new parameter values.  
5. The authors observe that parametric uncertainty can be treated as a **multi‑task learning problem**, which aligns with **meta‑learning** (“learning‑to‑learn”) [25].  
6. **Goal of the paper** – integrate meta‑learning with Koopman theory to produce a **meta‑adaptive model** that can be updated on‑the‑fly and a controller that preserves stability for unseen tasks.  

**Contribution claim** (end of Introduction, Section 1):  
- First integration of metalearning and Koopman operators for general parametrically uncertain nonlinear systems.  
- Rigorously prove convergence of the online adaptation and stability of the closed‑loop.  
- Demonstrate superior performance to competitive baselines on three benchmark systems.  

---

## 2 Preliminaries  

### 2.1 Meta‑learning  
- View each parameter setting \( \Theta_i \) as a distinct learning task.  
- Introduce the standard meta‑learning loss  

  \[
  \min_{\theta}\sum_{i=1}^{N}\mathcal{L}(D_i;\theta)
  \]

  where \(D_i=\{[x_{ik},u_{ik}]\}_{k=1}^T\) are time‑series from task i.  
- Goal: find parameters \( \theta, (A_i,B_i,C_i) \) that minimise the multi‑step prediction error (discussed in section 3.1).  

### 2.2 Koopman Operator  
- For an autonomous system \(x_{k+1}=f(x_k)\), introduce observables \( \varphi\in \mathcal{G}\) such that  

  \[
  \varphi\!\circ\!f(x_k)=K\varphi(x_k),
  \]  

  where \(K:\mathcal{G}\rightarrow\mathcal{G}\).  
- For controlled systems, the operator satisfies  

  \[
  \varphi_x \!\circ\!f(x_k,u_k)=A\,\varphi_x(x_k)+B\,u_k,\qquad
  \varphi_u(x_k,u_k)=u_k. 
  \]  

- In practice a finite‑dimensional sub‑space \( \mathcal{G}\subset\mathcal{G}\) is used, defined by an observable dictionary \(\psi=\{ \psi_l\}_{l=1}^h\).  

---

## 3 Methods / Approach  

### 3.1 Meta‑trained Koopman Model  

The MAKO architecture (Fig. 2) has two modules:  

1. **Meta‑Trained Neural Network (MNN)** – encodes state \(x_k\) into a shared observable vector  

   \[
   \psi_\theta(x_k)=\phi_\theta(x_k) \in \mathbb{R}^h.
   \]  

   The same MNN processes data from all tasks, encouraging a common latent space.  

2. **Task‑specific Linear Koopman Operators** – for each task \( \Theta_i \) a distinct triple \((A_i,B_i,C_i)\) is learned to propagate observables and reconstruct the state.\[  
   g_{k+1} = A_i\,g_k+B_i\,u_k, \quad
   \hat x_{k+1}=C_i\,g_{k+1}\]  

   with \(g_k=[\psi_\theta(x_k);u_k]\).  

#### 3.1.1 Meta‑Learning Objective  
The loss to minimise over the entire meta‑dataset \( \mathcal{D}_\Theta=\{D_i\}\) with horizon \(H\) is  

\[
\mathcal{L}(\theta,\{A_i,B_i,C_i\})=\frac{1}{N}\sum_{i=1}^{N}
\sum_{k=1}^{T-1}\Bigl\|g_{k+1}-(A_i,B_i)(\psi_\theta(x_k),u_k)\Bigr\|_2^2+\bigl\|x_{k+1}-C_i\,g_{k+1}\bigr\|_2^2.
\]  

Training uses Adam with the hyper‑parameters listed in **Table 1** (Appendix).  

### 3.2 Online Adaptation  

After meta‑training, a new task (parameter setting unknown at training time) is observed using online data \(x_k,u_k\).  

1. **Initial Guess** – average of all learned operators  

   \(\hat A_0=\frac{1}{N}\sum_{i}A_i,\quad\hat B_0=\frac{1}{N}\sum_{i}B_i,\quad\hat C_0=\frac{1}{N}\sum_{i}C_i.\)  

2. **Prediction Error** –  

   \[
   \tilde g_{k+1}=g_{k+1}-\hat\Psi_k X_k,\qquad \tilde x_{k+1}=x_{k+1}-\hat C_k g_{k+1}
   \]  

   where \(\hat\Psi_k=[\hat A_k,\hat B_k]\).  

3. **Gradient‑Based Update** –  

   \[
   \hat\Psi_{k+1}= \hat\Psi_k-\lambda_k \nabla_{\hat\Psi}_k \mathcal{J},\qquad
   \hat C_{k+1}= \hat C_k-\lambda_k \nabla_{\hat C}_k \mathcal{J},
   \]  

   with cost \(\mathcal{J}(\hat\Psi_k,\hat C_k)=\|\tilde g_{k+1}\|_2^2+\|\tilde x_{k+1}\|_2^2\).  

4. **Adaptive Learning Rate** – Eq. (10)  

   \[
   \lambda_k=\begin{cases}\alpha\frac{\|X_k\|_2^2}{\|X_k\|_2^2+\|g_{k+1}\|_2^2}&\text{if }\|X_k\|_2^2>0\\
   0 & \text{otherwise}\end{cases},
   \]  

   with \(\alpha\in(0,2)\).  

#### 3.2.1 Nominal Adaptation Theory  

- **Assumptions**:  
  1. \(U\) is bounded and forward‑invariant.  
  2. \( \psi_\theta(\cdot) \) and system dynamics \(f(\cdot)\) are continuous.  
  3. For every \( \Theta\), there exist exact Koopman operators \((A,B,C)\) satisfying Eq. (5).  

- **Theorem‑1** (Section 3.3.1) – with update laws (9)–(10) the parameter errors \( \tilde\Psi_k,\tilde C_k\) are ultimately bounded and the state‐prediction error \( \tilde x_k \rightarrow 0\).  

  *Proof Outline*:  
  - Lyapunov \(V_k=\tr(\tilde\Psi_k^T\tilde\Psi_k)+\tr(\tilde C_k^T\tilde C_k)\).  
  - Using Eq. (9–12) shows \(V_{k+1}\le (1-\alpha\lambda_k)V_k\).  
  - Summation over \(k\) gives \(\sum \lambda_k(\|\tilde g_{k+1}\|^2+\|\tilde x_{k+1}\|^2)\le V_1\), implying convergence.  

#### 3.2.2 Robust Adaptation Theory  

When exact operators may not exist (\(w\neq0,v\neq0\) residuals), an extended cost  

\[
\bar{\mathcal{J}}=\|\tilde g_{k+1}-w_k\|^2+\|\tilde x_{k+1}-v_k\|^2
\]

is minimized w.r.t. auxiliary variables \(w_k^*,v_k^*\). Update laws (15)–(16) are derived similarly, leading to **Theorem‑2** (Section 3.3.2): parameter errors remain bounded and \(\|\tilde x_k\|\le\bar\epsilon_v\).  

---

## 4 Meta‑Koopman‑based Adaptive MPC  

### 4.1 MPC Problem  

With updated operators \((\hat A_k,\hat B_k,\hat C_k)\) the predictive dynamics are  

\[
\hat g_{k+1}=\hat A_k \hat g_k + \hat B_k u_k,\qquad
\hat x_{k+1}=\hat C_k \hat g_{k+1}.
\]

The finite‑horizon optimal control problem (Eq. (19)) is  

\[
\begin{aligned}
\min_{\{u_i\}_{i=0}^{T-1}}&
\sum_{t=0}^{T-1}\| \hat C_k \hat g_{k+t} - x_s\|_Q^2+\|u_t\|_R^2\\
\text{s.t. }&
\hat g_{k+t+1}=\hat A_k \hat g_{k+t}+\hat B_k u_t,\ \forall t< T,\\
&\|u_t\|\le u_{\max}.
\end{aligned}
\]

The first‑stage action \(u_k^*\) is applied at time \(k\).  

### 4.2 Stability Proof  

**Theorem‑3** (Section 4.2): Under Assumptions 1–3 and the MPC of (19) the tracking error of the closed‑loop system is asymptotically stable.  

*Proof Sketch*:  
- Construct Lyapunov candidate using optimal cost \(V_k^*\) (Eq. (20)).  
- Use optimality to show \(V_{k+1}\le V_k - \|u_{k+T+1}\|^2_R\) and bound the prediction deviation \(\delta_t^k\).  
- With Theorem‑1 the operators converge, hence \(\delta_t^k\rightarrow0\) yielding \(V_k\rightarrow0\).  

**Corollary‑1** (section 4.2): If only Assumptions 1–2 hold (no exact Koopman), the tracking error is ultimately bounded.  

---

## 5 Experiments / Data / Results  

### 5.1 Simulation Setup & Baselines  

*Benchmarks* – **Cart‑pole**, **Gene Regulatory Network (GRN)**, **Chemical process** (three tank reactor & separator).  

For each, parametric uncertainty ranges are specified (see Section 5.1.1‑5.1.3).  

**Baseline** – **DeSKO** [41], a deep stochastic Koopman model trained on **nominal** parameters only.  

**Hyper‑parameters** – Detailed in **Table 1** (Appendix): observable dimension, trajectory‐length, noise upper bounds, batch size, learning rate, etc.  

### 5.2 Modeling Performance  

For both MAKO and DeSKO, training uses 400 epochs, Adam, batch size 128.  

- **Figure 3**: Cumulative mean‑squared prediction error (over 16 steps) on validation & test sets for all three systems.  
  - MAKO achieves ≤ 1 × 10⁻² on most models; consistent across random initialisation.  
  - DeSKO slightly outperforms MAKO on GRN (likely because it was trained on nominal parameters).  
  - MAKO better on Cart‑pole and Chemical process.  

*Key claim*: MAKO provides **generalised** modelling while maintaining or slightly reducing accuracy versus a specialised baseline.  

### 5.3 Control Performance  

Two controllers evaluated:  
- **Nominal** MAKO MPC (using updates (9)–(10)).  
- **Robust** MAKO‑Robust, using updates (15)–(16).  

**Procedure** – For each benchmark, 9 random parameter sets (unseen during training) plus one nominal configuration for DeSKO comparison.  

**Figures 4‑6** show cumulative tracking errors and trajectories of \(\|e_k\|_2\) for MAKO vs DeSKO.  

- *Cart‑pole*: MAKO achieves lower cumulative cost than DeSKO; tracks faster & smoother, especially under high pole‑mass/length variations.  
- *GRN*: DeSKO tracks nominal reference for the 3 parameter sets; MAKO maintains low error across all 9 sets, exhibiting robust gene‑control.  
- *Chemical process*: DeSKO fails to stabilise in all 9 settings; MAKO MDR obtains stable transient and comparable or smaller steady‑state error.  

**Computational Time** – MAKO‑Robust requires ~0.0203 s per time step (i7 CPU), illustrating real‑time feasibility.  

**Key claim**: MAKO‑based MPC with robust adaptation yields **faster, more stable transient response** and **steady‑state performance** against the strongest baseline across diverse parameter variations.  

### 5.4 Discussion  

- MAKO’s meta‑trained observable space captures common dynamics across tasks; the linear operators adapt to task‑specific dynamics.  
- The adaptive learning rate (10) removes the requirement of a fixed small rate used in prior works [34], enhancing convergence speed.  
- Despite absence of strict forward invariance, MAKO achieves high robustness (see Remark 4).  

---

## 6 Conclusion  

- **MAKO** successfully fuses meta‑learning with Koopman operators, yielding a **meta‑adaptive model** that generalises to previously unseen parameter settings.  
- **Online adaptation** is guaranteed to converge (Theorems 1 & 2).  
- **Adaptive MPC** built on MAKO is proven stable (Theorem 3) and performs robustly in simulations, outperforming DeSKO.  
- **Future avenues**: real‑world deployment, persistent‑excitation analysis, trajectory‑length impact, higher‑dimensional extensions (Section 6).  

---

## 7 Key Claims & Contributions  

| Claim | Supporting Evidence / Section |
|-------|--------------------------------|
| 1. Personalised Koopman modelling via meta‑learning | Eq. (6)–(7), Fig. 2, Section 3 |
| 2. Adaptive online update with adaptive learning rate | Eq. (9)–(10), Theorem 1, 2 |
| 3. Stability of closed‑loop AMPC | Theorem 3, Corollary 1, Section 4.2 |
| 4. Superior modeling & control performance versus DeSKO | Fig. 3–6, Section 5.2–5.4 |
| 5. Applicability to arbitrary parametrically uncertain nonlinear systems | Benchmarks: Cart‑pole, GRN, Chemical process (Section 5) |

---

## 8 Definitions & Key Terms  

- **Parametric Uncertainty**: unknown scalar or vector parameters \( \Theta\in\Xi\) affecting dynamics.  
- **Metadataset \( \mathcal{D}_\Theta\)**: collection of tasks \( \{ D_i \}_{i=1}^N\) with data sampled under different \( \Theta_i\).  
- **Observable** \( \varphi(x)\): function mapping states to a higher‑dimensional space where dynamics are linearized.  
- **Koopman Operator \( \mathcal{K}\)**: linear operator satisfying \( \varphi\!\circ\!f = \mathcal{K}\varphi\).  
- **MNN (Meta‑Trained Neural Network)**: multi‑layer neural net that parameterises observable map \( \psi_\theta\).  
- **Adaptive Learning Rate \( \lambda_k\)**: defined in Eq. (10), bounded by \( \alpha\in(0,2)\).  
- **MPI (Model‑Predictive Control)**: solves finite‑horizon optimisation based on predicted future states.  

---

## 9 Important Figures & Tables  

| Symbol | Description | Reference |
|--------|-------------|-----------|
| Fig. 1 | Overview of Meta‑Koopman pipeline (from overview). | Section 3.1 |
| Fig. 2 | Detailed pipeline of MNN + tasks operators. | Section 3.1 |
| Fig. 3 | Cumulative prediction error (training, validation). | Section 5.2 |
| Fig. 4 | Tracking error trajectories for Cart‑pole. | Section 5.4 |
| Fig. 5 | Tracking error trajectories for GRN. | Section 5.4 |
| Fig. 6 | Tracking error trajectories for Chemical process. | Section 5.4 |
| Table 1 | Hyper‑parameters for each benchmark. | Appendix |

---

## 10 Limitations & Open Questions  

- **Theorem assumptions**: Existence of exact finite‑dimensional Koopman operators (Assumption 3) may not hold for all nonlinear systems; empirical results show robustness nevertheless.  
- **Persistence‑of‑excitation (PE)**: No formal analysis of requisite excitation signals for learning; potential future work.  
- **Trajectory length**: Effect of horizon \(H\) on meta‑learning not systematically explored.  
- **Scalability**: Experiments limited to moderate‑dimensional systems; extension to high‑dimensional industrial plants remains unsettled.  

---

## 11 References to Original Sections  

- Meta‑learning formulation: Section 2.1  
- Koopman theory: Section 2.2  
- MNN & operators: Section 3.1  
- Lost function: Section 3.1.1  
- Online update law: Section 3.2  
- Adaptive learning rate: Section 3.2  
- Nominal & robust adaptation proofs: Section 3.3.1–3.3.2  
- MPC problem: Section 4.1  
- Stability proof: Section 4.2 (Theorem 3)  
- Simulation setup: Section 5.1  
- Baseline DeSKO: Section 5.2  
- Modeling results: Fig. 3, Section 5.2  
- Control results: Figs. 4–6, Section 5.4  
- Conclusions: Section 6  

---

## 12 Executive Summary (Optional)  

- MAKO learns meta‑adaptive Koopman operators via shared neural observable decoder, enabling fast online adaptation.  
- Adaptive update law with dynamic learning rate guarantees convergence (Theorems 1, 2).  
- AMPC built on MAKO is provably stable (Theorem 3).  
- Simulations on three nonlinear benchmarks show MAKO’s modeling errors ≤ 1 × 10⁻², and robust control performance outperforming DeSKO.  
- Future work will target real‑world deployment, formal PE analysis, trajectory‐horizon trade‐offs, and scalability to high‑dimensional systems.  

---