# Video Information

**Title   ** : ðŸ¤– Deepu Talla, NVIDIA: Ushering in the Age of Physical AI at Automate 2025  
**Uploader** : Automate Show  
**Duration** : 42:07  
**URL     ** : https://www.youtube.com/watch?v=JY6CjLYCG4Y  

---

## Transcription

 Thank you for the warm welcome and thanks Alex for the introduction. I am incredibly fortunate to be representing Nvidia and our vibrant ecosystem partners that are all over Automate here. In fact I've been in this job for little over 10 years and I always felt like working in the future, always working on futuristic things. But for the first time I feel like the future is coming to the present. In fact if you look at automation as an industry robotic automation has been happening for the last 30 years, especially for high volume manufacturing like automotive and electronics. In the last 10 years AI in the form of deep learning has come about and there have been a lot of experiments, a lot of products that have been built trying to bring AI and more autonomy into automation. And we could say it's okay but it's largely the deployments are brittle and some would call it even flaky. It hasn't really taken off because we are trying to solve an impossibly or incredibly tough challenge. What humans can do very well, it's not that easy for AI and robots to do. So today I'm going to share with you some of the technologies that we have been working with for the last several years and I believe they have reached a reasonable maturity and then the talk will focus on some of the technologies and then how our ecosystem partners, all the robotic companies in the world will literally work with every AI company and every robotic company on the planet. Now they're taking advantage of these technologies to bring products and solutions to the market. So that's how I'm going to focus this talk on. So this is just the list of robotic partners that are exhibiting on the show floor here at Automate, close to 50 partners. As I mentioned we are incredibly fortunate to be working with literally every company, the company on the planet. You know, in a think of AI growing up very young, even before I heard the word AI, robotics has always been incredibly cool. Growing up watching Star Wars and Star Trek and looking at all of that. I mean, always felt like when AI came about, robotics is the ultimate application of AI. In fact, the world is physical. If you look at the whole GDP of the world, little over $100 trillion and majority of it is physical, real things, right? However, AI has not come about to the physical world until very recently. So I'm going to share with you why now? What has fundamentally changed now that the trajectory at which we're going to make progress in the next few years is going to be incredibly different than what you've seen in the last 10 years. What I'm going to share with you is some of these technologies. Some of these will be imminently deployable. In fact, some of them are being deployed as we speak. Some of the technologies are a little bit medium term. Say I call it the next one to three years. And some technologies we're working on might take longer than that. And of course, it's very important to realize what we have seen in the last few years, some of these technology revolutions when they happen. They don't happen on a linear curve. They don't happen on even an exponential curve. They happen sometimes in a step function curve. Just look at chat GPT, barely two and a half years old. Think about what happened before chat GPT in the world and now how it's being used. So I believe we are at a similar scenario where we can use those technologies in physical AI and robotics. So I mean, literally everything that we do can be automated, right? From factories to warehouses to the future, of course, everybody talks about human rights and the incredible potential of human rights where it's not inconceivable. Sometimes the future that there's more than one human right per human in the world. So incredible opportunity in front of us. Now let's talk about why now. I think the first one that existed for several years. It's not new. Labor shortages, whether it's a dangerous job or a job that somebody, you know, the new wave of humans do not want to do certain things and then that's perfectly fine, right? So labor shortage is not a new phenomenon. Resuring manufacturing is real. I can give you a great example. In video, we just announced last month half a trillion dollars of manufacturing is going to be brought into the, not into North America, United States and other regions close by over the next four years. So it is real. It is factual. And this is just one company I'm mentioning, right? But you know, you see the trends. It is happening. So it's incredible opportunity for North America, you know, in the US for, you know, as reshoring of manufacturing is going to happen. And the only way this is going to happen is of course, there's going to have to be more autonomy and more automation, right? Okay. And the third thing that's really where, you know, why now there's a change that has happened and the real change is technology has caught up. There's several technologies that have caught up, but I view those as fundamentally two technologies have reached a tipping point in the last 12 months or so. And we've been working on this technology as an industry for a decade if not longer. Okay. And I'm going to share with you what those two technologies are. Number one, physical AI. What does physical AI mean? Of course, it's AI applied to the physical world. But essentially what happened with chat GPT to large language models and in the digital domain if you will, whether it's, you know, you're using it for productivity application, whether you're using it for recommendation, you're using it to summarize an email or, you know, summarize a document. All the things that we use it in the digital, pretty much everybody I'm sure here is using generative AI chat GPT or something equivalent now in your daily life. What if we take those same large language models, apply to other modalities like video, vision, and then take it one step further from vision language models to vision language action models apply to robotics. So that's exactly what, you know, industry all the way from companies to researchers, everybody is laser focused on this now because it's, as you can imagine, physical AI is the largest opportunity in humanity. So the whole industry, the whole ecosystem is moving towards developing physical AI right now. And you see that. You know, you can test it in, you know, you can see that in, you know, funding of companies, in AI companies and where the money is going. A lot of it is now started to go into, you know, creating this foundation show called foundation models for robotics, physical AI. So that's the first technology that has reached a tipping point. Second one is obvious in hindsight, but in a simulation. You know, the, actually the worst way to build a physical AI solution, it's actually in the real world because it's slow, it's not safe and most expensive. Simulation is the right answer because it's faster, safer, cheaper. However, you know, in many industries we use simulation all the way. For example, Nvidia we started building chips, we still built chips today. We never send a chip to manufacturing, and it'd be 100%, unless we 100% tested it in simulation and emulation. Only then we send it to manufacturing, but however in the area of robotics simulation has not been used as much and why? And the reason is simply because what we call the SIM2 real gap. So what we simulate is not really that representative of the real world and what's the point, right? So we've been working on technology, simulation technologies instead Nvidia for well over 15 years. We call it, you know, omniverse. And that also has reached a tipping point in terms of how we connect to all the different tools and the accuracy. So because of these two fundamental technologies, the why now makes sense. So now we are in a completely different trajectory in the next five to 10 years and how we can solve some of these problems. Okay, let's talk about now the one of the biggest challenge we have for robotics. Now imagine this, you're trying to, you know, your job is to build a product. So what do you do? First, you build the product, you test the product, and then you deploy the product, right? Three basic steps. So in the case of AI, the build is called train, right? You train the AI, you test the AI, and you deploy the AI. That's pretty much common sense, that's how you apply it. So you take charge GPT, they trained it, they trained it on the whole internet, and they tested it, and they deployed. But when it comes to robotics, what do you train your AI, what do you train your robot on? Where's the data coming from? It's a massive challenge. So you could say, look at all the YouTube videos. Well, I would be interested to find out what would happen if you train a robot, you know, taking all the YouTube videos. It might do some sort of a dance, but I'm sure it's not going to solve any of our, you know, industrial, industrial automation problems. So data is a big challenge, and in fact, that's where we're spending a lot of time as an industry. And in fact, human data is necessary, but not at all sufficient. And I would argue that in the end, human data will be a fraction of one person. It's not, it's important, but not sufficient. So we can do human demonstration, you can do imitation learning, you can do teleop, you see all of this, you know, people teleop or you can have motion capture suits, all that's good. But that will barely be, you know, a fraction of a person. And then you take that data and you use generative AI, use other technologies to generate hundreds thousands of times more data, what we call synthetic data generation. And that's the first step of creating AI, physical AI into robotics. So data creation step one, training step two, testing step three, deploy step four. And that is the circle of life that goes on into a robot, a fleet of robots for rest of time. Because even once you deploy a robot, these robots, once they're in the field, they're going to be constantly updated. And so we at Nvidia, basically, we are a technology company. We don't build robots. We don't build solutions, but we build the underlying technology. And we work with all the robotics companies on the planet. We integrate our technology into their products and solutions. And that's how it's going to come to market. So what do we do then, fundamental? So here's the tech class. We think it takes three computers to make robots happen. When you talk about a robot, when you go and talk to your kid or, you know, anybody can say, what's the first thing that comes to their mind when you talk about a robot, the physical robot itself, right? And the robot brain. And the robot brain turns out is the final step. That's the deployment. Remember, I talked about the four steps. Creating data. Training AI, testing AI, deploying AI. And the deploying app, deployment finally happens in the, that's the third computer you see at the bottom. That's the runtime. Of course, incredibly important, because that's the end goal. It's the destination. But to get to the destination, the fastest way to get to the destination actually turns out is to do the testing in simulation. So that's what we have at the top left that you see, where we can simulate the robot. And in fact, in simulation, we can do simulation orders of magnitude faster than wall clock time, right? I mean, simulation can be 100 times faster, 1,000 times faster. In fact, you can run thousands of experiments in parallel. And of course, simulation is safe and cheap, because you can break things in simulation. It's okay. I mean, until you're sure that, you know, your, your, your, uh, autonomy, whatever, your algorithm, your technologies, good in simulation, you don't have to test it in the real world. Of course, it's always important to test it in the real world once you're sure offered in simulation. So we have three computers. Train the AI typically happens in large scale. Test it in simulation, deploy in the real world, and then that's, that's the loop. So to summarize, to summarize all of our technology, I have a video that I'm going to play from a very wise man. Take a look at it. Everything that moves will be autonomous. Global AI will embody robots of every kind in every industry. Three computers built by Nvidia enable a continuous loop of robot AI simulation, training, testing, and real world experience. Training robots requires huge volumes of data. Internet scale data provides common sense and reasoning, but robots need action and control data, which is expensive to capture. With Blueprints built on Nvidia omniverse and Cosmos, developers can generate massive amounts of diverse synthetic data for training robot policies. First, in omniverse, developers aggregate real world sensor or demonstration data according to their different domains, robots, and tasks. Developers use the robots to use omniverse to condition Cosmos, multiplying the original captures into large volumes of photoreal, diverse data. Developers use Isaac Lamb to post train the robot policies with the augmented data set, and let the robots learn new skills by cloning behaviors through imitation learning, or through trial and error, with reinforcement learning AI feedback. Practicing in a lab is different than the real world. New policies need to be field tested. Developers use omniverse for software and hardware in the loop testing, simulating the policies in a digital twin with real world environmental dynamics, with domain randomization, physics feedback, and high fidelity sensor simulation. Digital world operations require multiple robots to work together. Mega, an omniverse blueprint, lets developers test fleets of post train policies at scale. Here, FoxContest heterogeneous robots in a virtual Nvidia Blackwell production facility. As the robot brains execute their missions, they perceive the results of their actions through sensor simulation, then plan their next action. Mega, let's developers test many robot policies enabling the robots to work as a system, whether for spatial reasoning, navigation, mobility, or dexterity. Amazing things are born in simulation. Today, we're introducing Nvidia Isaac Group N1. Group N1 is a generalist foundation model for humanoid robots. It's built on the foundations of synthetic data generation and learning and simulation. Group N1 features a dual system architecture for thinking fast and slow, inspired by principles of human cognitive processing. The slow thinking system lets the robot perceive and reason about its environment and instructions, and plan the right actions to take. The fast thinking system translates the plan into precise and continuous robot actions. Group N1's generalization lets robots manipulate common objects with ease and execute multi-step sequences collaboratively. And with this entire pipeline of synthetic data generation and robot learning, humanoid robot developers can post-trained Group N1 across multiple embodiments and tasks across many environments. Around the world in every industry, developers are using Nvidia's three computers to build the next generation of embodied AI. My job is so easy. You've got our CEO doing videos for us. You've got the best team and the best partners. So, yes, it's so easy. So you've seen a lot of technology in there. Fundamentally balls down to three computers and four major steps as I talked about. Creating data, building your AI or training your AI, testing your AI in simulation, and then the fourth step deploying it on the robot. And within each of the steps, we provide the computing infrastructure. We provide workflows for developers to build their solution. We provide acceleration libraries to speed up computing. In some cases, we even provide foundation models working with the whole industry. So we've got a lot of technology where a platform company. And what we then do is we work with robotics companies to take whichever part of the platform that they need. It's rarely everything. It's whichever technology that you feel you need and will help you integrate that technology into your solution into your platform. That's basically what we do. And we keep rinsing and repeating this over and over again. So let me share with you some of the recent successes of real deployments that are happening now. As I mentioned, the technologies that you saw there, some of those deployable right now, some of those medium term horizon call it one to three. Some of those could be longer than that. But we're working in everything from today to the future. So at this ratio, in fact, yesterday, Universal Robotics announced you are 15 robot. You should actually check it out in the, you know, they're displaying this in the, in their booth. It is built on Nvidia technology using AI for faster motion generation and project planning. So we work with Universal. We've been working with them for over two years to take select pieces of our technology and integrate into their Policicop X platform. We also have Venxion. They're also on the show floor. They've announced machine motion AI and Venxion focused on small and medium enterprises. Because one of the biggest challenges we have in industrial automation today is we are only able to deploy automation by pre-programming it in high volume applications. But much of the world is going to be high mix and this millions, if not tens of millions of small and medium enterprises that need automation. The only way we're going to bring automation to them is to solve it through autonomy and make it easily accessible. So machine motion AI is also built on some of the technologies that I've demonstrated earlier. KUKA announced their robot controller, again bringing AI right into their controller using Nvidia products. And then yesterday's standard bot is also using. So these are just four examples that are actually on the show floor. Now let's again switch gates to a little bit of technology and then again I'll come back to showing you real world examples. So the way to think about automating industrial AI, the way I think about it, is think of it as two ways. One, we call it inside out, just like humans. We have sensors, we have our eyes and other sensors. We perceive the world inside out, right? We have sensors, I'm able to see all of you and then perceiving it. There's also another way to solve some problems, it's called outside in. There are sensors that are in the facility and you can see down upon what's happening, kind of like a traffic control, right? You have planes with pilots that are run amus but there's a traffic control that's giving information as to what's happening. In fact, the way to solve industrial automation, both of them have to happen. Each of these robots become increasingly more and more autonomous. But at the same time, we can use technology from other sensors that are watching these robots move, robots humans, combination of all of these. And then you can use that as an a traffic control system, essentially, and give more intelligence back. For example, if you're an AMR and you have a machine going from point A to B to C and there's a spill happened on the other side of the warehouse. The AMR wouldn't know. However, an a traffic controller should be able to know that, right? And if you feed that information, you dynamically change your route and optimize. And this is how it's going to increasingly become, right? So we are developing technologies for both inside out and outside in. So let's look at some of the examples of how customers or partners have been using our technology, especially simulation for deploying these AI use cases for inside out. So you saw it briefly in the video. So four companies have recently deployed this in their factories or deploying it in their factories. I'm going to talk about Foxconn briefly. Foxconn, in fact, has is building. It's been going on for the last year. A factory in Guadalajara, Mexico to build a lot of NVIDIA GPUs which are going into the future AI factories. And this factory, I haven't been there personally, but I've seen the digital twin of it. In fact, the digital twin was built first before the factory was started. It's a 450 meter long. I've never seen a 450 meter long, anything. I only know a 400 meter circle Olympic track, but this is like 450 meter long. And they significantly cut down the time to build this and try to basically place all the different stations where and do it all into digital twin. So that's Foxconn. So they have used a simulation environment first to test it, to design and test it. Once they like the answer in simulation, they put it in the real world. So it significantly reduces the planning time. The build factory roughly half the time. That's significant. When you're talking about this, tens of billions of dollars worth of factory that's being built. It's significant savings when you're able to cut down by half. So the accessories Benz is also planning their factory and they use electronic humanoids. Similarly, before the build is new car factories for EV manufacturing and testing these humanoids, absolutely doing it in simulation first. Sheffler with agility robotics. By the way, agility also has a booth. You should definitely check it out here using, again, using the idea of digital twin. And the reason why you want to use a digital twin is obvious, right? How many robots, like humanoids exist in the world today? You have like, each company has a few tens. It's impossible to build hundreds of these physical things before you even know how good they are. So in simulation, you can put hundreds, thousands of these robots if you choose. It's all, it's free, literally free, right? And so you can do lots of experiments in simulation, unlike in the real world. So that's an example I gave you of inside out. So majority of what's happening right now, almost everyone is building and testing in simulation first because of the superpower. Simulation technology has matured sufficiently. I'm not going to say that's perfect. 100% seem to real. No, we're not there yet. But it has matured sufficiently that we can confidently build more things in simulation. That's what we're observing from whether it is for a single robot training or a fleet of robots, orchestration, everybody, all of our partners are building and testing in simulation and significantly cutting down the time to development. And then once they like the answer, they're going to deploy it in the physical robot and then surely nothing is perfect and then they go back and find you. Now let's talk about outside and the other side, which is the a traffic control. So the idea is this. It's pretty simple. If you look at any factory, any warehouse, any scenario, there are already cameras existing whether they're for security purpose, whether it's whatever reason this. Plenty of cameras already deployed. What if you can use those cameras, apply AI, generative AI, large language models, vision language models. And it can tell you what's going on. For example, as a factory operator, your interface is LLM. You just talk to it. You don't need to learn special UI. You can just ask, was there a spill? Or like show me what happened through, you know, was there an accident that happened throughout the day? Show me what the throughput is. That's kind of the level of autonomy and automation we want to get to. And it's now possible with generative AI. So what we've done is we've created a blueprint and several of our partners are using it here and by the way, this is one, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we need to assertion summarization. That's what VSS stands for. It's a platform called metropolis which is our outside, in perception system. So this is a blueprint that combines large language models, vision language models,ilight which is essentially, you know, enables you to connect with the database and retrieval augmented generation. Combination of all of this is as simple as you can query one camera or a fleet of cameras. Anything you want. basically the answer comes back. Either answer comes back as a tech summary, answer might come back as an audio summary. So you can ask questions like, show me the queue length. Or you can ask questions like, what was the throughput in the last one hour of the goods, for example? That's what you want to get to. Or show me, if you're planning a four-clift, and in this example, one of the video that just rolled by, show me where's the best place to place in the warehouse. So the VLM can do 3D volumetric estimation and decide based on all the open spaces where to put it. So we have, again, the same thing, the outside in, we are starting to see our partners leverage this technology and start putting it into industrial automation. So Siemens is a great example where they're essentially using that exact blueprint that I mentioned, and now the operator is going to be able to do the same thing, and now the operator interface is just type the question in or just query the question, just like you use TadGPD for summarizing your email, if you will, right? So they're using that to significantly improve automation. Pegatron, our partner, they are using it for, you know, assessing, this is server assembly. Is the engineer there or is the installer following all the right steps in the right order? And just have a camera and the camera, the generative AI knows exactly what are the right steps, and if it's not, it's going to flag it. You just have to show the right steps one time and it can always track it. So in summary, how does the journey look like? How do we see this evolving? The first step, as I mentioned, is you want to do development in the digital twin as much as possible. It's faster, safer, cheaper. Once you have a digital twin, once you create a digital twin, you already have cameras and sensors. You can do video analytics instantly for outside in a traffic control. And from then on, your steps start, you know, depending on what you're building, you can use generative AI, you can do if you're building a humanoid, if you're building an AMR, if you're building a four-clift, if you're building an industrial arm, you start basically, you know, using all the technologies available for training AI to deploying AI, and eventually it'll all lead to multi-robot testing. So our Nvidia platform is built towards enabling the future of autonomy. So my last slide, the way we do it again is, Nvidia, we do not build robots. We do not sell robots, but we provide underlying technology that gets integrated into every AI, every robotics company at different layers in the ecosystem. So we're very proud to say that, you know, thanks to all the vibrant ecosystem partners that take our technology and we help you with integrating your technology. And then one more, we have two, anybody wanted to travel? By the way, I'm going to Taipei tomorrow. So there's GTC Taipei coming up next week, and then there's also GTC Paris to hear more about NVIDIA's efforts in AI and physically AI. With that, I guess we're opening up for Q&A. All right, guys, we have time for just a few questions here. Is there anyone who wants to start us off? I'm here, and we've got Dana over on that side. Oh, you guys are too scared, huh? All right, we got one on the front of the screen. Very interesting talk, what's happening here. How are you seeing the discussions around data and security evolving in the industry and with customers as this is coming online? Yeah, that's a good question. So security, I didn't talk a lot about security and safety, but those are non-negotiable. I didn't spend a lot of time in safety, but in all of our platform, there is safety from the chip level to the software level, the complete deployment level, and then same thing with security. So the thing, in fact, when you talk about data, the way this works is we expect in this whole workflow that I talked about, there's going to be open data sets that help you train the foundation model, if you will. It's no different than, let's say you have your company, you have your secrets, but you're going to hire an engineer first, but that engineer is learning common curriculum that's available to everybody. Once they come into your company, they're going to train on your secret stuff, right? And then they become specialized in your. Same thing with all of this AI, eventually. So there's going to be base models that are created with open source or open data that's accessible to everybody. And then you, as a robot company, or an end customer that's deploying robots, you have your own data that you're going to fine tune or what we call post-trained, the AI, and just no different than how we humans operate. So that's all part of the workflow that needs to be done. It was an amazing talk. One thing I wanted to understand in terms of, like, from industrial automation and what you talked about today, I want to understand, like, when are you seeing, like, the point of time where these are going to, like, where, like, we have traditional automation, where we have PLC's, different protocols and things like that. And then you have edge computing and all you showed today. What do you think about, like, the point of time that these are going to, like, fuse together and be more integrated? Yeah. I think we're starting to see that for some applications. I think there'll be some applications which are happening now. And the reason that happening now is because of, as I mentioned, Gen AI and simulation have reasonably matured. That's why you see products, like, a partners at Universal Robotics and Venshin and Kukai. You see that trend happening in the last year or so. So that's near term, right? There are some technologies that I showed, like, simulating a fleet of robots, a digital twin, orchestrating. That's partly research, partly development, right? So these will take a little longer. So it's not a one answer for one use case, but it's going to be a journey. But I think we have hit a tipping point where this whole thing is significantly accelerated. Yep. Hi, thank you for the interesting presentation. I'm a fan of Schneider. And my question is, so now, I think this is the robots, new technologists, AI, they are cool. But we need to know that nowadays there are many, many existing, old-fashioned manufacturers. And there are not that level of automation. And but still there is a lot of asset for them because they already invested in many years and still running. So what do you think, how can we support or help those, the old-fashioned manufacturers to evolve into this new automation generation? That's a good question. There's two things that need to happen. One, the tech needs to be better than what exists and solve real problems. I think that we are starting to see. And number two, the integration, the ease of integration has to be very seamless. And I think that's super important, especially as you're trying to replace some of these older ones. So some of these will take longer. And in fact, what we are seeing, the beauty of Genai, and beauty of this is, you know, even the user interface is being reimagined. Just like, you know, in fact, the fastest application to 100 million users was chat GPT. Faster than TikTok, faster than all the other ones. I think it was like nine days. And this reason is not the tech. Of course, the tech is great. The reason is ease of use. So I believe for industrial automation to really take, not just the technology problem, technologies are must, but the ease of use must happen. So I think all manufacturers, whether you're a system integrator, whether you're an end user, whether you're a robot manufacturer, the ease of integration needs to be solved. And we are starting to see that. I mean, I showed you examples of Siemens, or examples of UR. Many of these increasingly, you know, creating, using Genai to simplify the interfaces to almost eliminate programming completely. You're right over here. A deep, but a great presentation. Larry Nelson from VM. Question for you. We've seen a lot of investment in humanoid robotics, and seen a lot of different form factors. I'm curious to your research. You've seen other development in form factors for robots, other than the humanoid form, and what you think about the difference between humanoid form and other, you know, just different functional type of robotics. Yeah, that's a good question. And in fact, get there a lot, because, you know, that's one of the topics where somebody would be like, I actually don't need a humanoid for this, and some people believe, the way I think about it, I think it's great for several reasons, but I'll give you the most important reasons. Number one, human arts could be the largest opportunity in the future, which is great, because that's bringing a lot of funding, a lot of the best in the world, are starting to work in this area, right? And then humanoid also is a good form factor to experiment fast, because over the last two plus centuries, we designed the world around humans. So something in that form factor enables us to quickly collect data, quickly do things, right? Now here's the best part. In the end, the form factor doesn't matter to me, because it's going to be many form factors. But because of this humanoid form factor, the whole world's coming to it, we can advance the development of AI models for it, the foundation models, and the trick is this. No matter what model we develop for humanoids, it is going to be cross-embodied. We'll take those models, we'll apply to AMRs for navigation. We'll apply to industrial arms for manipulation and gripping. Or three arms, five arms, doesn't matter, right? Whatever is the right form factor for it, because in my mind, when I think of a humanoid, I don't think of just five feet, seven, 150 pound traditional one. I can think of, you know, eventually, you know, ten years from now, every kid that will be born will have a R2D2, maybe, right? Right? Why not? Right? And they'll grow up with it, and there'll be different form factors for doing household chores, and elderly care, and not. So I think it's all positive because of the amount of funding, the amount of research that's working on it, and I believe, no matter what we build for a specific humanoid, that's going to be cross-embodied into multiple form factors. So it's going to uplift the whole robotics, whole industrial automation market. All right. Deep, deep, deep, last question right here. So you talked about the ease of use, simulation, inside, out, outside, in. When you're thinking about rebuilding an organization around these concepts, do you envision a team of people that build out that digital twin and simulation, together, outside in information, and then the larger, broader organization, be able to ask questions to drive the innovation, or along those lines, what are your thoughts? Yeah. So I think, assuming most of Brownfield, because we want to serve a lot of Brownfield industries, and the easiest thing to do for a Brownfield is, essentially, if you can create a digital twin of it, and the digital twin, how a year ago it was hard to create a digital twin, today, it's not as hard. Is it easy enough to create a digital twin for everyone, not yet? But the technology is coming where you can, essentially, eventually, there'll be technology in the next few years where you can just put a camera and just pan around it, and Genai will build exactly all based on all that depth and all of that. We're getting there, right? And then, for a Brownfield facility, you already have cameras, so the air traffic control, so if you will, right? So that's instantly doable, instantly available right now. So from there on, it's easier to build, add new type of autonomous robots. So that's like a Brownfield. Now, if it's a Greenfield, it's already proven that the best way to build a Greenfield is start the digital twin due in simulation first, anyway, because it's faster and cheaper, right? So that's kind of the way to think about the problem. All right. That's it. Keep it up, everyone. Thank you very much. I'll do that.