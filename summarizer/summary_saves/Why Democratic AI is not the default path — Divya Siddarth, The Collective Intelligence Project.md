# Video Information

**Title   ** : Why Democratic AI is not the default path â€” Divya Siddarth, The Collective Intelligence Project  
**Uploader** : DEMi network  
**Duration** : 17:47  
**URL     ** : https://www.youtube.com/watch?v=SEN1WXCS5O0  

---

## Transcription

 So what we've done with the global dialogues is tie this into an experiment on digital twins. The idea of digital twins is that you can have an AI agent negotiate on your behalf, participate in decision-making, improve your epistemic structures, improve the information efficacy of the institutions you're a part of. I'm Divya Siddharth. I run a nonprofit called the Collective Intelligence Project. And today I'll be talking about Democratic AI. I think it's important to point out that when I say Democratic AI, I think about democracy. I started this nonprofit around three years ago, and my motivation for starting it is that I didn't think a few people should get to choose what the future looks like. So when I think about what it means to democratize AI, I think about the core things that a democracy is supposed to afford. People having agency over their lives, people having the freedom to make choices, and the kinds of public capacity and infrastructure that actually allows for that to happen. I think you can use the term in a lot of different ways, but to me, democratization means four things when it comes to technology. The first is democratization of use. I think that's a lot of what we've been hearing about today. More people should be able to use something that is good and valuable, and that's great. That's definitely a part of what it means to build something democratic. But to me, that's pretty much insufficient if we're thinking about democracy as a question of agency and rights, which again is how I think about it. So the second thing that I think is necessary is having a democracy of development. Who is involved in the process of creating the technology? Is it really closed? Is the infrastructure not available to people? I think we've been touching on a little bit of questions of development here, and we can't forget that just because people can use the end product of something, they're not involved or able to take charge of that development process. We're not talking about democracy anymore. The third is democratization of benefits. Who gets the money from the projects? I think it is easy to build structures in which the benefits concentrate at the top. We see this very often. It's particularly true in systems with high concentration network effects, as a lot of the ecosystems we're talking about are compute, data, etc. These are things that concentrate benefits. We're not building systems to actively distribute benefits. We're still not talking about democratic AI. And fourth, and the thing that CIP most directly works on is governance. Governance is the question of who decides what gets built, who decides what gets deployed, what the regulations are, what the accountability looks like, and who decides who decides. A lot of our core democratic mechanisms are about choosing people who get to make decisions. This is a pretty archaic way of making decisions, but at least you have some say in them. If we don't have say in the higher institutions around the things that we're building, we're not democratizing them. So to me, when I think about democratic AI, I think about these four things. Use, development, benefits, and governance. And I think it's important to remember that getting these things democratized is not the default path of this technology. That's why I assume and hope we're all working on shifting that path towards a different basin that could be in a tractor other than the one that we have. And I think the default path is that AI is outpacing our collective intelligence, our ability to make these decisions, build this infrastructure, deploy this technology democratically. So all of the work to me that we need to do is being one realistic about what the end goal is, and not throw under the bus things like democratizing benefits and governance. I think we're doing enough by democratizing use and development. And to build the infrastructure, path to feasibility, and leverage points to make sure that we can actually do these things. Where are the resources being built? By whom? Where is the compute being owned? Are the data sets open or closed? If they're open, is that good for the people whose data it is? If they're closed, is that good for the people whose data it is? How do we actually make sure that the democratic infrastructure for this isn't way slower and therefore going to be outcompeted than the centralized infrastructure? And finally, what kinds of coalitions need to be built so that it's not always the case that democracy moves slower isn't as productive and again gets out competed by more centralized processes. So that's a lot of the scaffolding for which I started CIP, otherwise I mentioned, our work is specifically on the governance side. How do we get more direct input and data from people around the world and deciding what frontier models look like? Our theory of change is creating a feedback loop between artificial intelligence and collective intelligence. We fundamentally believe that we can use faster technology, more capable AI, to build better institutions and information mechanisms, and then we can use those improved institutions and information mechanisms to better govern AI, create more beneficial outcomes, and we could have otherwise. I'll walk through some of the kinds of projects that we've done in this space. I'm realizing putting this up that this is a slightly out of date, but you can see our more recent projects at COQ.org. And it has looked like trying to find leverage points in the model training, deployment, evaluation, application stack, in which we can include more diverse data, we can distribute benefits from data to larger groups, we can regulate based on public input, and we can make sure that there's public capacity, not just private capacity, in solving the core problems of developing AI. Some of the initial projects we did with AI labs like OpenAI and Anthropic were finding core decision points within their model training pipeline and saying the public should have a say here, what kinds of information are we not including? How do we have broader groups involved in decision making over the OpenAI case, evaluation, in the Anthropic case, the constitution, or the model spec of how the values of a model that we all use every day, or at least I do, get decided. We've worked with governments most recently in the UK, in Taiwan, in India, in Sri Lanka. We work with them primarily on evaluation and sovereign models. I think it's important that countries have a say and have domestic capability on some of the most important technology in the world. And we should have a plurality of values that builds those technologies. We work with these governments to build out the data sets needed and the evaluations needed to create sovereign models while trying to build coalitions between countries so that we're not replicating work and recreating the same thing in multiple places. Finally, we think a lot about data. I think it's the case that data has a really interestingly shaped good in the AI ecosystem. What does it mean to appropriately collect it? What does it mean to compensate people for it? Who should be involved in those processes? Especially because you can't retrain models particularly well. And so it's not really the case that you can go back and directly compensate people for any of this data. What does it mean to govern data as a commons based good? We've worked with organizations like Creative Commons and others in that space. I'll walk through one of my favorite projects we launched recently. And the goal of global dialogues, it's a project in which we talk to people, thousands of people from around 70 countries every two months to help get their thoughts on what the future of AI should look like, what AI policy should look like, and have them directly evaluate models for their own values and use cases. It's a representative with many asterisks behind it process around the world in that you can see we're a bit overrepresented in Asia and we're very underrepresented in older people who have a very hard time understanding some of the questions. But overall, we try to get broad input and translate that into leaderboards and evaluations because it's very easy, even when you think about democracy, to still focus on the countries that's easiest to reach. And it's important to us that that's not the case. I think one interesting thing to bring up with this project is we do believe that every project should try to complete this flywheel. So go from collecting a lot of information from people to make sure they have agency over the future, sure, but then also try to improve the technology itself with that information. And so what we've done with the global dialogues is tie this into an experiment on digital twins. The idea of digital twins is that you can have an AI agent negotiate on your behalf, participate in decision making, improve your epistemic structures, improve the information efficacy of the institutions you're a part of. There's a great Oscar Wilde quote I really like. He says I love democracy, but it takes too many evenings. And I think this is essentially true. If we want the information structures of the future to be democratic, we need ways to have information efficiency that's infinitely higher than people coming together and having conversations. So a lot of our work now is around how do we maintain the core diverse direct agentic input of humans while trying to operate at the speed and scale of technology. So our digital twins work is trying to move in that direction. I'll finally say to my point at the beginning, I think a lot of what's contested in the space is still pretty wide open. We don't have clear institutions to build or govern AI. It may feel like that race is already won. It's definitely not. We have very complex incentive structures for these models. It's very unclear what the market is going to look like next year, let alone in the next five years. And we are in a period of having an absolutely terrible information ecosystem that we are all involved in. There is a lot of space to win the battle on these three fronts, institutions, incentives and information to make them more decentralized and democratic. And if I can have a call to action for anything, I think it's us being clear about working in those spaces. For CIP, we laid out a path to democratic AI as we see it last year. And this is what we're now working on. Collective fine tuning of frontier models, direct public input into the life cycle, connecting the open source and democracy movements more closely. I think it's a real tragedy that these are two powerful, massive spaces that don't talk to each other, especially around the world. And don't create infrastructure together to make sure that our coalitions are strong. Expand our worked other parts of the world, build AI and able tools for democratic governance. And make sure that the institutions that are building these models are at least slightly better and more interesting than the institutions that have run our world for the past 100 years. I think it's kind of crazy if we can't make that happen. That's everything from me happy to talk about any of these pieces and encouraged by all the work people are doing to really democratize AI. Thank you. I would like to hear more if you can elaborate on the the arrow that goes from the CI to the AI. Like how you mentioned there's something on there about anthropic, constitutional design. How do you really get, once we know what we want, as a society, as a people, how do we really get that into not just high level policy, but really systematically into the AI-driven companies and organizations that are really shaping our world? Yeah, that's a great question. I think if we're thinking about building collective input directly into models, then you have to find the leverage points where interesting and somewhat natural language adjacent decisions are being made. That's why the constitution is a really good fit for this. Model specs are a really good fit for this and we've been working on that. I think if you want to get a little bit broader, you can have public input into determining the contours of training data, basically. What do we over under sample? As we have better mechanistic interpretability, what kinds of features are we particularly over under sampling can also be something that you can translate public input into. Evaluation is something we've been working on a lot. I think eVALs are interesting. A bunch of people have talked about benchmarks today. I think eVALs are interesting in setting incentive structures. If you can build a robust and rigorous and ideally living as in consistently running and gathering data evaluation, you can push people towards scoring well in that eVAL. Basically, right now for a lot of public good stuff, you just have to hope that things are getting better. There's no real way to evaluate. Figure out what people want. Maybe we shove it into the model, then have we succeeded. That feedback loop closing isn't there yet. I think those are some of the ways you can build into models directly. People are also doing interesting work on auditing development structures, which is company decision making, security practices, public infrastructure, things that aren't at the highest level of policy are somewhere in between actually building it directly into the technology and setting the GDPR lines or whatever it is. Things that are more about company policy, who gets to build the technology, what do compensation curves look like? You can translate public input into those aspects as well. It's hard. I'm interested in your thought about the, because you're sort of in the interstitial area to some extent between private and public sector. Clearly, you've been hired for some public by some public sector. The solids you have reaching out to a lot of people. They are demographically non-representative in some ways. Do you see your work sort of as aiding the public sector and private sector equally? And also, do you see in the longer term VZVI, does it end up consolidating as being sort of on the public side? Or is it, we're just going to give enough information to the world that the private sector folks will be leading in terms of AI governance and the public sector may need to be hands off because the private sector aided by you is in the right direction. Am I right in understanding the question is, are we thinking more about the public or the private sector both and then be which is going to be more influential in governing AI in the future? Yeah. Honestly, I think we live in an age of cross-border planetary problems and that makes the public sector a really difficult type of entity to work with because it's fundamentally not a cross-border jurisdiction. And that's why we're interested in doing more global processes, in trying to bring countries into coalition, etc. So I'll say that. That's actually, I think, private sector entities are much better at solving cross-border problems and coordinating across countries than public sector entities are because that's just not how they were set up. So I think that is part of the reason when we care about global governance that we go to the entities that think globally, which tends to be private sector. It is also important to me that there's what Galbrith would call countervealing power in the space. I don't think either sector or any part of the stack should have such power that they can set, let's say in a market sense, like set prices for the rest of everything, right? So I think it's important that's why we help set up part of the UK AI Safety Institute. We do a bunch of work with national governments because I think it is important for there to be expertise in the public sector that can act as a check on the private sector, even if it's not across all the countries the private sector operates in. We talked about the word polycentric yesterday. It's kind of a midpoint between something being decentralized, which I think decentralized things don't last. Like, it's impossible to keep systems decentralized and centralized. Centralization has a very single point of failure problem. And so I think the best you can do is have polycentric systems where different centers have accountability and power over others. So in that sense, if I think the private sector is becoming too powerful, I would be happy to work with the state. And if I think the state is overreaching, I would be happy to work with the private sector. So that's an interesting question because I think a lot of people, a lot of companies in the private sector, go to problems because they have vision, they have an interest, they can solve it, they have some mixed parties there. In the private sector, a lot of these problems are brought to them because that's what's important. And on one side, you've got people who have that expertise that don't make policy. And the other side, you've got people who make policy. In fact, why do we allow elected officials to make any decisions about anything? What they know about traffic, let alone AI or technology. So how do you solve that problem? How do you get people in policy in the public sector to have more private sector skills? And how do you get people in private sector to think more about policy when they seem to be opposed and people from the private sector who try to get policy are often very self-server. You're asking expertise problem in the public sector, kind of like policy and accountability problem in the private sector. I mean, it's a tale as old as time. I think I have a few thoughts here. One, like spend most of my time at UK, I see hiring people. Like I think you actually just have to allocate budget and hire excellent people into the public sector. There's basically no way around it. And is that possible in different governments around the world? In some places, yes, and in some places, no. But I think you just have to bring the expertise in house. And I'm proud of some of the Eval's institutes that governments have come up with when they have actually gone out and hired a lot of excellent AI researchers. In terms of the incentive structure of private companies, I mean, it's in some ways a failure to exercise power by the public sector, right? Like there should be a feedback loop and there should be a willingness to regulate and cut off bad outcomes if they happen. And I think it's kind of a problem on both sides that that's not a feedback loop we currently have. I also think in our work with the private sector, private sector is a huge term. But with the specific companies we work with, there is generally a desire to not make extremely consequential decisions without input if you can. And that's why they tend to work with us to try to get broader input in just because they want to make better decisions and some will understand the, I guess, power that's being accorded to them. So I think you can work like inside game with the private sector. You can bring expertise into the public sector and you can make sure that there is the muscle within the public sector to actually regulate one necessary.