# Video Information

**Title   ** : AI Engineering with Chip Huyen  
**Uploader** : The Pragmatic Engineer  
**Duration** : 1:14:43  
**URL     ** : https://www.youtube.com/watch?v=98o_L3jlixw  

---

## Transcription

 How would you define AI engineer or AI engineering? Yeah, so before when you want to build a machine learning applications, you need to build your own models. So that means that you need your own data and you need expertise in how to train a baby suit model. However nowadays if you want to build an application leveraging machine learning or AI, you can just like send it directly API calls and access to this like wonderful capability. So that's like really really lowers the entry barrier to people. Like you don't need data anymore, you don't need a fancy AI degree anymore. It's a shift of focus from less machine learning and more engineering and more product. JPUN is a computer scientist and writer and author of the book AI Engineering. This book is currently the most retidal on the ORIO platform. Previously chip was at research at Netflix, a core developer of Nemo, MVAs Genai Framework, ML and Gears snarkled AI and founded and sold an AI star called Claypot AI. She taught machine learning system design at Stanford, and her current book is the second one on ML and AI Engineering. It's safe to say she's one of the most ret ML and Gearing and AI engineering experts in the world. In our conversation today, we cover what is AI and Gearing and why does it feel a lot more full stack than ML engineering did? What are typical steps to build an AI application from choosing a model through using drag all the ways to fine tuning? What are practical ways for software engineers to get started building AI applications? And a lot more on this very timely topic. If you enjoyed this show, please subscribe to the podcast on any platform and on YouTube. Thank you. This greatly helps the show get to even more listeners and viewers. Chip, welcome to the podcast. Hey, hi, I'm Chip. I'm very excited to be here. So I've been following your sub-stack for a while, so it's like really, I was really looking for what was a chat to the chat. So first of all, I really want to congratulate you on this book. I've started to read the book, so I've not read the whole thing. I have started with some chapters and I went deeper in others. And what I found is when I looked at a table of contents, I was like, well, this looks okay. In terms of it'll span a broad sense because it goes from how do you understand foundation models, how do you evaluate them, what is prompt engineering, and you go into things like fine-tune, drag, fine-tuning data set engineering. But then on each of those sections, it just starts to get, initially, there's an introduction, but it starts to then go deeper. So for example, for evaluation mythology, like here, I'm just looking at a table of contents, but I start to read this. So well, we know it's important to evaluate AI models. We know it's harder to do, but then you go into things like AI as a judge or ranking model who is comparative evaluation and challenges of it. And then it goes, that's where some parts, in my sense, is that I had to slow down. I had to look things up. So it does go really deep into a lot of these sections, which I found for your refreshing, that it's got to mix up breath, but it also goes deep. So it is definitely not a fast read for me, but it's one of those things where I'm just going to keep coming back to it. Thank you. It was not a fast read either. It took quite a while and a lot of references. I think I'm so published, so I think in the book, I cited about like over a thousand references. So like, I actually read even more like papers. I've got a bunch of good bases. I have a tracking of like a thousand like repost, like at least like now 800 stars now. And get up stars. So eventually a lot of those code bases and a lot of blog posts, other books from my the 80s, from the 70s, 90s to understand AI. So I also like published this like at least about like approximately 100 links. A reference link, so I felt like really, really useful for me in the process, the breading, the book. So if you just want to like look at those references on their own, like is on my GitHub. Yeah, and I was surprised with some of the really original research. Like it's not just like, oh, here's I wrote about these papers or here was what I read. But as you mentioned, the thousand reposts, you actually have a paragraph or a section about how many, how did GitHub repositories change over time? The ones that were about infrastructure, AI, application level, other things. And you actually have more than 900 reposts mapped out. I never saw anything like that. And clearly that was you, you know, like kind of slicing and dicing and doing your own kind of research. Yeah, I feel like I have this thing like I do a lot of manual labor. I do think I get a lot of value I was doing things in non-optimally. I feel like a lot of focus on my here. What is the quickest way to do it? What is the fastest way to do it? But sometimes if you're willing to put into effort into things that like a lot of books don't I'm not willing to, I feel like you can get some kind of insights other people like don't get. This episode was brought to you by Swarmia, the Engineering Intelligence Platform for Modern Software Organizations. Swarmia gives everyone in your organization the visibility and tools they need to get better and getting better. Engineering leaders use Swarmia to balance the investment between different types of work, stay on top of cross team initiatives, and automate the creation of cost capitalization reports. Engineering managers and team leaders get access to a powerful combination of research back engineering metrics and developer experience surveys to identify and eliminate process bottlenecks. Software engineer speed update daily workflows with Swarmia's two way slight notifications, working agreements, and team focused insights. You can learn more about how some of the world's best software organizations including Miro, Docker, and Weftflow use Swarmia to build better software faster at Swarmia.com slash pragmatic. That is SWARMIA.com slash pragmatic. This episode was brought to you by Graphite, the developer productivity platform that helps developers create review and merge smaller code changes, stay unblocked, and ship faster. Code review is a huge time sync for engineering teams. Most developers spend about a day per week or more review in code or blocked waiting for a review. It doesn't have to be this way. Graphite brings stack pull requests, the workflow at the heart of the best in class internal code review tools that companies like Meta and Google, to every software company on GitHub. Graphite also leverages high signal, code leads to where AI to give developers immediate action will feedback on their pull requests, allowing teams to cut down on review cycles. Tens of thousands of developers at top companies like Asana, Rampe, Tecton, and Versel rely on Graphite every day. Start stacking with Graphite today for free and reduce your time to merge from days to hours. Get started at gt.dev slash pragmatic. That is g for Graphite, t for technology.dev slash pragmatic. So one thing that is a little interesting about this book, it is about AI engineering, and this field moves so quickly. Just in a week, we've now had a new model. Come out, for example, deep seek that people are talking about in a few weeks. How did you write this book? How were you able to write a book about such a fast moving industry so bad that the time is released, which was clearly a few months after you finish it, it will still be relevant. That is a great question. When I started writing the book, I was thinking the same. It's now the right time to write it because there's so many things that still changing. But when when chat really came out, like a lot of people, I had this existential crisis. I was in a group chat and I was like, oh no, what is it mean for us? I feel like there's two things that I usually identify with, one is being an engineer and the other is being a writer. Guess what you use case is that AI is really good at writing code and writing. It was like, oh shoot, what does it mean? So I started interviewing a lot of people. I started reading so much and I talked to a good tongue of people. I started making a lot of notes. As a process, what I realized is that what a lot of those things seem new. A lot of the fundamentals that I have been there for a while. So for example, a language modeling is not a new task. Closh and introduce it back in the 1950s. As a template, we'll talk about RAC. RAC is not new. It's based on which you can write. RAC is a retrieval-argumented generation. Retrieval is a very old technology. It's already powering a lot of use cases on the internet. Like search or recommended systems. I have better databases that have been around for a while. In fact, the search has so many cool algorithms already. So I thought it's like, first I see a lot of things that I've used. The second is that I try to focus on like, we try to focus on asking the question. So sometimes it was like, oh, there's a problem with this solution to this problem. I ask a question of whether this is a due to fundamental limitations of AI, or it just just deals with the temporary capabilities of AI. I try to see if it's due to something of the recent current capabilities. How fast is that capability changing? So for example, in the early days, a lot of people have shared a lot of like prompt tips. For example, you can try to surprise the models. Say if you answer this correctly, I'm going to give you $200. We brought up a prompt robustness. How robust is a model to prompt perturbations? Then I was reading about it. I found that actually, the model is getting more and more robust to prompt. For example, I saw from a GPD 3.5, compared to GPD 3, it's already so much more robust. That means that the smart changes to prompt actually reduce to a lot less variations in model performance. So this kind of thing, I felt like it's probably not going to stick around. So it's just kind of tips and not going to be very much. So already, when as people were still like, this was at the height of people saying there might be a job as a prompt engineer. So you already saw this is likely trending down. Do I understand it correctly? So I do things that just try. So writing, I think this is like a a risk score is like making a bet. When you write a topic, you try to bet on whether it's going to stay relevant in the future. So I think it's the people I've been trying to look at progress and try to see what is going to be one or two years from now on. So for some other examples, like context length. So as a point, I was like, okay, we want long context length. But then I kept seeing people will recommend really, really fast from like what like 8,000, 8,000 context length to like 128, like in a few months, it's like super, super fast. So it's like, okay, maybe the question is like less about like context length, but like context efficiency. Because it's like can can a model use a context efficiency? Like really, really well. So like there are certain so so like those are like the bad and I do things that there's certain changes during the process of writing the books that made me feel more confident in in in the best I made. Or like another thing is like a multi modelality. I do think it's like when I wrote about multi model back in 20, 20, 23 people told me is it's like, you're too early. Everyone was still working on language now. We're not there yet. But it was it's like, yeah, it's just like inevitable, right? Like I think like we we learn how to work with with language, but like I do want to do a lot of stuff with more than just language. And I think it's just like in nowadays, just everywhere, like almost own models, nowadays like multi model. So the title of the book is AI engineering and we also now have this term of AI engineer. It's a spreading like wildfire. What how would you define AI engineer or AI engineering? Because I feel it's a little bit of a load of terms these days. It is. I feel like a lot of terms, nowadays loaded like you're not allowed to you agent anymore. You're not allowed to use like a lot of things anymore. So when I was I was I was I was agonizing over the title for the book because people was like first I know that we need a different term for machine engineering. And the reason is that why why when when with foundation models, there are a lot of fundamentals or like systematic approaches that still the same as machine engineering. There are a lot of new things. So for example, one thing is that before when you wanted to build a machine application, you had to do a whole lot of. And so at least be clear, you're comparing it with machine learning, right? So like what machine learning engineers did versus what AI engineers are doing. Yeah. So yeah, just trying to explain why we need a different term for machine engineering engineers to describe what we are doing today. So yeah. So before when you wanted to build a machine application, you need to build your own models. So that means that you need your own data and you need like expertise in like how to train a babysitter model. However, nowadays if you want to build like the application leveraging machine learning, yeah, you can just like send it direct API calls. And I access this like wonderful capabilities. So that's like really, really lowers the entry to people. You don't need data anymore. You don't need like a fancy AI degree anymore. A second thing is that like before, right, like you need distributions because you deploy application, machine learning application as part of the xd applications. So first of all, it will build a regular system. You need like an e-commerce website. So that you can have like recommend like some some kind of website, right? So it's deployed as part of city application. Far detection is part of like maybe in banking applications or like some kind of like payment app. But however, now you can just like put it out. There's then a lot of applications. You don't need existing distribution channel. But like having a distributed distribution channel is like really, really useful. So I think that another very big thing is that it's a shift of focus from like less machine learning and more engineering and more product. So before, right, like you start from a good machine engineer, you start from data. Like you have to gather data. You maybe have like human annotations. And then you train a model. And I want the model is good to deploy that into the audio product. But nowadays, like you actually start with a demo, right? So we have a cool idea. So okay, let's just try it out and see if it works. So so you start with a product and after you say, okay, so it looks pretty well. I want to make it like better, right? So so they started gathering more data, maybe like with like as more of like from examples or like in rare rare case, I don't recommend what we'll do in the early days. It's like fine tuning. But it's very rare. But like it basically is sacrificing more data maybe for evaluation. It's truly important like having good evaluations become even way, way more important with engineering. So okay, so you've got data. And then after that, it was like maybe you've been like sending a lot of API calls. So like open AI and anthropic like Google and we're like, okay, now it's actually expensive. Now I need to like, develop my own model. So you start like hosting your own model using some open source alternative or like, fine tuning model. So so yes, also like before machine engineer, right? You go from a data model to product and now with engineering, you go from product to data and to model. So so it like plays a lot more focus on on product and data, which wouldn't be compared to a foundation when everyone shares the kind similar base like AI capabilities. So I do think that says need a different term to separate it from machine engineer. And I didn't know like what term he used. And then I I was like, okay, let's just ask the people. So I still very like a bunch of people. I was doing what does this building applications on top of our models. And almost it was like a engineering. And I was like, okay, he does okay, since it's let's go with a engineering. So do I understand correctly that you know, the biggest difference is that machine learning engineers did a lot more kind of groundwork getting the data and building the model. Whereas AI engineering, you have a lot of that at least initially you can start either as APIs or something. So there's more of engineering. You kind of hack things together. You put it together. And then over time as things become more serious, your product is bigger. You do a lot more of the you might build your own model or you might host it. You might one day even build your own model. If if it's there, but it's just a lot later. So like a lot of ML engineering comes down the path. If it's big enough, if it works, etc. Whereas with ML engineering, it was the other way. You had to put in all this effort and then see if it even works. So first, I feel like every company's may have different definition of the role. I think it's like even the same company, right? Like people with the same title or say, we can do very different things. So it's not very never like a clear cut definitions. Second thing is that I don't think the question is like machine learning or engineering. In the vast majority of JWA system have seen, there are very strong traditional machine learning or classifier components. So like imagine you're building a customer support chatbot, which is like whenever I ask the conference, I see a lot of raise ahead. We have very very classic genre applications. And so like, yes, so you get a request from a customer and maybe like you have several different potential solutions for it, right? Like maybe if it's an easy query, you might send it to a cheap model or if it's like a harder query, you might send it to a more expensive model. But it means something very sensitive. Like, hey, why did you charge me twice for the build last month, right? Then you might want to send it to a human operator. So you might have this one like a router or like an intent classifier. I like to choose what to send it to, which is like a traditional classical machine learning model that you can build. Or like after you get a response from an AI model, you might think it's like, okay, does this contain like PII? So because I don't want to send back to users like responses contain like private information. So that like PII add detections can be or like toxic toxic city detections can be a classifier or in the rest system nowadays, like we talk about like how a lot of it using retrieval like retriever systems, which are also like I think it's like in the realm of like classifier like classical machine learning that you can build yourself. So what are the most common techniques used when building AI applications? Things that you know the software engineer who's going into building AI applications I should know about. And later I can go deeper into. So the assumption here that you have you have like tried a lot of solutions and I try JDAVA and you think just JDAVA is a solution for you. And it thinks like what is the first like what should I progress from there, right? Is that correct? Yeah, yeah, it's just like what are some common approaches, you know, like I think things like drag fine tuning or other things that are just good to know about. I should probably learn more about those topics. Yeah, so I think like those are just techniques are useful. And they I was usually recommend some development or like I not recommend but like a common pattern have seen is a certain developmental part of path. So initially the first thing I would say is like trying to understand what means a good response what what a good response is and what a better response is. So like what you want is a more shared rate. And it's not it's not always intuitive, right? So for example, like LinkedIn has a very great example. So they build a candidate like Jof fit assessments for candidates. And they found out that like a majority of their time spending just to understand like what candidates needed from the model. So initially they focus on correct but then they realize that candidates like how it not helpful. Like say if you even got it as a model like am I good fit for this job. And the AI respond with your terrible fit. Then the candidate was like okay, what am I doing is information, you know? So you need to like try to understand like okay here is they want to say one more like understanding what are the gaps and how they can feel the gaps or like guess a suggestion for other roles. So the better fit for them right now. So like half is like a picture is like understanding and then build a guy like like okay given this response answer like this like be helpful. Show them like the gaps or like show them the role like may have a clear guy like for the model like in the prompt. So so you you try those prompts and then you see look at the output and then maybe you can try to add more examples. And then just like go through like get really good response and try to evaluate maybe like have create a set of queries and like experience responses using both automated metrics like as a judge and also like human evaluation to measure the progress. So okay so like you have done prompting we have like added more examples just to prompt and maybe you will like start like making more complex. So you maybe you may use like you can give someone with a model more context so they can answer better questions right. So so maybe when our users ask a question you can like have the model pull out like on the documents or on the job listings related to this question or information about the company like guess a candidate resume right. So you build a system like reach like you augment the context with with with the document. So so that's a rack pattern. So so I do think it's like rack is a very very powerful pattern. It's nothing really really fancy. um and so I'm interested in about rack is that a lot of people equate rack with web research right and like when I see a lot of people like oh I want to use rack and he is like what very data base I should use. Oh so like people jump straight to vector search well yeah because I mean you do have the chunks you know like the embeddings can be stored as vectors right so as engineers were like I need a vector search database. Yeah yeah yeah people love that databases. So yeah very interesting but I feel like the first solution is probably not like jump search embedding uh base a retrieval because you need to build like an embedding model like the qualities that really had a dependence on the quality of embeddings. You'd have embed embeddings numbers you retrieve things well. Also like very data bases can be quite expensive to like to run in an add latency also another thing is it's like better databases can like embeddings can is so like obscure um certain keywords for some way I'm searching for like a specific error code right like through embedding you don't really get the exact error code anymore. So so it's like so like there's sort of a challenge here with vendor databases um and vector search so I think it's like the usual like come on approach and I maybe just start with something as simple like keyword retrieval. Like if you are so like to like track all the keywords from uh from the user query and find all the documents that with the query and you say okay maybe the documents are true law and now it can't fit into context right that's when it's like with chunking okay now how do I chunk this like documents into into into context like they can fit into the the context length and now with chunking have like other problems right so for example like um you have um maybe like the keyword like the document is about the company x right but like it was from now on company x is reverse s company so like it's a rest of the document you don't have x appear so so you don't get the chunk uh like if you search for x you don't get the chunk like below so so now you might want to like okay now I need to extract the keywords from documents and like get admitted data to every chunk or like get the title of the document or like some people like start adding summary to it or like anthropic has a very very good article called like contextual retrieval it's like as I asked chattypd to generate the key information metadata about like each document and a pen at to like like a prepand at to x chunk to have your retrieve uh the right chunk so so I do think it's like having data preparations right actually give a really huge performance boost like I have seen this like giving wayward better performance boosts and I focusing on like which better databases I should use you know uh so so what what is a little bit of like um I'm not saying that they're not useful is just like uh in the beginning you you probably want to try something simple like with the case performance gains and then you start moving up like the complexity level um also like for a lot of retrieval uh someone told me very interesting quote a little bit of like hearty game but he was just like I'm not gonna take any retrieval system seriously if they don't benchmark against VM25 so VM25 is a pretty old school uh like 20 plus E or now a retrieval uh it's a lot simpler uh like term based retrieval not not embedding and it's a really really hard to beat uh retrieval uh systems um and I think a lot of time we use it like uh if you start a complex you can like now combine like both term based retrieval and a simpler solution with like very databases so you have both the semantic like on the embedding side but you have both the term the exact keyword mesh like on the term base so so a lot of things the hybrid search is very it's very come on uh so okay so we talk about like promenginering at my samples uh rack right and then afterwards that maybe have it in max out on a lot of those things which usually take a while people like maybe that you might consider fire tuning um but I think like usually like have a lot of reservations against fire tuning um because like fire tuning has like a whole host of like new problems that needs to deal with yeah like fire tuning my first you need to think of like now you have this model you fire to and now you need to think of like how you host it and and like yeah a lot of models are big like they're building the parameters right like they they are not necessarily yes yes this this part in the book you actually go into the details of the problems with you know the memory size and and also like you cover the alternatives where you might not need as much memory but they also bring uh other trade-offs so it's it's trade-offs within trade-offs within trade-offs right like you yeah you you you you you're starting to solve one problem but you're gonna get a a bunch of others and you'll need to decide you know is it worth my time efforts resources yeah yeah so so I definitely think that um also like when you fight in the model you kind of own the fight in model and now I mean like you the question is like how do you maintain it like anything so we have this whole world of very smart people having new models so like this ability increase like like rapidly like so when heaven like how long can the fight in model like outperform like those new models are putting out so you might spend a lot of energy and effort and you fight in the model and just a wicknitter maybe like some like I don't know random Chinese coming you never heard of or released it's like extremely fast and extra model right so so yeah yeah so so yeah it's quite uh challenging so yeah five two nifty things is the last resort not uh not the first uh first five defense yeah but what what what what I've heard is basically like if I got it correctly like do a structured approach like you know start with prompting start with simple start with you getting responses that makes sense then add more data you can do this with uh you know rag you can you can do it with chunky and keyword extraction data preparation really really makes a big difference which a lot of people don't think about and then you can go to more advanced thing there's a whole whole stuff things that you could do but my understanding is you're probably saying like you're probably get there over time but initially these things will keep you busy and you'll probably be able to build a pretty good system just with the with the basics and a little bit of engineering and and most importantly understanding the problem that you're trying to solve as opposed to you know building whatever shiny technology or approach yeah I was so like in the I still like the approach can also be different for like individual um developers or like enterprise like if you look as a whole organization yeah so so one thing I haven't seen is just like for especially in the early day technologies usually like enabling enabling new UK system and you bring more returns on like this like incremental improvement over existing use cases so like maybe like if instead of like spending the effort into like investing a lot of energy getting the etching out like a little bit of performance with like fancy or like complexity maybe like using the same stack you have had like opening up like new applications so um yeah so so I see like that's why I feel like a lot of companies will take a wide to get you the fight to an infest trust isn't just earned it's demanded whether your starter founder navigating your first audit or seasoned security professional skill your governance risk and compliance program proving your commitment to security has never been more critical or more complex that's where vanta comes in vantick and help you starter skill your security program by connecting with auditors and experts to conduct your audit and set up your security program quickly plus with automation and AI throughout the platform vanta gives your time back so you can focus on building your company businesses use vanta to establish trust by automating compliance needs across over 35 frameworks like sock 2 and ISO 2701 with vanta they centralize security workflows complete questioners up to five times faster and proactively manage vendor risk join over 9000 global companies to manage risk and proof security in real time for a limited time my listeners get $1,000 of vanta at vanta.com slash pragmatic that is vant.a.com slash pragmatic for $1,000 off so let's say that at my company we decide to build an AI solution and let's take the example that it's going to be customer service automation what are typical approaches that I should know about and you know you do cover some of these things in in your book as well like some of the kind of more common steps that I'll need to take so customer support solutions so I would say this like the first thing I could look into is this like what are the bottlenecks for further solution right now so for example like I have worked with another sort of I work with this actually has a challenge oh they had a lot of like customer support requests and they don't know what you answer and the solution is very interesting it's just like okay let's try to drive a lot of questions and choose a common channel like public discord so that means that like all the users can so help answer the questions so like it's much and also like in the future it's not have a question right it can just like refer to previous discussions instead of like asking me so like they try to make a lot of the discussion customer support requests public so like another solution was pretty popular in like in 2018 2019 is that you do like routing to the right department so so the challenge then is like they realize a bottleneck is in like triaging so like first we get a request I don't know which department to say it so so there were a bunch of startups then it's like okay let's try and build a system like to predict like is this choose to go choose the finance department I choose a go choose like a technical support department you know like that's a sky routing already reduced like the the fresh and a lot and if you and if you think it's like okay when we need like a gen AI to do it so so I think it really recommend the frameworks that Microsoft introduced the code is like Cron walk run like going from like a slightly lower stake to like higher stake deployment so first of all customer support check board right so so maybe initially you you can like have a human in the loop so for example you can have like for every request instead of like a human agent right in the response from scratch you can have like a suggest like a few options and like the human can choose what all I can just like two ones as a starting point and then make make make sure to quick edit and send it so like once you see this like okay maybe the accept and rate is like getting really high so first of all for this category of queries maybe the 7 there's just like 90% right so you might feel more more confident she like wrote it out so maybe wrote it out she'll maybe a small the users or I can write it out she'll even like internal use cases and then so like you give it more automations but like reduce the scope of the scope of like up deployment and then after you really happy with it you can like wrote out she'll like more more users so yes so that would how it would go about something like customer support check board nice because like my what I was kind of expecting you might say is like oh you know like you know just build this like you know AI framework deploy it see it I feel that's what a lot of companies are doing by the way a lot of things like oh Jen AI oh let's let's get you know this model from chat GB to your anthropic let's try to you know put it there let's let's put it out there but I really like how what you're saying sounds like it's not really like you know what you explain it's not really specific to Jen AI that you can went through like you look at the business problem look at the problems you have look at the options which include not just Jen AI but more traditional machine learning as you set classifier and then you look at if you know these tools help your problem and then you'll make sure that it actually solves your problem when you roll it out don't just blindly roll it out which I mean all of this sounds to me it's not really new is it it's not like you you could have said the same thing two or three years ago before Jen AI except we would not have those Jen AI tools to play with yeah so I think actually before I actually look up one of those talks before and you have just talk about like things that haven't changed like things that feel very similar about your engineering and I'm definitely like dealing with a new technology is one of the things that never change so I feel like every time there's a new technology that comes out I can hear like the collective side of like senior engineers everywhere saying like not everything is a nail like like people just try to get some technology to work for everything so yeah so so I do think that a very common challenge is that like people just like jump straight into it like you just want to use Jen AI for when they don't need Jen AI so I think this is actually two different there's two different headlines so one headline is like I use JLVI right or in the other headlines I sold the problem so if you want to focus on first headlines and here you get AI but you want to sold the problem then you need to understand like what what is a problem like yeah like what as a challenge is there was a roadblock and removes a roadblock using the simplest solutions not the fancy is one yeah I feel that there's a bit of a really strong fear of missing out across most tech companies that everyone knows this is such a transfer of a transport technology it gives us so many new capabilities that it will be important I think everyone knows that their company will be using it but now there's a fear of missing out of oh what if what if my team doesn't build it what what if what if someone else gets ahead of me and so like a lot of companies many teams are all building it and you're trying to just you know like using a hammer looking for nails even though they might not need it at the time I mean I'm not sure if this is a bad thing necessarily because you know people at least get experience with it and you know that they will need to learn about it but it's a very interesting time because it's rare to see usually we see like you know a new back-and-frame work come out or or like some something that's limited to the main and people jump on it but this is what on the first time I've seen that the whole whole industry is jumping on it and everyone is trying to use it and put it in whether it works or not I definitely agree with you on this foam-oating I do think this is like everyone jumping on it is actually a pretty good thing like I feel like the energy is incredible I've never before seen so like so many smart people focusing on the same problem it's like incredible and the progress is amazing I do think however like I think there's an irony is that's like so more than we want to not miss out things the more things we will miss because what I think is feel like if we try to keep up with news right which is trying to jump in from one new should not one piece of news should another we will always stay in the surface level we never really go deep into anything so I actually don't quite read news so I find it's a little bit distracting so I think I try to like I feel like my my approach is like okay pick a problem that you care about and then only care about things just like help you so on this problem so like if there's some news coming out and was just like does this help me so on this problem and if it doesn't I can't wait right because I feel like if there's something important it will still be important like to wish for now on like a month from now on like I don't drop everything it's like okay let's just go and understand what it is like you know so so I feel like trying to get a more uh try to stay with Carmen uh yeah yeah so when you're building an AI system one of the things that you will come across is you need to evaluate the output how well it works uh does it solve your problem well why is it difficult to evaluate AI systems and what what are common ways to do that so evaluation I think is like $1 question or even a trillion dollar give it how much you put in investing right now yeah um like you know it needs to go big and I think it would go big better be go really big um so um so so I think it's like this challenging because um just more than AI becomes uh the harder it is for us humans to evaluate it so um before right like if AI was like incoherent um it can pretty tell if it's a if the response is bad it's like okay it doesn't sound good like it's a bad response but nowadays like it's pretty coherent right like for some of what you ask it should like jerry it's a summary of a book um if the summary sounds convincing you actually don't know if it's like a good summary or not and you might have to read entire books or self just to evaluate whether it's a good summary um or like the math a lot of time um I personally use a yeah I should ask a lot of question because I don't know the answer and because I don't know the answer I don't know if the answer is correct right so so an example is it's like um a lot of people can tell if a math solution to a first grade um questions is correct but like they're a few people can tell it's like a fancy like equations like proof is like correct um so I remember recently when i won chemo at taren's tau he says amazing mathematicians on i think it's one of the the the best mathematicians of our time uh he actually took time to evaluate a wand and he says like the experience of like using a wand is similar to advising a incompetent but not completely stupid uh okay like a mediocre but not completely incompetent so be a geese be a student but this amazing thing is like if we really need like the brightest mysy day jive i i then we soon should run out like really really smart people should like evaluate i so so i think i was like so so what could be like the chip what would be like the next step forward uh so before a lot of time we use a whole human as a ghost standard for AI performance it's like okay so humans like start writing out like uh here is how you should respond to this and here's how you should do it and like yeah i should try to copy human but now we for many many tasks like we have a like owl before like human we better so so i think makes it a several uh so i thought about a several approach and like to deal with it and i think that's why i separate the chapter like those even so initially i i had like one chapter of an evaluation there's more and more about it it was like shoot there's so much to enough when i took pretty long chapters on evaluation the first chapter is on general methodology and the second chapter is about how she uses like different techniques she like evaluates the AI system so like so like as mythology like one one is that um functional correctness so it evaluates the output of application based on how well it performs the task so like if you say that like uh hey use AI to a stiff energy you can see how much energy is actually safe or like hey use this AI to to place its video game you can see how high the score you can actually get or a very common use case for this coding so i do think this like it's not a code incident that coding it was the most popular use cases because like we actually know how to evaluate jitter like we might not know jitter like essay instead of right but you know what you if evaluate jitter code uh because i've been testing like code for like for a long time so so we've been called you can do like use functional correctness to to evaluate like first like whether this code compile does it run second like does jitter is expected outputs um that's what we wanted to do so like that one approach the second approach is like a uh using a actually evaluate other AI so we've been using AI to evaluate or to automate a lot of applications so can we also use AI actually automate like evaluations and actually doing pretty well I think we have this like um I think like um in many many even back in like 2023 uh Langchian is his report uh this sort as the majority of applications they saw already has a some some sort of like AI as a judge or like LMS with judge and I think this like is growing um and we we do things as like um it's getting pretty cost-affections and like useful but of course I see a lot of like challenges around using AI just so we can go into later uh but another approach that's very interesting is like when a comparative evaluation um and the reason like as humans um is maybe hard for us to give an absolute score on something but if we can give like two versions of something we can tell oh realizes one better so we have done a lot of studies showing that like even for um even for task where wait wait wait wait wait wait wait yeah it's like apple for my doing at the level with like humans uh experts like can't really do we can still tell like detects the differences so so I think it's been like um I don't think this has been a lot because I've been like guiding um not just evaluations but also a model development sounds like there's just no no simple answer right like you kind of need to go through all these options and figure out like in your case which makes sense for cost for what you can do can you have a human in the loop so there's there's no real silver bullet no one thing that you can just use um yeah uh I don't think there's a simple solution so that's one thing about a little bit skeptical about evaluation tooling because a lot a lot of a lot of challenge with evaluations um are not because we don't know how to evaluate but because it's required disciplines and hard work and a lot of things like tombs cut really automate uh so so for example um one thing for evaluation like we need to evaluate an application based on what users want um and and we don't so that means that we need to like go and talk to users we need to look at their interactions because a lot of things what we think is like we want you like met evaluate what what matters right we have to measure what matters so for example uh I have several examples of like how it's very counter intuitive thinking that we're measuring one thing but usually it can't be other things so so in the beginning for example like um does it have a friend who building a pretty big application just like busy building like to summarize meetings um and initially is it will like okay we we we try to get a we try to measure the correctness like does the summary covers the content of this meeting um all I say is something like thing about like hey do uh does the model follows the format because they think this like users want like shorter summary and they agonize over like do we want like three sentence summaries and I fail five sentence summaries and they try to measure though but actually eventually what they found I was like users like don't really care about the whole content of the meeting people only want like what is the action item for me like what we had to do after this right yeah actually yeah does it start changing like so they don't measure correctness anymore I mean they still like don't make up things right but they focus on like yet don't miss out on action items specific for the person asking for the summary um yeah so it's like or like um are the examples like some people um using chatbot for for um let's so we talk about the customer support chatbot so we want to go back to this example so um a pretty uh big tax firm uh so it be with chatbot so you know tax software again for your child which is what you're company it is um so so it was like launch a chatbot and uh you help people with task preparations and the as a response is when I very looped war the well I said they were measuring but like the users how how they use it and they was like people just didn't really seem to use it and they were like why is that uh is that because it's like not it's what he hallucinate like what what is the challenge is as a child you'll imagine all the sky metrics right but in the answer far just like people usually didn't use it because it was a heat typing uh the people just like don't really like typing and also like because you give face with a domain a domain that you don't really know like I use a software because they don't know a lot of things about tax I don't know what was it you asked yeah oh so so these didn't know what's a type they didn't understand that domain you know they went to the tax thing because they wanted to take care of their own get their tax yeah so so I think myself is starting like uh trying to like understand more like what kind of questions like people would ask and like suggest that in the beginning and then this basically is a guide users so it's kind of education like how you hear the question you should ask and then here's as the answer is keep going so so I think it's like um a lot of that is um it's just a lot of understanding your domain as a problem domain like go talk to users looking as a data um I do still think that looking at data is a very very important I think Greg Brockman has a great quote about it so we think that's like manual data inspections is one of the activities that has a highest ratio of like value should predish so that means that's like people don't not think highly of like manual data inspection of data handling let's give it just some interns you do like let me think with something fancy like algorithms and stuff but actually it's too many high value because by looking at data you detect patterns you understand how the user's user product um so so I actually like I usually a very good practice I really highly recommend to teams it's like don't forget human evaluations so you use as a judge um but as a judge um have a lot of challenges because like so quality as a judge depend on the underlying model and the prompt and also not deterministic so things can change over time but like if you have some like human evaluations like very consistent like very clear guide like like every day let's go in there look as maybe like 50 samples of like actual interactions or like if you have more resources go at high like 500 or 1000s right so that you can like get some kind of like a picture of the how-lure users like first how the users are using job product um and it changes in behavior based on like current events maybe because a lot because of reasons like administration change maybe we'll have a lot more questions about that topic for example or like have you like co-relate with like other automated metrics for example like if the AI judge scores somehow like start changing compared to the human judge score maybe is there something you need to do to investigate yeah so I guess you cannot like as you said you can't really skip hard work if you want to get good results and you can't really pull humans out of a loop fully at least initially what are some common mistakes you've seen when teams are building AI applications um come with text yeah but I feel like I don't want to say no voice like oh everyone is an idiot um so um yeah um so so one I think we touch on a several um so one red comma mistake is like use genai when you don't need genai so so first of all this is a start off that came with me with a pitch it was like oh I'm gonna use genai to help people optimize electricity usage so when I ask like so the people can tell Chuck the chat the AI like hey here's our like I live here and here are the activities and do doing the days of very energy intensive maybe in a charging car or like doing laundry or something and the AI is gonna tell you like hey you should do this activity is this time and this time so that you can maximize like minimize the electricity bill and it will like oh our reasons show that like you can save you an average at 30% of electricity bill it was like free money why would anyone not want that um as I was asking them it's like um what is the what is what is this cost saving if I'm just like manually schedule like the most intensive one during the off big hours uh right and it's just like okay it's just charged so card like you know 10 p.m. or something you know yeah they were like we we haven't done that yet but we're gonna try it and as you know and they never got back to me and they abandoned the idea later so I feel like a lot of social optimization problems can be so like griddley like maybe even I especially yeah without genai um another spectrum is that like I see a lot of companies giving up on genai uh because it thinks that genai is not good for that problem because they have tried and it doesn't work and in a lot of a lot of time like it got surprised and it's like wait a second I just talked to another company who's just like the user for the similar use case it worked really well and when we look into it it usually because I'm like bad product like because they don't promise to well they don't understand the users they don't yeah this is like they don't even know what you evaluate well um so so for some more like it working with these like companies that does um um basically like um extracting uh resume information so it's a person I get a resume and they try to like map out like where is a person work before and like credit summary of that person's life and and um they have a few steps and like first like from resumes uh they try to extract all the text and then after all the extract texts they extract the organizations uh from the extractor text so by the way the resume is a pdf and not not not potex right and and then I asked them so they said okay it worked terrible like the the never like they got like the organizations like wrong about like 50% at the time and then I was asking them like uh when the process does this fail is that in the from the pdf you extract the text or from the extractor text you know organization extraction and they were like oh we don't know we didn't do that also if you if you can't pinpoint it you can't localize great fails then how can you fix it so so a lot of times it just like seems like come on stand but somehow I don't know is this something that's always like puzzled me a little um or like uh another is um another is just like statue complex for first of all like like jump straight to better databases or like a fire tuning um or another come on one nowadays it's like when you see a fancy agent framework it's just like let's let's use this framework you know let's just try it and I think it's eventually attractions are like really really cool like like I think really great for many attractions make my life easier but I do think that the attraction should encode like best practices and should be heavily tested but I think we still in the face we're still learning like uh best practices and also like a lot of attractions can introduce like unnecessary uh very very painful bucks uh so when I was going through the code basis of a lot of those frameworks and I found out something interesting like a lot of those frameworks have some different prompts you can have you get started right because it's like it's made you very easy for you to like uh to begin um but then like everything goes those prompts and look like have some type typos and they're just like changed so you have somebody submit a quick a PR to like fix the typos but it's not probably to release anything so if you're using like this framework using one of the different prompts and then suddenly there's a performance like applications that's like change you actually don't quite know like why is that changing because the problem was like changing under the feet um so so yes like those um those those are very interesting um like those are just patterns they are interesting because what you mentioned it sounds like if I'm you know collecting these it's like using this technology when you don't really need it giving up on it uh without you know just for common sense reasons you could have just fixed some easy things uh using a new framework when it's just not really high quality or it's and you know it doesn't really have the best practices that's kind of all sounds stuff that we could just replace Gen AI with you know a new a new technology or a new a new stack and we'll probably hear similar things right it's it's typically these things because it's just it's changing all the time there's no best practices no one really knows how to use it there's you know whoever tells you they're the expert they're still just you know they have a maximum of year of experience with it it's it's it's not really new is it yeah I think I definitely agree with you I do think that like even though like technology is changed over time they're like systematic thinking so like systematic uh approaches to problems usually don't change like yeah if you want to sort of problems uh you first start by like breaking down the problems like seeing where the challenges are and like go through different solutions you like do that it seems seem come on but I think like a lot of times a lot of us get uh formal I think formal get it the way and was like okay we know there's the right things you do but I also feel like it just needs to check this thing out first you know and it keeps doing that like three times a day so there is God and you just like never really get time to sit down and a thing really deeply about about what what that what is that you're trying to do yeah so I guess we're gonna see a lot of the kind of mistakes that are with these signals you happen plus if someone you know some of the listeners are have adopted new technology you can probably use some of that approaches I mean you know just localize it for for for Gen AI and you know see if you can avoid the some of those yeah speaking of speaking of new technology as someone who learning Gen AI a software engineer who wants to get into AI engineering what would your recommendation be to learn you have things do change so fast you did mention the importance of fundamentals what would you focus on um so um I have a lot of cars on learning uh because I like learning a lot and I think over time it's like try to like observe some patterns and like by the way like the way it's the way it's learning might not be the same as the way you're learning people have different learning style um but in general I think it's like um I think of learning has like two different uh approach once it's like project-based learning and the other is structure learning project-based learning is like okay you you choose a project and you work on it really like go and try and solve every problem in that in that project when I finish it well structure-based learning is more like when you take a course or you read a book it's just like you like somebody else like here's the things that you want to do and I think there's quite a bit of a debate on like um somebody told me recently that he um that like a friend a very good friend and he said like oh he thinks it's a problem nowadays with people who want to become an engineering is that they spend too much time learning and not enough of time doing uh and he was just like just forget both the courses forget all the books just take a project and just work on it and I do think this project-based learning is very very very valuable but you think of like here is a set of the skills and knowledge I want to do right I want I need to have to become like really good at something um project-based learning can have you hit a lot of this point but it doesn't all always have you hit like on the point and you can get it sometime again the confusion whereas sometimes it still needs to complement the structure learning and another thing that's shot on a project-based learning um that along with like usually do it follows some tutorials like people like here with someone hands is pretty sure how to do this and um I think tutorials it's a really cool and uh anything like I'm so fortunate to do it a lot but I still notice that it's very easy to just like mindlessly clicking oneself to another and it just runs the cell run another and don't really start to ask like why is this being this way like why is this library being important like why is this code written this way why is the batch size is 16 instead of like 64 like why is it like it's expert the mixture of expert so so people um it's very easy to not stop like there's no like mechanism to force you to stop you just want to run to the N and see what the output of it makes some changes like by the best guesses um it's something funny like when I was like working as it's like open source project um and um and I was like it would want to do a market research and like see who is using this uh this is framework so it was a framework as I did and I thought it's like I knew that if you needed if you wanted to use this framework you knew she'd do an import IBS right so I went through on a GitHub and I searched for all the reports that have the like import IBS and then found a report that I went through it it has the import IBS but then it doesn't like it the code base does not have IBS anywhere else it's not used at all and I was like what is happening so it realizes a lot of those reports like copy from a tutorial yeah and that tutorial used import IBS um and then and that's my mistake and then like everyone else so maybe the original developer like import IBS and then deleted the code because I didn't use it anymore um and then like everyone to cook up is just like did the same thing um so so I feel like that is something a little bit dangerous like tutorial based learning is great but I do think it's very important to be able to stop and ask questions and sometimes structure learning can help you like ask the right questions um like think think think think through um uh so so yeah so so I think my forebub was starting I would recommend maybe a mixture like yes choose to some project you want to work on it doesn't have it doesn't have to be like big fancy project like just let's try to like uh pick one um and then as the same time complement it with nice some structure learning like pick whatever like um maybe a book or doing a course with a friend's uh read paper um I think repeat paper is a bit a bit interesting um because repeat paper reading paper is like it's a skill it can be quite time consuming and you need to know like what what you want to get out of it um but yeah so like um start a project complement with like structure learning um at the same time there's an exercise that I felt very very useful at least for me initially is that like for a week I try to observe like what I do like try to make note of like what I do and try to think of like what percentage of that can be automated by AI like what what could be done by AI yeah and then it tries to use AI to do those uh and then this is give me a lot of ideas on like on on the use cases like this thing about what matters to me and it would be an application that gives us a problem is just great and ready yeah that's that I think it's an unconventional way but it's a good way to look at it because it's in the end also it kind of I think it might help you get ahead of you know this like dread of like what would AI do for me because you realize what happens when you automate things which actually suits my next question there's a lot of fear mongering around oh AI will mean the end of software engineering because they are AI is very good at coding it's a lot better than a lot of other areas what is your take on will as as AI gets better will it actually you know end software engineering or it will change it or it's not going to much change actually I think it goes back to the question of I was sub-engineering so um or maybe you can get a analogy um maybe it can help it explain it better so the writing right so we tend to confuse um the most salient activity if something as the job itself so first about writing writing in the past writing means as a physical act of like putting some like worse on a paper yeah yeah and and back then right like it was people think of writing as that and people actually took pride in the calligraphy like oh we have beautiful handwriting you must be smart you must be intelligent right but then had computers and now writing doesn't refer to the act anymore writing refers to process of arranging ideas into a readable format and I think the same thing as coding so now the whole thing like sub-engineering what I think I like is like it's a physical act of like putting code on like I know like a VS code or like VIM or whatever software that you use but but that's not what sub-engineering is so engineering is about like solving problems like here's a problem how do I like come up with executable programs to solve this problem coding itself is just like a physical act of it and I do things that's like yes um maybe AI can have you automate coding but I don't think it's gonna fully like automate like problem solving because you still need to know what problems it and and only you can understand like what problems you're facing like well and and also you know AI has the problem like coding really or software engineering really is like yes you need to solve problems but what I don't think we say is you need to do it very precisely the reason the job software engineer or programmer exists is is because it is very hard to be specific to speed the computer's language because you know if you move that if statement somewhere else or if you change a variable suddenly you know the program crashes because then you have a stack overflow exception which you of course understand if you're a software engineer but if this is just a business user who says I want to show you know if if you resize the window I want the button to move over it's easy to say but then as a software engineer you you know the edge cases you know the environment you know what needs to worry about you need to worry about system events etc and then you write code for all of those things and I'm sure we'll get to a point where the we'll be able to manage some of that but it might not and and you will still at some point need someone who understands you know that code and can figure out where the gap is because English is a language is not as precise as a programming language you know programming language is more invented to be very precise and unambiguous and very easy you know you can you can go from assembly code to the programming language because it's a one-on-one mapping or and then from English to a programming language that's very fuzzy right yeah yeah yeah definitely I think that profession will not go away like for users it might work for some kind of more obvious use cases of like you say something and you get something roughly that's it and you try multiple times and it generates something else and you're happy what for you know a business or professional use case you will need those people who will be able to guarantee that you get exactly what you want yeah I'm actually really excited about like AI can automate like part of coding because like it's actually an annual software engineer should do software much more complex so like I go back to the RRGF writing like before when he had to do the manual like copying words onto papers like all the books back then go very small like I think it's like like 5,000 words of 10,000 words considered like big because like it took a long time for people to like copying things but but now like we have books like a hundred of thousand words right and I think it's just to make things a lot easy and do things like with software engineering like if you don't have to manually record it quickly and I turn ideas into like snippets uh excitedly accessible programs I don't think the annual like it was right like much much more complex software yeah and maybe one software engineer will be able to kind of command a lot more software debug or or maintain a lot more complex system by by oneself because right now you know there's a reason that you know for a million lines of code usually there's like several engineers it's it's it's rare to have just one engineer if a company broke that I'm not talking about dependencies so that that will be interesting yeah so what are use cases are you excited about for that AI could bring outside of just coding um let's see I'm doing I think I'm excited about education um so so I do things just like AI can help people learn we want to learn a lot faster um so so I think one thing I realize is that like nowadays if you know the answers uh if you know the questions fighting the answer is actually quite easy like you can ask like AI and it's it's usually give you like pretty a lot of or like at least it can give you a lot of like references for you to like go and read more about it um but then like what's still hard is like how how should come up with the right questions and add things it's like education needs to like focus on like forcing students like create the habit of like asking questions and understanding like so I do think it's like it's help learning to become a lot more efficient like people can learn a lot of things um yeah and I feel like I do I do believe that if we are can learn better and faster then we can actually do more things so I'm very excited about like what what it would that uh what that would look like um what other use cases are I'm excited about um I'm excited about entertainment um yeah I think it's like um I think it's like sometimes we were thinking of like entertainment and education as separate things but I don't see why we can't have like games that help with learn things more like we have some strategy games like the shingles about negotiation so it could be like really fun um yeah so um or this is like being like more um intelligially stimulating content you know like movies or shows don't have to be like about like my less but I don't know other people like watch that because it's have them as give but I also I also think like I kind of like genre I was like maybe think a little you know like for all like understand more about like different uh fields um so so I do think that we now I can ask sisters and like creating content that is both entertaining and intellectually stimulating um yeah so I think it could be like a lot of fun uh for some people it's like now that we have a lot of like mediums adaptations so you have a book you can convert it into a movie and sometimes we have a movie you convert it into a game um or like you have a papers convert into a podcast so so now if we have some content I can help us like convert a daft different medium that could be like very very exciting um yeah I think it's like is there a lot like small problems um that that I'm also like interested in um I think it's like I haven't touched on any of the enterprise I feel like that's where most money is still uh I do think that sell a lot of like um I think that enterprise or company organization structure is going to change um so so first of all what does that mean it's like if you think about like a lot of organization what what is the job with the middle middle management it's like two two like first like chance like a great information from the report and transmitted like up uh up to the executives and all like and this is yeah the way they transport it like a chance meeting information like directions from executive to like lower layers uh layers but like information aggregations actually like really really that's something that I can be like really well so so I do think this like companies can be a lot more uh um more affectionate so let's close up with some rapid questions so I'll just shoot some questions and you tell me what pops your mind uh what programming languages use most when you built AI applications or did ML engineering and why I thought in JavaScript JavaScript as well how come uh yeah differently I think it's like a huge part of like uh building products when I have to debut demo quickly um so so I think that's that's very very handy I'm not very good like I think I've always been scared of JavaScript but very grateful like AI actually helped me like getting started a lot easier nowadays yeah and which one is your favorite addela model right now and why oh uh I don't really have a favorite uh I use it for different things so I use you use chatty pdl habit um because I have a bunch of prompts I use already like have little things like rebuilds prompt for chatty pdl I'm still using um I use cloth sometimes for like career writing um because I think it's less sometimes less cliche um I'm reading on deep sick R1 and who's not reading about I want so like just trying it up I don't think I have like a favorite I think it is some of the lama uh like lava uh like which is the vision uh uh version of like lama before for like some some kind of interesting use case like from like a screenshot to quote like just trying to test it out um I think fun with it um but yeah I'm not emotionally attached to any of them yeah and what's the neat AI tool that you've used and that you like um so I have something that I built that's really help me good research uh so so one thing is like when when I saw a link like a papers um there's a lot of links and I try to get a so I realize when I read a links I do through the same same process like every see the abstract uh I look up the authors use a little work um ask questions um I also check my when I was out uh check the citation so I have a little tool so I just like go and scan so I have a link you just give me all the information on I don't know yeah like it's like it's just about tools to scratch your own itch um yeah so it's like I think that's just a beauty of AI now you can you can feel like it's taking like very small amount of time like you just view a tool like just for you like like before it would take me like wicks but now I can just like do one of that like I don't know you know this is something to be excited about I agree with yeah yeah what are what are one or two books that you've read and would recommend um oh I recommend a lot of books but I feel like it's recommending books and we feel like forcing people to do what you enjoy uh so um and like books that's like help me get a news perspective um or like get give me like inside and choose some topics I don't know a lot about so also I really like the book like first of all like complex adaptive systems uh it's a very interesting book about a system thinking like how should design um like yeah like um how should you how should design like social dynamics so that to get people to work towards like the goals as you want to work on it's very interesting book uh have forcied the thing about systems um I like the book uh suffice gene um because uh you understand more like a free will and makes you like question a little stuff uh but it's the idea of like you can live on either through like genes or through ideas like there are two ways that you can live on it's like yeah so genes continue to live on with their offspring and like uh reproductions the other thing is like if you have ideas and the ideas can also like replicate and like go on like the idea of memes like genes and memes um I like the book uh anti-frasio uh so so so yeah so so the ideas is saying um I think the author is it's a very interesting character uh I just read several his books actually we like them um yeah yeah I like the book um yeah I think I think there are a lot of books that I like no this then thank you for the recommendations so thank you for being on the podcast I mean AI enduring is such a new feels and it was great to hear from from yourself who has clearly gone very broad and also very deep and has been in this field even before was called AI engineering so thank you for this thank you so much for letting me ramble on on the show um yeah I really appreciate it and um I think that I think what I'm saying for me is I really enjoy from writing or talking is that I get feedback because somebody was like oh I um I think I'm less interested like I agree with you I mean it's great to hear but it can also like a little bit like not good for the ego uh but it's a really like like maybe a little pushback like okay you didn't think about this uh you forgot this or I did take it to a card this one so I would love to get those feedback uh so if they're anything that you feel like I miss out on like do let me know so I appreciate it thank you to chip for this conversation about AI engineering to get in touch with chip including to get feedback on her book you can find her contact details on her website linked in the show notes below I've also found her book AI engineering to be abroad and deep overview of this important field it's a book that focuses on the fundamentals that are unlikely to change so if you want to learn these it's a good book to have in the pragmatic engineering we've previously sit on several AI and ML related deep dives check them out also linked in the show notes below if you enjoyed this podcast please do subscribe on your favorite podcast platform and on YouTube thanks and see you in the next one