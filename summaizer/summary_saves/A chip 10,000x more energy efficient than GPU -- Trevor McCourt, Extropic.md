# Video Information

**Title   ** : A chip 10,000x more energy efficient than GPU -- Trevor McCourt, Extropic  
**Uploader** : DEMi network  
**Duration** : 17:54  
**URL     ** : https://www.youtube.com/watch?v=uzxTDzvZy1o  

---

## Transcription

 The takeaway from this is that if you're going to be a startup working on kind of new computing technology, you shouldn't be aiming for a 10x, you should be aiming for a 10,000x. Hello everyone. I'm Trevor from Extropic. I'm going to tell you a little bit of what we've been doing in the last two years, which is shrouded in extreme mystery. Cool. So I don't know if you guys have heard, but this whole AI thing is kind of big. A lot of people talking about it, and you wouldn't believe the things they're saying. It's crazy. No, I was looking into this recently for something else, and I found that the big tech CEOs are actually making a lot of really specific claims about what they think the near future is going to look like. And you know, you can read through this yourself, but the kind of central pattern is they're all predicting that we're all going to be using AI, basically all the time within a few years from now. And that's awesome. I think everybody in this room wants that, and thinks that would be cool. And the result of that is when they say stuff like that, stock price goes up and everyone's happy. But when they make these kind of claims that tease up physicists like me to kind of try and do a fermi estimate of what the cost would actually be of that kind of future. Like what would it actually take for all of us to have an AI assistant that we're using all the time every day? Because it's a cost to running AI models, as we all know, because we're here, AI uses electricity. And the more you use AI, the more electricity you draw. And specifically, the type of models that most of us are using most of the time actually have this really easy way to predict the resource utilization that's behind them. So for transformers, basically you can come up with a simple formula that tells you how much power you would need to supply a data center or many data centers to process a particular amount of tokens per second. And basically you can come up with this kind of Drake equation type model that will tell you exactly how much power you would need to supply the world with the particular token throughput for a particular size of a transformer model. And if you plug in all of the right parameters that you would need for all of us to have kind of an agentic AI system running all the time, even if that system is only processing all of the text we interact with every day, what you'll find is that those AI's are going to be drawing about half of the total grid. So half of all of the power in the US would be going towards these systems. And if you extend this even just to one FPS video, which is what someone like Mark Zuckerberg is really betting on with his glasses, you can come up with truly insane numbers like 10X 20X 30X the total grid. And that's just because these models are really big and really power hungry. And if you extend this further to the type of robotic stuff that people are talking about this morning, the numbers would go off the chart. So hundreds of thousands of times more power than we can supply today. And obviously this is all the wild extrapolation and I'm off by a lot, I'm sure. But this is kind of just giving an order of magnitude of what's actually possible. And so if you look at models that incorporate kind of supply side constraints in terms of power and hardware, they're most aggressive projections of growth in the next few years. And so I'm only go up to about 10% of the total grid. So there's clearly a huge gap between expectation and reality. And these really high estimates of the video models they correspond to about a 20,000 gigawatt draw compared to the 500 gigawatts that we used today. And again, you should take all of these numbers with like a whole cup of salt because I'm extrapolating wildly. But I think you get the point. So if we want to actually realize this future that I think we all want, we're going to need a lot more resources. And our two options are basically to produce a lot more energy or to make computers a lot more efficient. Spoiler, these are both really, really hard problems. So in terms of more energy, basically every big tech company has started to kind of also become a energy supplier. So pick a name, Microsoft, Amazon, whatever, they all have projects underway where they're bringing up like gigawatt scale energy production facilities for data centers. And that's great. There's two problems with this, especially from my perspective as someone who's involved with startups. Energy tech is a really, really hard path for a startup, not something that I would ever do, although I think there's probably people here this week that are. And the other thing is that, you know, one gigawatt power project is massive, but from the math I just showed you, even if I'm off by a factor of 10 or 100, that's not even close to enough to what we actually need. So if we're going to go the power production route, this is going to be like a massive global scale effort. You know, that's going to conclude in something that looks like a Dyson sphere, which I think will happen anyways, but it's going to take a really long time. The other option is making computers much more efficient. And this is a really hard technical problem, which I'll get to in a second. But the other thing that makes this problem really challenging is it's really hard to find people that are capable of meaningfully working on this because you kind of need to be an expert in both hardware and device physics and also algorithms. So this is potentially one of the most technically all encompassing things you can work on today. The other thing, and this is the real kicker as a startup, is that GPUs are getting better at a very predictable exponential rate. If you just plot the GPU flops per jewel as a function of time, it's doubling about every two years. Despite all the naysayers that thought this was going to stop, it just keeps going because engineers are resourceful. So if I'm a startup that's trying to go out and build a more efficient type of computing, I'm fighting against this kind of constant incremental gain that will just kind of bowl me over if I'm not fast enough. So the takeaway from this is that if you're going to be a startup working on new computing technology, you shouldn't be aiming for a 10x. You should be aiming for a 10,000x, but nobody will fund that. So you see, there's a lot of kind of society scale problems that make this particular thing hard. Now, from a technical perspective, why is it hard to make a computer more efficient? This problem is actually kind of a bit of a rock in a hard place. So if you look at where energy gets spent in a computer today, it's actually mostly in charging capacitors. So computers are made out of wires and metal. And when the voltage in a piece of metal changes, there's a energy change associated with that. And today, that energy kind of just gets dumped when you discharge a capacitor. And so if you want to make that energy smaller, you can either make a capacitance smaller, you can make voltage smaller. You can't make a capacitance smaller anymore. We've kind of made that as small as we can for various regions. It's actually quite a complex problem. And voltage is actually constrained by some pretty fundamental thermodynamic considerations about how transistors work. So there's this kind of intrinsic noise to transistors. That has a scale that's called VT. It's the thermal voltage. And as V starts to approach VT, your signal will start to be consumed by this noise. Moreover, from the same physics, this thermodynamic effect limits the off current in a transistor. So when a transistor switched off, it actually still conducts an electricity. And the amount of electricity that gets conducted is actually an exponential function of what's called the threshold voltage, which is the on voltage of the transistor. So as I drop the threshold voltage to make this V squared term smaller, there's another source of energy consumption that gets exponentially larger. So when you're designing a computer these days, you're kind of boxed in on both sides by these two conflicting constraints. This is a pretty hard problem. A lot of ways that people try and solve this are by getting away from this one half CV squared entirely. So that's kind of the angle of these photonic computing approaches that try to use light because light doesn't interact with anything. This is also the angle for adiabatic logic. They try and get rid of this charging scheme. You can go even crazier to stuff like quantum computing, which is a completely different paradigm, very low voltages. But very, very hard problem. And this is kind of, this is probably like the most important equation in the world right now. If you want to solve this, I think what you have to do is look back on wire things the way they are today, right, from both an algorithm and hardware perspective. And if you really think it through, you'll realize that there's no fundamental reason why anything is the way it is today. It was really just a series of coincidences. Back in 2012, there was this original work where people figured out GPUs are really useful for deep neural networks. And that kind of set off this cascade where AI researchers would push the frontier of algorithms and they'd want better GPUs. So then in video would make better GPUs, right, and then they do even crazier stuff with those GPUs. And then you end up with some kilowatt monstrosities that data centers are full of. And we look at that now and we're like, well, we're so entrenched in this way of doing things that we can't even think about doing something else. Right, and so we're kind of stuck. And so I think at this point tech companies are bringing up data centers to power this feedback loop. I think it's very prudent to look back and ask, is there a better way to do things that has nothing to do with the current paradigm. And so that's kind of what the way we're approaching things at Extropic, which is these thermodynamic limitations about transistors. We're kind of asking, is there a way we can use that to our advantage, right, instead of having it form this kind of brick wall that blocks progress? And the way that you can consider doing that is by kind of using this intrinsic thermal noise in transistors as a computational resource instead of having it just be a burden. Right, and so what that corresponds to is building a probabilistic computer. So a computer that doesn't compute deterministically, it doesn't compute using deterministic signals that compute using probabilistic signals. And from the perspective of machine learning, this actually makes a lot of sense because all of the algorithms that were dumping all this energy into a probabilistic, right, like transformers or a lot of aggressive LLM sample from distributions. It's even a bit more on the nose when you look at diffusion models because those are really like a simulation of a noisy physical system. Right, so there's a lot of connections that you can draw between physics and machine learning that make this kind of probabilistic hardware actually quite natural. And so, you know, we've been working on over the last few years is actually realizing kind of a minimal version of this. And what I'm showing on the screen here is kind of a hello world type demo of our tech. We have kind of put together this probabilistic hardware system that performs a generative modeling task and we've compared it head to head against traditional GPU based algorithms like VAE's and GANS on a simple benchmark, which is this so-called fashion eminus data set. Right, the kind of punch line is if you want performance parity between our hardware and a VAE, you need to use about four times more energy than us. Sorry, four is a magnitude, not four times. That's an important detail. And, you know, this kind of improvement is at the scale that we actually need to solve this problem I pointed out. Right? And Nvidia is not going to beat four is a magnitude. Right, so this thing actually has a chance of getting off the ground. And that's really what we're going to be striving for for the next two years is building a actual computer that does this instead of just a simulation of one. And so I'm just going to talk a little bit about what is behind that plot there. This is going to be a bit of a teaser because I don't have time, but this is going to be a paper out on this sooner rather than later. And so the key thing here is emerging energy based models, which are a, you know, not so popular form of machine learning with denoising diffusion, which is extremely popular. And the idea here is that it turns out it's very straightforward to build a piece of hardware that implements a probabilistic piece of hardware that implements energy based models at a very low level. But energy based models are not a good model class on their own because it turns out as you try and use an EBM to fit more and more complex data sets, it becomes completely intractable to sample from. So the amount of energy you have to expend to sample from a complex EBM grows extremely rapidly and this is kind of why EBMs haven't taken off as generative models of their own. But if you go, if you look at how denoising model works, the way it works is basically you start with your data at times zero and you gradually add noise to it until you get a pure noise. So you convert your image into noise and then you try and train a machine learning system to go backwards from the noise to the image. And it turns out, you know, if you look at the math of this, if you make the time steps small enough in this forward process, you end up with a really simple distribution you can sample from like a Gaussian. Now, if you want to take bigger steps, you need to be able to sample from a richer distribution which you can do with an EBM. And I don't have time to totally flush this out. But the idea is you can use our probabilistic hardware to approximate these large steps in the reverse process of a denoising model. And what this does is it alleviates this kind of intractable sampling problem that EBMs not only have. So by merging our hardware with these denoising models, we basically have it have a way to use our hardware efficiently to solve a problem in machine learning. And this is a bit much to explain in four minutes. So you'll have to stick with that and read the paper when it comes out. And so the kind of key technology we had to develop to make this possible was a good source of randomness that is mass-manufacturable. So this might seem a bit contradictory because I just told you that a big problem with transistors is that they're noisy. But it turns out that actually harnessing this noise and using it to do some kind of computational task is difficult and is a pretty tricky engineering problem. And so this is something that we have kind of cracked over the last two years. Basically, we figured out how to harness the noise and transistors to sample from computationally useful distributions. And in fact, we actually built the test chip recently and put a lot of this theory to practice and validate it. So basically, we can build a circuit that's just a handful of transistors. It takes up a few or microns on a chip. And that circuit acts as a programmable source of random bits. So it generates 1s or 0s and I can bias the probability of whether or not it's 1 or 0 electrically. And this random number generator is brutally efficient. So it's around 10,000 times more efficient than a pseudo-random number generator used on a CPU or something. So really, really low energy consumption. And so when you combine this really efficient RNG with the probabilistic hardware architecture that I barely described a minute ago, you get that plot that I showed you earlier. Again, kind of a tease you're going to have to read the paper. But I hope that gets you excited at least. And the plot I showed you earlier was a kind of simple data set, a bit of a hello world. And the reality of this is that this probabilistic thing is probably a subroutine in a bigger machine learning system. It's probably not carrying the whole load by itself. Right? And so this slide is a bit of a research snapshot, not really a complete result. So we're looking into how do we combine these probabilistic computing subroutines with traditional neural networks to solve more complex problems. And so, you know, what we're looking at here is we're using a combination of auto encoders and gans to embed a less trivial data set, this C4-10, which is about 10 years more advanced in the last one, into the space of one of these probabilistic computers. And kind of do this generative modeling in latent space. Right? And so you can think about it as, you know, again, usually starts from Gaussian noise and shapes that into whatever an image. Instead of seeding the GAN with just Gaussian noise, we seed it with the output of this denoising process. Right? So the noise has a lot of structure and correlation that's useful. And by doing that, you can save quite a bit of deterministic compute over a traditional solution. So, you know, this snapshot here shows about an order of magnitude improvement, but I think this can be pushed a lot further, and that's kind of what we're working on right now. And that's, you know, mostly why it came to this conference is to try and get people excited about this and come on board. So, you know, right now I'm looking to bring on kind of a cohort of research residents to push this hybrid machine learning angle. If you actually join, you'll have more than 15 minutes to understand what I'm talking about, and maybe you'll get a real idea of what we're doing. Extrafic is a really awesome company, very small, very elite. We have this amazing analog design team that has over 75 years of combined experience that unlocked this novel hardware that I was talking about. We've got some really specialised talent in transistor noise modeling, and we have some really excellent probabilistic machine learning researchers that kind of put all of this work together. And there's just a ton of work to do. So, if working on this kind of pure probabilistic machine learning excites you, I encourage you to scan this QR code here, and it'll take you to our website. Also exciting over the summer, you're going to start hearing a lot more about us, and there's actually going to be kind of an early access program for our hardware and algorithms. And this QR code will take you to kind of like a alpha signup page for that. Cool, so thanks for your attention, and feel free to come talk to me later if you have questions.